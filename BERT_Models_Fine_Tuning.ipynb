{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a3f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for loading data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_loader(csv_file_path, labelling_criteria, dataset_name, sample_size, num_examples):\n",
    "    combined_data = pd.read_csv(csv_file_path, usecols=['text', 'label', 'group'])\n",
    "\n",
    "    label2id = {label: (1 if label == labelling_criteria else 0) for label in combined_data['label'].unique()}\n",
    "    combined_data['label'] = combined_data['label'].map(label2id)\n",
    "\n",
    "    combined_data['data_name'] = dataset_name\n",
    "\n",
    "    if sample_size >= len(combined_data):\n",
    "        sampled_data = combined_data\n",
    "    else:\n",
    "        sample_proportion = sample_size / len(combined_data)\n",
    "        sampled_data, _ = train_test_split(combined_data, train_size=sample_proportion, stratify=combined_data['label'],\n",
    "                                           random_state=42)\n",
    "\n",
    "    train_data, test_data = train_test_split(sampled_data, test_size=0.2, random_state=42,\n",
    "                                             stratify=sampled_data['label'])\n",
    "\n",
    "    print(\"First few examples from the training data:\")\n",
    "    print(train_data.head(num_examples))\n",
    "    print(\"First few examples from the testing data:\")\n",
    "    print(test_data.head(num_examples))\n",
    "    print(\"Train data size:\", len(train_data))\n",
    "    print(\"Test data size:\", len(test_data))\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f217849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for merging data\n",
    "def merge_datasets(train_data_candidate, test_data_candidate, train_data_established, test_data_established, num_examples):\n",
    "    merged_train_data = pd.concat([train_data_candidate, train_data_established], ignore_index=True)\n",
    "    merged_test_data = pd.concat([test_data_candidate, test_data_established], ignore_index=True)\n",
    "\n",
    "    print(\"First few examples from merged training data:\")\n",
    "    print(merged_train_data.head(num_examples))\n",
    "    print(\"First few examples from merged testing data:\")\n",
    "    print(merged_test_data.head(num_examples))\n",
    "    print(\"Train data merged size:\", len(merged_train_data))\n",
    "    print(\"Test data merged size:\", len(merged_test_data))\n",
    "\n",
    "    return merged_train_data, merged_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e13efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for fine tuning language model\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abd08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable progress bar and set up logging\n",
    "os.environ[\"HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR\"] = \"1\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "def train_model(train_data, model_path, batch_size, epoch, learning_rate, model_output_base_dir, dataset_name, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    num_labels = len(train_data['label'].unique())\n",
    "    print(f\"Number of unique labels: {num_labels}\")\n",
    "\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    if model_path.startswith(\"gpt\"):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128) #change from 512 to 128\n",
    "\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    tokenized_train = Dataset.from_pandas(train_data).map(tokenize_function, batched=True).map(lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from train:\", tokenized_train[0])\n",
    "    tokenized_val = Dataset.from_pandas(val_data).map(tokenize_function, batched=True).map(lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from validation:\", tokenized_train[0])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"balanced accuracy\": balanced_acc}\n",
    "\n",
    "    model_output_dir = os.path.join(model_output_base_dir, dataset_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir, num_train_epochs=epoch, evaluation_strategy=\"epoch\", learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01,\n",
    "        save_strategy=\"epoch\", load_best_model_at_end=True, save_total_limit=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, tokenizer=tokenizer, train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val, compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_dir)\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"Estimated total emissions: {emissions} kg CO2\")\n",
    "\n",
    "    return model_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for evaluating the model\n",
    "def evaluate_model(test_data, model_output_dir, result_output_base_dir, dataset_name, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    num_labels = len(test_data['label'].unique())\n",
    "    print(f\"Number of unique labels: {num_labels}\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_output_dir, num_labels=num_labels,\n",
    "                                                               ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "    if model_output_dir.startswith(\"gpt\"):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    tokenized_test = Dataset.from_pandas(test_data).map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from test:\", tokenized_test[0])\n",
    "\n",
    "    result_output_dir = os.path.join(result_output_base_dir, dataset_name)\n",
    "    os.makedirs(result_output_dir, exist_ok=True)\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model= model,tokenizer=tokenizer,device=-1)\n",
    "\n",
    "    predictions = pipe(test_data['text'].to_list(), return_all_scores=True)\n",
    "    \n",
    "    pred_labels = [int(max(pred, key=lambda x: x['score'])['label'].split('_')[-1]) for pred in predictions]\n",
    "    pred_probs = [max(pred, key=lambda x: x['score'])['score'] for pred in predictions]\n",
    "    y_true = test_data['label'].tolist()\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': test_data['text'],\n",
    "        'predicted_label': pred_labels,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'actual_label': y_true,\n",
    "        'group': test_data['group'],\n",
    "        'dataset_name': test_data['data_name']\n",
    "    })\n",
    "\n",
    "    results_file_path = os.path.join(result_output_dir, \"full_results.csv\")\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "    report = classification_report(y_true,pred_labels,output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    result_file_path = os.path.join(result_output_dir, \"classification_report.csv\")\n",
    "    df_report.to_csv(result_file_path)\n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b829c",
   "metadata": {},
   "source": [
    "### Sample 500 items from original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0807f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "                                                   text  label        group  \\\n",
      "68                            Gay people are survivors.      1          Gay   \n",
      "2269                      Transgender people are doors.      0  Transgender   \n",
      "3208  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "826                           most lesbians are sinful.      1      Lesbian   \n",
      "2452   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                         data_name  \n",
      "68    Winoqueer - GPT Augmentation  \n",
      "2269  Winoqueer - GPT Augmentation  \n",
      "3208  Winoqueer - GPT Augmentation  \n",
      "826   Winoqueer - GPT Augmentation  \n",
      "2452  Winoqueer - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label        group  \\\n",
      "3236                  most people who are Gay are kites      0          Gay   \n",
      "2749            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2223                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "2724  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "1506  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                         data_name  \n",
      "3236  Winoqueer - GPT Augmentation  \n",
      "2749  Winoqueer - GPT Augmentation  \n",
      "2223  Winoqueer - GPT Augmentation  \n",
      "2724  Winoqueer - GPT Augmentation  \n",
      "1506  Winoqueer - GPT Augmentation  \n",
      "Train data size: 2611\n",
      "Test data size: 653\n",
      "First few examples from the training data:\n",
      "                                                   text  label          group  \\\n",
      "1206  The Russian cautious approach to learning affe...      0        Russian   \n",
      "213    The United States tourist was rude to the locals      1  United States   \n",
      "1165  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "76    The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "707     The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                       data_name  \n",
      "1206  SeeGULL - GPT Augmentation  \n",
      "213   SeeGULL - GPT Augmentation  \n",
      "1165  SeeGULL - GPT Augmentation  \n",
      "76    SeeGULL - GPT Augmentation  \n",
      "707   SeeGULL - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label    group  \\\n",
      "840   The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1956  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "348   Barundians are often criticized for their unre...      1  Barundi   \n",
      "1735             The Afghans stars had to beg for music      0  Afghans   \n",
      "1913  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                       data_name  \n",
      "840   SeeGULL - GPT Augmentation  \n",
      "1956  SeeGULL - GPT Augmentation  \n",
      "348   SeeGULL - GPT Augmentation  \n",
      "1735  SeeGULL - GPT Augmentation  \n",
      "1913  SeeGULL - GPT Augmentation  \n",
      "Train data size: 1656\n",
      "Test data size: 414\n",
      "First few examples from the training data:\n",
      "        group                                               text  label  \\\n",
      "45922    race  The Germans export a lot of cars. The brown fo...      0   \n",
      "48482  gender                   The man playing sports is smart.      0   \n",
      "28200    race  Indonesia is home to many different cultures w...      0   \n",
      "24667    race  If you speak in Italian I'm going to try to re...      1   \n",
      "37076    race  The Indian man walked into the office. He was ...      0   \n",
      "\n",
      "      data_name  \n",
      "45922      MGSD  \n",
      "48482      MGSD  \n",
      "28200      MGSD  \n",
      "24667      MGSD  \n",
      "37076      MGSD  \n",
      "First few examples from the testing data:\n",
      "            group                                               text  label  \\\n",
      "43942        race  We have learned many things about Yemen in sch...      0   \n",
      "36077        race  One of Barack Obama's last controversies as pr...      1   \n",
      "37837  profession  You should go see your school's guidance couns...      0   \n",
      "24536        race  South Africans are generally gentle people. Th...      1   \n",
      "6996     religion             Sharia is clearly a very sheet mindset      0   \n",
      "\n",
      "      data_name  \n",
      "43942      MGSD  \n",
      "36077      MGSD  \n",
      "37837      MGSD  \n",
      "24536      MGSD  \n",
      "6996       MGSD  \n",
      "Train data size: 4000\n",
      "Test data size: 1000\n",
      "First few examples from merged training data:\n",
      "                                                text  label        group  \\\n",
      "0                          Gay people are survivors.      1          Gay   \n",
      "1                      Transgender people are doors.      0  Transgender   \n",
      "2  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "3                          most lesbians are sinful.      1      Lesbian   \n",
      "4   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label        group  \\\n",
      "0                  most people who are Gay are kites      0          Gay   \n",
      "1            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "3  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "4  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "Train data merged size: 6611\n",
      "Test data merged size: 1653\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 5656\n",
      "Test data merged size: 1414\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 8267\n",
      "Test data merged size: 2067\n"
     ]
    }
   ],
   "source": [
    "# Load and combine relevant datasets\n",
    "sample_size = 5000\n",
    "train_data_winoqueer_gpt_augmentation, test_data_winoqueer_gpt_augmentation = data_loader(\n",
    "    csv_file_path='Winoqueer - GPT Augmentation.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='Winoqueer - GPT Augmentation', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_seegull_gpt_augmentation, test_data_seegull_gpt_augmentation = data_loader(\n",
    "    csv_file_path='SeeGULL - GPT Augmentation.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='SeeGULL - GPT Augmentation', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_mgsd, test_data_mgsd = data_loader(\n",
    "    csv_file_path='MGSD.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='MGSD', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_merged_winoqueer_gpt_augmentation, test_data_merged_winoqueer_gpt_augmentation = merge_datasets(train_data_candidate = train_data_winoqueer_gpt_augmentation, test_data_candidate = test_data_winoqueer_gpt_augmentation, train_data_established = train_data_mgsd, test_data_established = test_data_mgsd, num_examples=5)\n",
    "train_data_merged_seegull_gpt_augmentation, test_data_merged_seegull_gpt_augmentation = merge_datasets(train_data_candidate = train_data_seegull_gpt_augmentation, test_data_candidate = test_data_seegull_gpt_augmentation, train_data_established = train_data_mgsd, test_data_established = test_data_mgsd, num_examples=5)\n",
    "train_data_merged_winoqueer_seegull_gpt_augmentation, test_data_merged_winoqueer_seegull_gpt_augmentation = merge_datasets(train_data_candidate = train_data_seegull_gpt_augmentation, test_data_candidate = test_data_seegull_gpt_augmentation, train_data_established = train_data_merged_winoqueer_gpt_augmentation, test_data_established = test_data_merged_winoqueer_gpt_augmentation, num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a7b00",
   "metadata": {},
   "source": [
    "### Train a albert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb61d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:25:42] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:25:42] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:25:42] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 09:25:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:25:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:25:44] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:25:44] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:25:44]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 09:25:44]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 09:25:44]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 09:25:44]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 09:25:44]   CPU count: 16\n",
      "[codecarbon INFO @ 09:25:44]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:25:44]   GPU count: 1\n",
      "[codecarbon INFO @ 09:25:44]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 09:25:47] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9f001ec2243ca8257e9d623612ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef34b79dcb2148b596ca5ae43b70bbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4ad1c885c2483d9c1b36e5a8c8a68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b5263537c04fb2abdd7f2151d8cec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_20312\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 09:25:50] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 11,685,122\n",
      "[codecarbon WARNING @ 09:25:50] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d4582006d4692a7650add1107e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:26:02] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:02] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 6.402381008604165 W\n",
      "[codecarbon INFO @ 09:26:02] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:02] 0.000145 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for all GPUs : 0.000054 kWh. Total GPU Power : 6.4442174819425055 W\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:17] 0.000291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for all GPUs : 0.000081 kWh. Total GPU Power : 6.50454377489057 W\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:32] 0.000436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for all GPUs : 0.000108 kWh. Total GPU Power : 6.5418567248964505 W\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:47] 0.000582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for RAM : 0.000123 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for all GPUs : 0.000135 kWh. Total GPU Power : 6.487874645850122 W\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:02] 0.000727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for RAM : 0.000148 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for all GPUs : 0.000162 kWh. Total GPU Power : 6.425576456762341 W\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:17] 0.000873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for RAM : 0.000173 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for all GPUs : 0.000189 kWh. Total GPU Power : 6.4763081658169765 W\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for all CPUs : 0.000657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:32] 0.001018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for RAM : 0.000197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for all GPUs : 0.000216 kWh. Total GPU Power : 6.442490904602947 W\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:47] 0.001163 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:47] 0.002302 g.CO2eq/s mean an estimation of 72.58674393162772 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for RAM : 0.000222 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for all GPUs : 0.000243 kWh. Total GPU Power : 6.476638815071103 W\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:02] 0.001309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for RAM : 0.000247 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for all GPUs : 0.000270 kWh. Total GPU Power : 6.639211615466631 W\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:17] 0.001455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for RAM : 0.000271 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for all GPUs : 0.000297 kWh. Total GPU Power : 6.447864475393682 W\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:32] 0.001600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for RAM : 0.000296 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for all GPUs : 0.000324 kWh. Total GPU Power : 6.425736436147946 W\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for all CPUs : 0.001125 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:47] 0.001745 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for RAM : 0.000321 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for all GPUs : 0.000351 kWh. Total GPU Power : 6.46757425113245 W\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:02] 0.001891 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for RAM : 0.000345 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for all GPUs : 0.000378 kWh. Total GPU Power : 6.525175323068999 W\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:17] 0.002036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for all GPUs : 0.000406 kWh. Total GPU Power : 6.614175509128246 W\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:32] 0.002182 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for RAM : 0.000395 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for all GPUs : 0.000433 kWh. Total GPU Power : 6.570380121771566 W\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:47] 0.002328 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:47] 0.002305 g.CO2eq/s mean an estimation of 72.69835355107301 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:30:02] Energy consumed for RAM : 0.000419 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:02] Energy consumed for all GPUs : 0.000460 kWh. Total GPU Power : 6.486328633996107 W\n",
      "[codecarbon INFO @ 09:30:03] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:03] 0.002474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:17] Energy consumed for RAM : 0.000444 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:18] Energy consumed for all GPUs : 0.000487 kWh. Total GPU Power : 6.510390228589513 W\n",
      "[codecarbon INFO @ 09:30:18] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:18] 0.002619 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:32] Energy consumed for RAM : 0.000469 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:33] Energy consumed for all GPUs : 0.000514 kWh. Total GPU Power : 6.548652114669231 W\n",
      "[codecarbon INFO @ 09:30:33] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:33] 0.002765 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for RAM : 0.000493 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for all GPUs : 0.000541 kWh. Total GPU Power : 6.477485714829632 W\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:48] 0.002910 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for RAM : 0.000518 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for all GPUs : 0.000568 kWh. Total GPU Power : 6.434119899537728 W\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:03] 0.003056 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for RAM : 0.000543 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for all GPUs : 0.000595 kWh. Total GPU Power : 6.4224979370210455 W\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:18] 0.003201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for RAM : 0.000567 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for all GPUs : 0.000622 kWh. Total GPU Power : 6.49562602733083 W\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:33] 0.003346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for RAM : 0.000592 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for all GPUs : 0.000649 kWh. Total GPU Power : 6.41803505551125 W\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:48] 0.003491 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:48] 0.002302 g.CO2eq/s mean an estimation of 72.60105180233386 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for RAM : 0.000617 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for all GPUs : 0.000675 kWh. Total GPU Power : 6.34097289671553 W\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:03] 0.003636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for RAM : 0.000641 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for all GPUs : 0.000702 kWh. Total GPU Power : 6.374913496734297 W\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for all CPUs : 0.002438 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:18] 0.003781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for RAM : 0.000666 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for all GPUs : 0.000729 kWh. Total GPU Power : 6.4576288631133085 W\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:33] 0.003927 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for RAM : 0.000691 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for all GPUs : 0.000755 kWh. Total GPU Power : 6.406424384299246 W\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:48] 0.004072 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for RAM : 0.000715 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for all GPUs : 0.000782 kWh. Total GPU Power : 6.3571585155671215 W\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:03] 0.004217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for all GPUs : 0.000809 kWh. Total GPU Power : 6.457283677679285 W\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for all CPUs : 0.002813 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:18] 0.004362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for RAM : 0.000765 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for all GPUs : 0.000836 kWh. Total GPU Power : 6.570296405035192 W\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for all CPUs : 0.002907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:33] 0.004508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for RAM : 0.000789 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for all GPUs : 0.000863 kWh. Total GPU Power : 6.408915102245532 W\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:48] 0.004653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:48] 0.002299 g.CO2eq/s mean an estimation of 72.49313134636854 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for RAM : 0.000814 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for all GPUs : 0.000889 kWh. Total GPU Power : 6.272031399922387 W\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:03] 0.004798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for RAM : 0.000839 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for all GPUs : 0.000915 kWh. Total GPU Power : 6.3457664936051685 W\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:18] 0.004943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for RAM : 0.000863 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for all GPUs : 0.000943 kWh. Total GPU Power : 6.495733566081381 W\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for all CPUs : 0.003282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:33] 0.005088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for RAM : 0.000888 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for all GPUs : 0.000969 kWh. Total GPU Power : 6.4565401301696745 W\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for all CPUs : 0.003376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:48] 0.005233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for all GPUs : 0.000996 kWh. Total GPU Power : 6.393105841820408 W\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:03] 0.005378 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for all GPUs : 0.001023 kWh. Total GPU Power : 6.4035975677068695 W\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for all CPUs : 0.003564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:18] 0.005523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for all GPUs : 0.001050 kWh. Total GPU Power : 6.4781400920026675 W\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for all CPUs : 0.003657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:33] 0.005669 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for RAM : 0.000987 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for all GPUs : 0.001077 kWh. Total GPU Power : 6.496102664290374 W\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for all CPUs : 0.003751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:48] 0.005814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:48] 0.002298 g.CO2eq/s mean an estimation of 72.4827834669698 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for RAM : 0.001011 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for all GPUs : 0.001104 kWh. Total GPU Power : 6.46374104577232 W\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for all CPUs : 0.003845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:03] 0.005960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for all GPUs : 0.001131 kWh. Total GPU Power : 6.606632043468878 W\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for all CPUs : 0.003939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:18] 0.006106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for RAM : 0.001060 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for all GPUs : 0.001158 kWh. Total GPU Power : 6.4866241675532805 W\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for all CPUs : 0.004032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:33] 0.006251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for RAM : 0.001085 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for all GPUs : 0.001185 kWh. Total GPU Power : 6.5027233434214144 W\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for all CPUs : 0.004126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:48] 0.006397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for RAM : 0.001110 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for all GPUs : 0.001213 kWh. Total GPU Power : 6.554916672717611 W\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for all CPUs : 0.004220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:03] 0.006542 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for RAM : 0.001134 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for all GPUs : 0.001240 kWh. Total GPU Power : 6.527646700926425 W\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for all CPUs : 0.004314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:18] 0.006688 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for RAM : 0.001159 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for all GPUs : 0.001267 kWh. Total GPU Power : 6.481546780983787 W\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for all CPUs : 0.004407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:33] 0.006833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for RAM : 0.001184 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for all GPUs : 0.001294 kWh. Total GPU Power : 6.40874526639795 W\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for all CPUs : 0.004501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:48] 0.006979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:48] 0.002304 g.CO2eq/s mean an estimation of 72.6612929210557 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for RAM : 0.001208 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for all GPUs : 0.001320 kWh. Total GPU Power : 6.374389951366324 W\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for all CPUs : 0.004595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:03] 0.007124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for RAM : 0.001233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for all GPUs : 0.001347 kWh. Total GPU Power : 6.4210216774052205 W\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for all CPUs : 0.004689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:18] 0.007269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for RAM : 0.001258 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for all GPUs : 0.001373 kWh. Total GPU Power : 6.363513294799653 W\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:33] 0.007414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for RAM : 0.001282 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for all GPUs : 0.001400 kWh. Total GPU Power : 6.39101480226428 W\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for all CPUs : 0.004876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:48] 0.007559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for RAM : 0.001307 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for all GPUs : 0.001427 kWh. Total GPU Power : 6.42744637824993 W\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for all CPUs : 0.004970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:03] 0.007704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for RAM : 0.001332 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for all GPUs : 0.001453 kWh. Total GPU Power : 6.402680367846498 W\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for all CPUs : 0.005064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:18] 0.007849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for RAM : 0.001356 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for all GPUs : 0.001480 kWh. Total GPU Power : 6.3390107544463214 W\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for all CPUs : 0.005158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:33] 0.007994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for all GPUs : 0.001507 kWh. Total GPU Power : 6.437501867490679 W\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for all CPUs : 0.005251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:48] 0.008139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:48] 0.002297 g.CO2eq/s mean an estimation of 72.43416719923835 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for RAM : 0.001406 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for all GPUs : 0.001533 kWh. Total GPU Power : 6.37305454921667 W\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for all CPUs : 0.005345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:03] 0.008284 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for RAM : 0.001430 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for all GPUs : 0.001560 kWh. Total GPU Power : 6.368513900875082 W\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for all CPUs : 0.005439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:18] 0.008429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for RAM : 0.001455 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for all GPUs : 0.001586 kWh. Total GPU Power : 6.35382167554423 W\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for all CPUs : 0.005533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:33] 0.008574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for RAM : 0.001480 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for all GPUs : 0.001613 kWh. Total GPU Power : 6.348543109423314 W\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for all CPUs : 0.005626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:48] 0.008719 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for RAM : 0.001504 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for all GPUs : 0.001639 kWh. Total GPU Power : 6.395766257675997 W\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for all CPUs : 0.005720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:03] 0.008864 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for RAM : 0.001529 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for all GPUs : 0.001667 kWh. Total GPU Power : 6.527584158046202 W\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for all CPUs : 0.005814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:18] 0.009010 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for RAM : 0.001554 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for all GPUs : 0.001693 kWh. Total GPU Power : 6.435429436829883 W\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for all CPUs : 0.005908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:33] 0.009155 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for RAM : 0.001578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for all GPUs : 0.001720 kWh. Total GPU Power : 6.3890925571119 W\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for all CPUs : 0.006001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:48] 0.009300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:48] 0.002297 g.CO2eq/s mean an estimation of 72.44652366831343 kg.CO2eq/year\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ee97865da49eca268f2f3600cce4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:42:03] Energy consumed for RAM : 0.001603 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:03] Energy consumed for all GPUs : 0.001747 kWh. Total GPU Power : 6.445976025908097 W\n",
      "[codecarbon INFO @ 09:42:03] Energy consumed for all CPUs : 0.006095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:03] 0.009445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for RAM : 0.001628 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for all GPUs : 0.001774 kWh. Total GPU Power : 6.39551424337787 W\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for all CPUs : 0.006189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:18] 0.009590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for RAM : 0.001652 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for all GPUs : 0.001800 kWh. Total GPU Power : 6.371289010052819 W\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for all CPUs : 0.006283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:33] 0.009735 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3995645344257355, 'eval_precision': 0.8061402082129911, 'eval_recall': 0.7684906016116206, 'eval_f1': 0.7816456135962535, 'eval_balanced accuracy': 0.7684906016116206, 'eval_runtime': 57.3863, 'eval_samples_per_second': 28.822, 'eval_steps_per_second': 1.812, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.3995645344257355).\n",
      "[codecarbon WARNING @ 09:42:45] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for RAM : 0.001673 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for all GPUs : 0.001823 kWh. Total GPU Power : 6.461851690114686 W\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for all CPUs : 0.006363 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:46] 0.009859 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1015.4997, 'train_samples_per_second': 6.512, 'train_steps_per_second': 0.408, 'train_loss': 0.48747024904702596, 'epoch': 1.0}\n",
      "Estimated total emissions: 0.00234238601529457 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='albert/albert-base-v2', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_albertv2', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a0131",
   "metadata": {},
   "source": [
    "### Further train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:20:41] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 03:20:41] [setup] GPU Tracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:20:42] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 03:20:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 03:20:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 03:20:44] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 03:20:44] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 03:20:44]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 03:20:44]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 03:20:44]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 03:20:44]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 03:20:44]   CPU count: 16\n",
      "[codecarbon INFO @ 03:20:44]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 03:20:44]   GPU count: 1\n",
      "[codecarbon INFO @ 03:20:44]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 03:20:47] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n",
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b576a9b6abbf4eda9e31324ab5352b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a997116fbab540109b4910eddfcbc39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b877ed361944871b1d2102faede09f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed18d1bf1b8c4fd5a17dc139a3dd67d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_5076\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 03:20:50] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, group, data_name, __index_level_0__. If text, group, data_name, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 11,685,122\n",
      "[codecarbon WARNING @ 03:20:50] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200e293955714c76a7c94fc54a7654e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:21:03] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05476910064529865 W\n",
      "[codecarbon INFO @ 03:21:04] Energy consumed for all CPUs : 0.000102 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:04] 0.000128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:18] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01326557583115578 W\n",
      "[codecarbon INFO @ 03:21:19] Energy consumed for all CPUs : 0.000196 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:19] 0.000245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:33] Energy consumed for RAM : 0.000071 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.00856198745888174 W\n",
      "[codecarbon INFO @ 03:21:34] Energy consumed for all CPUs : 0.000290 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:34] 0.000362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:48] Energy consumed for RAM : 0.000094 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.005134550254049503 W\n",
      "[codecarbon INFO @ 03:21:49] Energy consumed for all CPUs : 0.000384 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:49] 0.000478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:03] Energy consumed for RAM : 0.000118 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015097555991787734 W\n",
      "[codecarbon INFO @ 03:22:04] Energy consumed for all CPUs : 0.000478 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:04] 0.000595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:18] Energy consumed for RAM : 0.000141 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.019967565852979735 W\n",
      "[codecarbon INFO @ 03:22:19] Energy consumed for all CPUs : 0.000571 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:19] 0.000712 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:33] Energy consumed for RAM : 0.000164 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:34] Energy consumed for all GPUs : -0.000000 kWh. Total GPU Power : 0.12629734100761658 W\n",
      "[codecarbon INFO @ 03:22:34] Energy consumed for all CPUs : 0.000665 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:34] 0.000828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:48] Energy consumed for RAM : 0.000187 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.15737884085230242 W\n",
      "[codecarbon INFO @ 03:22:49] Energy consumed for all CPUs : 0.000759 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:49] 0.000946 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:49] 0.001851 g.CO2eq/s mean an estimation of 58.360049144619126 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:23:03] Energy consumed for RAM : 0.000210 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0025676719419991394 W\n",
      "[codecarbon INFO @ 03:23:04] Energy consumed for all CPUs : 0.000853 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:04] 0.001063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:18] Energy consumed for RAM : 0.000233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.004709025165574044 W\n",
      "[codecarbon INFO @ 03:23:19] Energy consumed for all CPUs : 0.000947 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:19] 0.001180 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:33] Energy consumed for RAM : 0.000256 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0012842071780674246 W\n",
      "[codecarbon INFO @ 03:23:34] Energy consumed for all CPUs : 0.001040 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:34] 0.001296 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:48] Energy consumed for RAM : 0.000279 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.007342726160133348 W\n",
      "[codecarbon INFO @ 03:23:49] Energy consumed for all CPUs : 0.001134 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:49] 0.001413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:03] Energy consumed for RAM : 0.000302 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.014341946654836736 W\n",
      "[codecarbon INFO @ 03:24:04] Energy consumed for all CPUs : 0.001228 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:04] 0.001530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:18] Energy consumed for RAM : 0.000325 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.019767997655589065 W\n",
      "[codecarbon INFO @ 03:24:19] Energy consumed for all CPUs : 0.001322 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:19] 0.001647 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:33] Energy consumed for RAM : 0.000348 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03075260034840828 W\n",
      "[codecarbon INFO @ 03:24:34] Energy consumed for all CPUs : 0.001415 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:34] 0.001764 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:48] Energy consumed for RAM : 0.000371 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.011200662279648981 W\n",
      "[codecarbon INFO @ 03:24:49] Energy consumed for all CPUs : 0.001509 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:49] 0.001881 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:49] 0.001849 g.CO2eq/s mean an estimation of 58.321182979884746 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:25:03] Energy consumed for RAM : 0.000394 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.002709834292410642 W\n",
      "[codecarbon INFO @ 03:25:04] Energy consumed for all CPUs : 0.001603 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:04] 0.001998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:18] Energy consumed for RAM : 0.000417 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.011073428114562921 W\n",
      "[codecarbon INFO @ 03:25:19] Energy consumed for all CPUs : 0.001697 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:19] 0.002114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:33] Energy consumed for RAM : 0.000440 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003282279828074076 W\n",
      "[codecarbon INFO @ 03:25:34] Energy consumed for all CPUs : 0.001790 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:34] 0.002231 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:48] Energy consumed for RAM : 0.000463 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008204997839982402 W\n",
      "[codecarbon INFO @ 03:25:49] Energy consumed for all CPUs : 0.001884 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:49] 0.002348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:03] Energy consumed for RAM : 0.000486 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01878890437071146 W\n",
      "[codecarbon INFO @ 03:26:04] Energy consumed for all CPUs : 0.001978 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:04] 0.002464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:18] Energy consumed for RAM : 0.000509 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0119078944071514 W\n",
      "[codecarbon INFO @ 03:26:19] Energy consumed for all CPUs : 0.002072 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:19] 0.002581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:33] Energy consumed for RAM : 0.000532 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0024966374422172723 W\n",
      "[codecarbon INFO @ 03:26:34] Energy consumed for all CPUs : 0.002166 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:34] 0.002698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:48] Energy consumed for RAM : 0.000555 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006561960700876752 W\n",
      "[codecarbon INFO @ 03:26:49] Energy consumed for all CPUs : 0.002259 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:49] 0.002815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:49] 0.001849 g.CO2eq/s mean an estimation of 58.322368154316955 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:27:03] Energy consumed for RAM : 0.000578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0157536604509635 W\n",
      "[codecarbon INFO @ 03:27:04] Energy consumed for all CPUs : 0.002353 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:04] 0.002932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:18] Energy consumed for RAM : 0.000601 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.016636047657511994 W\n",
      "[codecarbon INFO @ 03:27:19] Energy consumed for all CPUs : 0.002447 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:19] 0.003049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:33] Energy consumed for RAM : 0.000624 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.040713306560045744 W\n",
      "[codecarbon INFO @ 03:27:34] Energy consumed for all CPUs : 0.002541 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:34] 0.003165 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:48] Energy consumed for RAM : 0.000648 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.009346447458830766 W\n",
      "[codecarbon INFO @ 03:27:49] Energy consumed for all CPUs : 0.002634 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:49] 0.003282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:03] Energy consumed for RAM : 0.000671 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0003566266532119515 W\n",
      "[codecarbon INFO @ 03:28:04] Energy consumed for all CPUs : 0.002728 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:04] 0.003399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:18] Energy consumed for RAM : 0.000694 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0009977772249463008 W\n",
      "[codecarbon INFO @ 03:28:19] Energy consumed for all CPUs : 0.002822 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:19] 0.003516 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:33] Energy consumed for RAM : 0.000717 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 7.126084541989099e-05 W\n",
      "[codecarbon INFO @ 03:28:34] Energy consumed for all CPUs : 0.002916 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:34] 0.003633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:48] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.005059197321221231 W\n",
      "[codecarbon INFO @ 03:28:49] Energy consumed for all CPUs : 0.003009 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:49] 0.003749 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:49] 0.001850 g.CO2eq/s mean an estimation of 58.32971597591838 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:29:03] Energy consumed for RAM : 0.000763 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015542514632553042 W\n",
      "[codecarbon INFO @ 03:29:04] Energy consumed for all CPUs : 0.003103 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:04] 0.003866 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:18] Energy consumed for RAM : 0.000786 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.007770380506603148 W\n",
      "[codecarbon INFO @ 03:29:19] Energy consumed for all CPUs : 0.003197 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:19] 0.003983 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:33] Energy consumed for RAM : 0.000809 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.024693381901322496 W\n",
      "[codecarbon INFO @ 03:29:34] Energy consumed for all CPUs : 0.003291 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:34] 0.004100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:48] Energy consumed for RAM : 0.000832 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008337049036730326 W\n",
      "[codecarbon INFO @ 03:29:49] Energy consumed for all CPUs : 0.003384 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:49] 0.004217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:03] Energy consumed for RAM : 0.000855 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.031142514288380357 W\n",
      "[codecarbon INFO @ 03:30:04] Energy consumed for all CPUs : 0.003478 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:04] 0.004334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:18] Energy consumed for RAM : 0.000878 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.025081216866783374 W\n",
      "[codecarbon INFO @ 03:30:19] Energy consumed for all CPUs : 0.003572 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:19] 0.004450 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:33] Energy consumed for RAM : 0.000901 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.010126891604670233 W\n",
      "[codecarbon INFO @ 03:30:34] Energy consumed for all CPUs : 0.003666 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:34] 0.004567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:48] Energy consumed for RAM : 0.000924 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006139421525319991 W\n",
      "[codecarbon INFO @ 03:30:49] Energy consumed for all CPUs : 0.003760 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:49] 0.004684 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:49] 0.001849 g.CO2eq/s mean an estimation of 58.32537483491613 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:31:03] Energy consumed for RAM : 0.000947 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.014572393679472152 W\n",
      "[codecarbon INFO @ 03:31:04] Energy consumed for all CPUs : 0.003853 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:04] 0.004801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:18] Energy consumed for RAM : 0.000970 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.016962462343185734 W\n",
      "[codecarbon INFO @ 03:31:19] Energy consumed for all CPUs : 0.003947 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:19] 0.004918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:33] Energy consumed for RAM : 0.000993 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.030583753266486303 W\n",
      "[codecarbon INFO @ 03:31:34] Energy consumed for all CPUs : 0.004042 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:34] 0.005035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:48] Energy consumed for RAM : 0.001016 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05204873891917815 W\n",
      "[codecarbon INFO @ 03:31:49] Energy consumed for all CPUs : 0.004135 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:49] 0.005151 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:03] Energy consumed for RAM : 0.001039 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0013575431381136367 W\n",
      "[codecarbon INFO @ 03:32:04] Energy consumed for all CPUs : 0.004229 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:04] 0.005268 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:18] Energy consumed for RAM : 0.001062 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.004813609326774944 W\n",
      "[codecarbon INFO @ 03:32:19] Energy consumed for all CPUs : 0.004325 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:19] 0.005387 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:33] Energy consumed for RAM : 0.001084 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:36] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03604934897615201 W\n",
      "[codecarbon INFO @ 03:32:36] Energy consumed for all CPUs : 0.004426 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:36] 0.005511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:48] Energy consumed for RAM : 0.001105 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003168464249854288 W\n",
      "[codecarbon INFO @ 03:32:49] Energy consumed for all CPUs : 0.004511 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:49] 0.005616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:49] 0.001841 g.CO2eq/s mean an estimation of 58.072872830409864 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:33:03] Energy consumed for RAM : 0.001128 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0016648673012425905 W\n",
      "[codecarbon INFO @ 03:33:04] Energy consumed for all CPUs : 0.004604 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:04] 0.005732 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:18] Energy consumed for RAM : 0.001151 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.009423068966465985 W\n",
      "[codecarbon INFO @ 03:33:19] Energy consumed for all CPUs : 0.004697 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:19] 0.005848 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:33] Energy consumed for RAM : 0.001174 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006987331777983097 W\n",
      "[codecarbon INFO @ 03:33:34] Energy consumed for all CPUs : 0.004791 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:34] 0.005965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:48] Energy consumed for RAM : 0.001197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0007128408633873267 W\n",
      "[codecarbon INFO @ 03:33:49] Energy consumed for all CPUs : 0.004885 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:49] 0.006082 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:03] Energy consumed for RAM : 0.001220 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008848233388466801 W\n",
      "[codecarbon INFO @ 03:34:04] Energy consumed for all CPUs : 0.004979 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:04] 0.006199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:18] Energy consumed for RAM : 0.001243 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0014258108877669717 W\n",
      "[codecarbon INFO @ 03:34:19] Energy consumed for all CPUs : 0.005072 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:19] 0.006316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:33] Energy consumed for RAM : 0.001266 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.013051910387338448 W\n",
      "[codecarbon INFO @ 03:34:34] Energy consumed for all CPUs : 0.005166 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:34] 0.006432 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:48] Energy consumed for RAM : 0.001289 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0004994312695294511 W\n",
      "[codecarbon INFO @ 03:34:49] Energy consumed for all CPUs : 0.005260 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:49] 0.006549 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:49] 0.001849 g.CO2eq/s mean an estimation of 58.32404938623883 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:35:03] Energy consumed for RAM : 0.001312 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.002714543479372454 W\n",
      "[codecarbon INFO @ 03:35:04] Energy consumed for all CPUs : 0.005354 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:04] 0.006666 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:18] Energy consumed for RAM : 0.001335 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.026067548242428795 W\n",
      "[codecarbon INFO @ 03:35:19] Energy consumed for all CPUs : 0.005448 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:19] 0.006783 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:33] Energy consumed for RAM : 0.001358 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02648759235902858 W\n",
      "[codecarbon INFO @ 03:35:34] Energy consumed for all CPUs : 0.005542 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:34] 0.006900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:48] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.00328815559837207 W\n",
      "[codecarbon INFO @ 03:35:49] Energy consumed for all CPUs : 0.005635 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:49] 0.007017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:03] Energy consumed for RAM : 0.001404 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003783659733096631 W\n",
      "[codecarbon INFO @ 03:36:04] Energy consumed for all CPUs : 0.005729 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:04] 0.007133 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:18] Energy consumed for RAM : 0.001427 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01656538528706208 W\n",
      "[codecarbon INFO @ 03:36:19] Energy consumed for all CPUs : 0.005823 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:19] 0.007250 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, group, data_name, __index_level_0__. If text, group, data_name, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13895988d1b43be9798a326bebe8acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:36:33] Energy consumed for RAM : 0.001450 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015655529277242616 W\n",
      "[codecarbon INFO @ 03:36:34] Energy consumed for all CPUs : 0.005917 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:34] 0.007367 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:48] Energy consumed for RAM : 0.001473 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02041016133199112 W\n",
      "[codecarbon INFO @ 03:36:49] Energy consumed for all CPUs : 0.006010 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:49] 0.007484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:49] 0.001849 g.CO2eq/s mean an estimation of 58.30828614842882 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:37:03] Energy consumed for RAM : 0.001496 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:37:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03830154082838678 W\n",
      "[codecarbon INFO @ 03:37:04] Energy consumed for all CPUs : 0.006104 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:04] 0.007601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:37:18] Energy consumed for RAM : 0.001519 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:37:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02080406999919144 W\n",
      "[codecarbon INFO @ 03:37:19] Energy consumed for all CPUs : 0.006198 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:19] 0.007717 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35729289054870605, 'eval_precision': 0.8222906734707581, 'eval_recall': 0.8147742829271492, 'eval_f1': 0.8182760457802902, 'eval_balanced accuracy': 0.8147742829271492, 'eval_runtime': 60.5173, 'eval_samples_per_second': 27.331, 'eval_steps_per_second': 1.719, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.35729289054870605).\n",
      "[codecarbon WARNING @ 03:37:33] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 03:37:33] Energy consumed for RAM : 0.001541 kWh. RAM Power : 5.920974254608154 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1002.6741, 'train_samples_per_second': 6.595, 'train_steps_per_second': 0.413, 'train_loss': 0.3570411424130057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:37:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05883265248076508 W\n",
      "[codecarbon INFO @ 03:37:34] Energy consumed for all CPUs : 0.006288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:34] 0.007829 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0018600549026269544 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2_2epoches\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_albertv2_2epoches', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f878d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "490779c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751b411119d14f8ebfb7ed12abaf3118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a6b529b653431ab040cf8ff7766349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [2, 95, 57, 2691, 151, 564, 88, 12911, 19, 116, 9, 109, 148, 50, 86, 825, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.793578</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.791159</td>\n",
       "      <td>658.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.598266</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.601744</td>\n",
       "      <td>342.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.695922</td>\n",
       "      <td>0.697008</td>\n",
       "      <td>0.696451</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.726781</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726379</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.793578  0.788754  0.791159   658.000\n",
       "1              0.598266  0.605263  0.601744   342.000\n",
       "accuracy       0.726000  0.726000  0.726000     0.726\n",
       "macro avg      0.695922  0.697008  0.696451  1000.000\n",
       "weighted avg   0.726781  0.726000  0.726379  1000.000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained'\n",
    "evaluate_model(test_data_mgsd, model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='mgsd', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb391e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebca1b947b34a1c9e9725eba77866aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda676c950d8466abbae9c2adb28b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [2, 95, 57, 2691, 151, 564, 88, 12911, 19, 116, 9, 109, 148, 50, 86, 825, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.740940</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>658.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.499162</td>\n",
       "      <td>342.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.662627</td>\n",
       "      <td>0.637289</td>\n",
       "      <td>0.643024</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.687374</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.688484</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.740940  0.838906  0.786885   658.000\n",
       "1              0.584314  0.435673  0.499162   342.000\n",
       "accuracy       0.701000  0.701000  0.701000     0.701\n",
       "macro avg      0.662627  0.637289  0.643024  1000.000\n",
       "weighted avg   0.687374  0.701000  0.688484  1000.000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_mgsd, model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='mgsd', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd44108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb71aa14ce564fc392ab4a8cdf032bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd1c972b2542e4b79a3d78f7cb8545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879913</td>\n",
       "      <td>0.883126</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769120</td>\n",
       "      <td>0.763610</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.824516</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.823936</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.842499</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842628</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.879913  0.883126  0.881517  1369.000000\n",
       "1              0.769120  0.763610  0.766355   698.000000\n",
       "accuracy       0.842767  0.842767  0.842767     0.842767\n",
       "macro avg      0.824516  0.823368  0.823936  2067.000000\n",
       "weighted avg   0.842499  0.842767  0.842628  2067.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e8976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217ddf56a7414054958a1408322ba958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cb8f8c270b4ed9a5874f8aaf85cc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922535</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.907421</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.900693</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.912459</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.912367</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.922535  0.949275  0.935714  276.000000\n",
       "1              0.892308  0.840580  0.865672  138.000000\n",
       "accuracy       0.913043  0.913043  0.913043    0.913043\n",
       "macro avg      0.907421  0.894928  0.900693  414.000000\n",
       "weighted avg   0.912459  0.913043  0.912367  414.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f43a01fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d0d2e8ef8b4be58f3002a0508a63d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dd37725c5e49f29d0776f2111bc779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.981651</td>\n",
       "      <td>0.983908</td>\n",
       "      <td>0.982778</td>\n",
       "      <td>435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.974697</td>\n",
       "      <td>0.973605</td>\n",
       "      <td>0.974148</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977016</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.981651  0.983908  0.982778  435.000000\n",
       "1              0.967742  0.963303  0.965517  218.000000\n",
       "accuracy       0.977029  0.977029  0.977029    0.977029\n",
       "macro avg      0.974697  0.973605  0.974148  653.000000\n",
       "weighted avg   0.977008  0.977029  0.977016  653.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db139a101df47708641841b0501a7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140c77a8be045ea92ae753e70958560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.905771</td>\n",
       "      <td>0.865620</td>\n",
       "      <td>1369.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774081</td>\n",
       "      <td>0.633238</td>\n",
       "      <td>0.696612</td>\n",
       "      <td>698.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.81374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.801479</td>\n",
       "      <td>0.769504</td>\n",
       "      <td>0.781116</td>\n",
       "      <td>2067.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.808548</td>\n",
       "      <td>2067.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.828877  0.905771  0.865620  1369.00000\n",
       "1              0.774081  0.633238  0.696612   698.00000\n",
       "accuracy       0.813740  0.813740  0.813740     0.81374\n",
       "macro avg      0.801479  0.769504  0.781116  2067.00000\n",
       "weighted avg   0.810373  0.813740  0.808548  2067.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ed396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d64cbc4e9b45c99ab2e93530c5b1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4302d824def3422a802eed943ef5ee6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.910345</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.932862</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.906785</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.893912</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.907972</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.906895</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.910345  0.956522  0.932862  276.000000\n",
       "1              0.903226  0.811594  0.854962  138.000000\n",
       "accuracy       0.908213  0.908213  0.908213    0.908213\n",
       "macro avg      0.906785  0.884058  0.893912  414.000000\n",
       "weighted avg   0.907972  0.908213  0.906895  414.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503e70d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d8ebaebaae45eaa296990c3bee25e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb22a1ffbcd4354ad66e960066ed4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927473</td>\n",
       "      <td>0.970115</td>\n",
       "      <td>0.948315</td>\n",
       "      <td>435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.889423</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.909369</td>\n",
       "      <td>0.918869</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.929766</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.928654</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.927473  0.970115  0.948315  435.000000\n",
       "1              0.934343  0.848624  0.889423  218.000000\n",
       "accuracy       0.929556  0.929556  0.929556    0.929556\n",
       "macro avg      0.930908  0.909369  0.918869  653.000000\n",
       "weighted avg   0.929766  0.929556  0.928654  653.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='winoqueer_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd8d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef996724c1a482a84c8a621598c3bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855fbd44f6d7498b95c10451a407ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.830872</td>\n",
       "      <td>0.904310</td>\n",
       "      <td>0.866037</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772964</td>\n",
       "      <td>0.638968</td>\n",
       "      <td>0.699608</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.801918</td>\n",
       "      <td>0.771639</td>\n",
       "      <td>0.782822</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.811317</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.809836</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.830872  0.904310  0.866037  1369.000000\n",
       "1              0.772964  0.638968  0.699608   698.000000\n",
       "accuracy       0.814707  0.814707  0.814707     0.814707\n",
       "macro avg      0.801918  0.771639  0.782822  2067.000000\n",
       "weighted avg   0.811317  0.814707  0.809836  2067.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099566b2",
   "metadata": {},
   "source": [
    "### Train a Xlm-R Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 08:46:03] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 08:46:03] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 08:46:03] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 08:46:03] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:05] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 08:46:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 08:46:05]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 08:46:05]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 08:46:05]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 08:46:05]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 08:46:05]   CPU count: 16\n",
      "[codecarbon INFO @ 08:46:05]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 08:46:05]   GPU count: 1\n",
      "[codecarbon INFO @ 08:46:05]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 08:46:08] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdfd2ebe3864b739b6a31ca2a730564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a8a98ab3542e392cbbcca961eb7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:23] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:23] Energy consumed for all GPUs : 0.000025 kWh. Total GPU Power : 5.902435537359482 W\n",
      "[codecarbon INFO @ 08:46:23] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:23] 0.000143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for all GPUs : 0.000050 kWh. Total GPU Power : 6.0019148802554545 W\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:38] 0.000287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for all GPUs : 0.000075 kWh. Total GPU Power : 6.018714102073998 W\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:53] 0.000430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for all GPUs : 0.000100 kWh. Total GPU Power : 6.023766864524901 W\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:08] 0.000574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for RAM : 0.000123 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for all GPUs : 0.000125 kWh. Total GPU Power : 6.032137464614325 W\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:23] 0.000717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for RAM : 0.000148 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for all GPUs : 0.000150 kWh. Total GPU Power : 6.093037727346918 W\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:38] 0.000861 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for RAM : 0.000173 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for all GPUs : 0.000176 kWh. Total GPU Power : 6.13733501573982 W\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for all CPUs : 0.000656 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:53] 0.001005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for RAM : 0.000197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for all GPUs : 0.000202 kWh. Total GPU Power : 6.169179797772329 W\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:08] 0.001149 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:08] 0.002274 g.CO2eq/s mean an estimation of 71.72343093200598 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for RAM : 0.000222 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for all GPUs : 0.000227 kWh. Total GPU Power : 6.11434670432621 W\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:23] 0.001293 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for RAM : 0.000247 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for all GPUs : 0.000252 kWh. Total GPU Power : 6.097779012004364 W\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:38] 0.001437 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for RAM : 0.000271 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for all GPUs : 0.000278 kWh. Total GPU Power : 6.085730357514495 W\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:53] 0.001581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for RAM : 0.000296 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for all GPUs : 0.000303 kWh. Total GPU Power : 6.076182205542703 W\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for all CPUs : 0.001125 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:08] 0.001724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for RAM : 0.000321 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for all GPUs : 0.000328 kWh. Total GPU Power : 6.024677055933245 W\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:23] 0.001868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for RAM : 0.000345 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for all GPUs : 0.000353 kWh. Total GPU Power : 6.030542023482323 W\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:38] 0.002011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for all GPUs : 0.000378 kWh. Total GPU Power : 6.004597308868218 W\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:53] 0.002155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for RAM : 0.000395 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for all GPUs : 0.000403 kWh. Total GPU Power : 5.9870833492969995 W\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for all CPUs : 0.001500 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:08] 0.002298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:08] 0.002274 g.CO2eq/s mean an estimation of 71.72829980068263 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for RAM : 0.000419 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for all GPUs : 0.000428 kWh. Total GPU Power : 5.9485378959322315 W\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:23] 0.002442 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for RAM : 0.000444 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for all GPUs : 0.000453 kWh. Total GPU Power : 5.929093268604776 W\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:38] 0.002585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for RAM : 0.000469 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for all GPUs : 0.000477 kWh. Total GPU Power : 5.793803990333124 W\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:53] 0.002727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for RAM : 0.000493 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for all GPUs : 0.000501 kWh. Total GPU Power : 5.7721313306168645 W\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for all CPUs : 0.001875 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:08] 0.002870 kWh of electricity used since the beginning.\n",
      "loading weights file model.safetensors from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\model.safetensors\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3cc51d20be41408309159ee4a34035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4a00451914abb875b5805ec0bbbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb37cbf459ca45e3810a9097bbb2c5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:51:23] Energy consumed for RAM : 0.000518 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:23] Energy consumed for all GPUs : 0.000526 kWh. Total GPU Power : 5.888676291335624 W\n",
      "[codecarbon INFO @ 08:51:23] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:23] 0.003013 kWh of electricity used since the beginning.\n",
      "loading file sentencepiece.bpe.model from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c6535c49814c8db579b8f07d51d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cf22f68931483ba68c6f0279dc6ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714128d9531042f9a9b920b6f8ce7c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c180fcad285452789d5f79fa8c47201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_20312\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 08:51:28] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 278,045,186\n",
      "[codecarbon WARNING @ 08:51:28] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afbd7ea84744d9da80f634ca337e2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:51:38] Energy consumed for RAM : 0.000543 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:38] Energy consumed for all GPUs : 0.000550 kWh. Total GPU Power : 5.861444046047198 W\n",
      "[codecarbon INFO @ 08:51:38] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:38] 0.003156 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for RAM : 0.000567 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for all GPUs : 0.000575 kWh. Total GPU Power : 6.067289945916921 W\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:53] 0.003300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for RAM : 0.000592 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for all GPUs : 0.000601 kWh. Total GPU Power : 6.23505483343005 W\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:08] 0.003444 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:08] 0.002267 g.CO2eq/s mean an estimation of 71.48677969411824 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for RAM : 0.000617 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for all GPUs : 0.000628 kWh. Total GPU Power : 6.2969162914742824 W\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for all CPUs : 0.002344 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:23] 0.003589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for RAM : 0.000641 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for all GPUs : 0.000654 kWh. Total GPU Power : 6.3618935986613945 W\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for all CPUs : 0.002438 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:38] 0.003733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for RAM : 0.000666 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for all GPUs : 0.000681 kWh. Total GPU Power : 6.402600096713132 W\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:53] 0.003879 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for RAM : 0.000691 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for all GPUs : 0.000708 kWh. Total GPU Power : 6.49106795243007 W\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:08] 0.004024 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for RAM : 0.000715 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for all GPUs : 0.000735 kWh. Total GPU Power : 6.579482834216632 W\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:23] 0.004170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for all GPUs : 0.000763 kWh. Total GPU Power : 6.6303728115775185 W\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for all CPUs : 0.002813 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:38] 0.004316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for RAM : 0.000765 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for all GPUs : 0.000791 kWh. Total GPU Power : 6.715019715311984 W\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for all CPUs : 0.002907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:53] 0.004462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for RAM : 0.000789 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for all GPUs : 0.000819 kWh. Total GPU Power : 6.668322679403866 W\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:08] 0.004609 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:08] 0.002305 g.CO2eq/s mean an estimation of 72.69303070800196 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for RAM : 0.000814 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for all GPUs : 0.000846 kWh. Total GPU Power : 6.5389308631617675 W\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:23] 0.004754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for RAM : 0.000839 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for all GPUs : 0.000873 kWh. Total GPU Power : 6.456750409645287 W\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:38] 0.004900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for RAM : 0.000863 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for all GPUs : 0.000900 kWh. Total GPU Power : 6.473182014101419 W\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for all CPUs : 0.003282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:53] 0.005045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for RAM : 0.000888 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for all GPUs : 0.000927 kWh. Total GPU Power : 6.536225610852232 W\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for all CPUs : 0.003376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:08] 0.005191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for all GPUs : 0.000954 kWh. Total GPU Power : 6.5396001988006125 W\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:23] 0.005336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for all GPUs : 0.000981 kWh. Total GPU Power : 6.512384763442136 W\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for all CPUs : 0.003563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:38] 0.005482 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for all GPUs : 0.001008 kWh. Total GPU Power : 6.485916455162652 W\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for all CPUs : 0.003657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:53] 0.005627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for RAM : 0.000986 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for all GPUs : 0.001035 kWh. Total GPU Power : 6.508372645623242 W\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for all CPUs : 0.003751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:08] 0.005773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:08] 0.002304 g.CO2eq/s mean an estimation of 72.6637621967016 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for RAM : 0.001011 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for all GPUs : 0.001063 kWh. Total GPU Power : 6.505843728805227 W\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for all CPUs : 0.003845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:23] 0.005918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for all GPUs : 0.001090 kWh. Total GPU Power : 6.531308146871879 W\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for all CPUs : 0.003938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:38] 0.006064 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for RAM : 0.001060 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for all GPUs : 0.001117 kWh. Total GPU Power : 6.567430297608812 W\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for all CPUs : 0.004032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:53] 0.006210 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for RAM : 0.001085 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for all GPUs : 0.001145 kWh. Total GPU Power : 6.599890879935969 W\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for all CPUs : 0.004126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:08] 0.006356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for RAM : 0.001110 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for all GPUs : 0.001172 kWh. Total GPU Power : 6.460368559798439 W\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for all CPUs : 0.004220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:23] 0.006501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for RAM : 0.001134 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for all GPUs : 0.001198 kWh. Total GPU Power : 6.433788709662775 W\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for all CPUs : 0.004314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:38] 0.006646 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for RAM : 0.001159 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for all GPUs : 0.001225 kWh. Total GPU Power : 6.464466966163585 W\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for all CPUs : 0.004407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:53] 0.006792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for RAM : 0.001184 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for all GPUs : 0.001252 kWh. Total GPU Power : 6.485319270009119 W\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for all CPUs : 0.004501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:08] 0.006937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:08] 0.002304 g.CO2eq/s mean an estimation of 72.66776088352664 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for RAM : 0.001208 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for all GPUs : 0.001279 kWh. Total GPU Power : 6.509296416901 W\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for all CPUs : 0.004595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:23] 0.007083 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for RAM : 0.001233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for all GPUs : 0.001307 kWh. Total GPU Power : 6.523969446842842 W\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for all CPUs : 0.004689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:38] 0.007228 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for RAM : 0.001258 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for all GPUs : 0.001334 kWh. Total GPU Power : 6.488304779558429 W\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for all CPUs : 0.004782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:53] 0.007374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for RAM : 0.001282 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for all GPUs : 0.001361 kWh. Total GPU Power : 6.456524768620162 W\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for all CPUs : 0.004876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:08] 0.007519 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for RAM : 0.001307 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for all GPUs : 0.001388 kWh. Total GPU Power : 6.5014842883420165 W\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for all CPUs : 0.004970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:23] 0.007665 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for RAM : 0.001332 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for all GPUs : 0.001415 kWh. Total GPU Power : 6.487019347153467 W\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for all CPUs : 0.005064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:38] 0.007810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for RAM : 0.001356 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for all GPUs : 0.001442 kWh. Total GPU Power : 6.509083053685998 W\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for all CPUs : 0.005157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:53] 0.007956 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for all GPUs : 0.001469 kWh. Total GPU Power : 6.5464044600033535 W\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for all CPUs : 0.005251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:08] 0.008101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:08] 0.002304 g.CO2eq/s mean an estimation of 72.65338716186707 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for RAM : 0.001406 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for all GPUs : 0.001496 kWh. Total GPU Power : 6.514620999158129 W\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for all CPUs : 0.005345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:23] 0.008247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for RAM : 0.001430 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for all GPUs : 0.001523 kWh. Total GPU Power : 6.432495857602685 W\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for all CPUs : 0.005439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:38] 0.008392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for RAM : 0.001455 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for all GPUs : 0.001550 kWh. Total GPU Power : 6.4044963117132125 W\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for all CPUs : 0.005532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:53] 0.008537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for RAM : 0.001480 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for all GPUs : 0.001577 kWh. Total GPU Power : 6.445469575090384 W\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for all CPUs : 0.005626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:08] 0.008682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for RAM : 0.001504 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for all GPUs : 0.001604 kWh. Total GPU Power : 6.500826226325576 W\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for all CPUs : 0.005720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:23] 0.008828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for RAM : 0.001529 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for all GPUs : 0.001631 kWh. Total GPU Power : 6.456624826935719 W\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for all CPUs : 0.005814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:38] 0.008973 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for RAM : 0.001554 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for all GPUs : 0.001658 kWh. Total GPU Power : 6.541944325410182 W\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for all CPUs : 0.005907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:53] 0.009119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for RAM : 0.001578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for all GPUs : 0.001685 kWh. Total GPU Power : 6.5006745278711975 W\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for all CPUs : 0.006001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:08] 0.009264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:08] 0.002302 g.CO2eq/s mean an estimation of 72.60537076804808 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for RAM : 0.001603 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for all GPUs : 0.001712 kWh. Total GPU Power : 6.4306894341301835 W\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for all CPUs : 0.006095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:23] 0.009410 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for RAM : 0.001628 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for all GPUs : 0.001739 kWh. Total GPU Power : 6.4548198079546815 W\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for all CPUs : 0.006189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:38] 0.009555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for RAM : 0.001652 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for all GPUs : 0.001765 kWh. Total GPU Power : 6.438624939439415 W\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for all CPUs : 0.006282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:53] 0.009700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for RAM : 0.001677 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for all GPUs : 0.001793 kWh. Total GPU Power : 6.524133587281729 W\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for all CPUs : 0.006376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:08] 0.009846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for RAM : 0.001702 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for all GPUs : 0.001820 kWh. Total GPU Power : 6.539024563533249 W\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for all CPUs : 0.006470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:23] 0.009991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for RAM : 0.001726 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for all GPUs : 0.001847 kWh. Total GPU Power : 6.592891257910618 W\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for all CPUs : 0.006564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:38] 0.010137 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for RAM : 0.001751 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for all GPUs : 0.001875 kWh. Total GPU Power : 6.553652251188814 W\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for all CPUs : 0.006658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:53] 0.010283 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for RAM : 0.001776 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for all GPUs : 0.001902 kWh. Total GPU Power : 6.500344699488218 W\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for all CPUs : 0.006751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:08] 0.010429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:08] 0.002304 g.CO2eq/s mean an estimation of 72.66369478669621 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for RAM : 0.001800 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for all GPUs : 0.001929 kWh. Total GPU Power : 6.4796132883117075 W\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for all CPUs : 0.006845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:23] 0.010574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for RAM : 0.001825 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for all GPUs : 0.001956 kWh. Total GPU Power : 6.503488990781604 W\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for all CPUs : 0.006939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:38] 0.010719 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for RAM : 0.001850 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for all GPUs : 0.001983 kWh. Total GPU Power : 6.567975405958753 W\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for all CPUs : 0.007033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:53] 0.010865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for RAM : 0.001874 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for all GPUs : 0.002011 kWh. Total GPU Power : 6.687676954375361 W\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for all CPUs : 0.007126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:08] 0.011011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for RAM : 0.001899 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for all GPUs : 0.002039 kWh. Total GPU Power : 6.65606725115019 W\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for all CPUs : 0.007220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:23] 0.011158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for RAM : 0.001923 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for all GPUs : 0.002066 kWh. Total GPU Power : 6.5990629094086755 W\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for all CPUs : 0.007314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:38] 0.011304 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for RAM : 0.001948 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for all GPUs : 0.002093 kWh. Total GPU Power : 6.544890691879098 W\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for all CPUs : 0.007408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:53] 0.011449 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for RAM : 0.001973 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for all GPUs : 0.002121 kWh. Total GPU Power : 6.506107497612927 W\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for all CPUs : 0.007501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:08] 0.011595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:08] 0.002308 g.CO2eq/s mean an estimation of 72.79628172144935 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for RAM : 0.001997 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for all GPUs : 0.002148 kWh. Total GPU Power : 6.53858084399573 W\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for all CPUs : 0.007595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:23] 0.011740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for RAM : 0.002022 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for all GPUs : 0.002175 kWh. Total GPU Power : 6.518028563733236 W\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for all CPUs : 0.007689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:38] 0.011886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for RAM : 0.002047 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for all GPUs : 0.002202 kWh. Total GPU Power : 6.575784817016608 W\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for all CPUs : 0.007783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:53] 0.012032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for RAM : 0.002071 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for all GPUs : 0.002230 kWh. Total GPU Power : 6.591722168834508 W\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for all CPUs : 0.007876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:08] 0.012178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for RAM : 0.002096 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for all GPUs : 0.002257 kWh. Total GPU Power : 6.526339474578139 W\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for all CPUs : 0.007970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:23] 0.012323 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for RAM : 0.002121 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for all GPUs : 0.002284 kWh. Total GPU Power : 6.518930901308019 W\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for all CPUs : 0.008064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:38] 0.012469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for RAM : 0.002145 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for all GPUs : 0.002311 kWh. Total GPU Power : 6.522822459337724 W\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for all CPUs : 0.008158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:53] 0.012615 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for RAM : 0.002170 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for all GPUs : 0.002339 kWh. Total GPU Power : 6.600539124571437 W\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for all CPUs : 0.008252 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:08] 0.012761 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:08] 0.002307 g.CO2eq/s mean an estimation of 72.76005427541283 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for RAM : 0.002195 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for all GPUs : 0.002367 kWh. Total GPU Power : 6.6614831128055 W\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for all CPUs : 0.008345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:23] 0.012907 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for RAM : 0.002219 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for all GPUs : 0.002394 kWh. Total GPU Power : 6.6601183451990424 W\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for all CPUs : 0.008439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:38] 0.013053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for RAM : 0.002244 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for all GPUs : 0.002422 kWh. Total GPU Power : 6.516061786025075 W\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for all CPUs : 0.008533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:53] 0.013198 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for RAM : 0.002269 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for all GPUs : 0.002448 kWh. Total GPU Power : 6.460188925596394 W\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for all CPUs : 0.008627 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:08] 0.013344 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for RAM : 0.002293 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for all GPUs : 0.002476 kWh. Total GPU Power : 6.5107200050175775 W\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for all CPUs : 0.008720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:23] 0.013489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for RAM : 0.002318 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for all GPUs : 0.002503 kWh. Total GPU Power : 6.512588938249721 W\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for all CPUs : 0.008814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:38] 0.013635 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for RAM : 0.002343 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for all GPUs : 0.002530 kWh. Total GPU Power : 6.536331518043861 W\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for all CPUs : 0.008908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:53] 0.013781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for RAM : 0.002367 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for all GPUs : 0.002557 kWh. Total GPU Power : 6.520944214898771 W\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for all CPUs : 0.009002 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:08] 0.013926 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:08] 0.002307 g.CO2eq/s mean an estimation of 72.75428478402404 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for RAM : 0.002392 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for all GPUs : 0.002584 kWh. Total GPU Power : 6.396410414155539 W\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for all CPUs : 0.009095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:23] 0.014071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for RAM : 0.002417 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for all GPUs : 0.002610 kWh. Total GPU Power : 6.4086433249520915 W\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for all CPUs : 0.009189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:38] 0.014216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for RAM : 0.002441 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for all GPUs : 0.002637 kWh. Total GPU Power : 6.45799139701892 W\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for all CPUs : 0.009283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:53] 0.014362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for RAM : 0.002466 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for all GPUs : 0.002664 kWh. Total GPU Power : 6.447937045561769 W\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for all CPUs : 0.009377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:08] 0.014507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for RAM : 0.002491 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for all GPUs : 0.002692 kWh. Total GPU Power : 6.54663957665174 W\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for all CPUs : 0.009471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:23] 0.014653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for RAM : 0.002515 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for all GPUs : 0.002718 kWh. Total GPU Power : 6.451929707392118 W\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for all CPUs : 0.009564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:38] 0.014798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for RAM : 0.002540 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for all GPUs : 0.002745 kWh. Total GPU Power : 6.4088738087296955 W\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for all CPUs : 0.009658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:53] 0.014943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for RAM : 0.002565 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for all GPUs : 0.002772 kWh. Total GPU Power : 6.361984359812977 W\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for all CPUs : 0.009752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:08] 0.015088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:08] 0.002300 g.CO2eq/s mean an estimation of 72.51822446720526 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for RAM : 0.002589 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for all GPUs : 0.002799 kWh. Total GPU Power : 6.457752716065107 W\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for all CPUs : 0.009846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:23] 0.015233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for RAM : 0.002614 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for all GPUs : 0.002826 kWh. Total GPU Power : 6.5308945007872 W\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for all CPUs : 0.009939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:38] 0.015379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for RAM : 0.002639 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for all GPUs : 0.002853 kWh. Total GPU Power : 6.484591312169335 W\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for all CPUs : 0.010033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:53] 0.015524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for RAM : 0.002663 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for all GPUs : 0.002880 kWh. Total GPU Power : 6.446573028934487 W\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for all CPUs : 0.010127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:08] 0.015670 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for RAM : 0.002688 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for all GPUs : 0.002906 kWh. Total GPU Power : 6.380314755443399 W\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for all CPUs : 0.010221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:23] 0.015815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for RAM : 0.002713 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for all GPUs : 0.002933 kWh. Total GPU Power : 6.393750950757771 W\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for all CPUs : 0.010314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:38] 0.015960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for RAM : 0.002737 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for all GPUs : 0.002960 kWh. Total GPU Power : 6.441634641090397 W\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for all CPUs : 0.010408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:53] 0.016105 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for RAM : 0.002762 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for all GPUs : 0.002986 kWh. Total GPU Power : 6.434173024276784 W\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for all CPUs : 0.010502 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:08] 0.016250 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:08] 0.002300 g.CO2eq/s mean an estimation of 72.5400842686946 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for RAM : 0.002787 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for all GPUs : 0.003013 kWh. Total GPU Power : 6.4062377592298585 W\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for all CPUs : 0.010596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:23] 0.016395 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for RAM : 0.002811 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for all GPUs : 0.003040 kWh. Total GPU Power : 6.486482405469568 W\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for all CPUs : 0.010689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:38] 0.016541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for RAM : 0.002836 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for all GPUs : 0.003067 kWh. Total GPU Power : 6.517274978887297 W\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for all CPUs : 0.010783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:53] 0.016686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for RAM : 0.002861 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for all GPUs : 0.003095 kWh. Total GPU Power : 6.5457152044954094 W\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for all CPUs : 0.010877 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:08] 0.016832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for RAM : 0.002885 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for all GPUs : 0.003122 kWh. Total GPU Power : 6.491876276512889 W\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for all CPUs : 0.010971 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:23] 0.016978 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for RAM : 0.002910 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for all GPUs : 0.003149 kWh. Total GPU Power : 6.484059105367076 W\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for all CPUs : 0.011064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:38] 0.017123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for RAM : 0.002935 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for all GPUs : 0.003176 kWh. Total GPU Power : 6.5967112092903895 W\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for all CPUs : 0.011158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:53] 0.017269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for RAM : 0.002959 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for all GPUs : 0.003203 kWh. Total GPU Power : 6.55689537267565 W\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for all CPUs : 0.011252 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:08] 0.017415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:08] 0.002305 g.CO2eq/s mean an estimation of 72.6799108022938 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for RAM : 0.002984 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for all GPUs : 0.003231 kWh. Total GPU Power : 6.559187162724073 W\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for all CPUs : 0.011346 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:23] 0.017560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for RAM : 0.003009 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for all GPUs : 0.003258 kWh. Total GPU Power : 6.5132598334060665 W\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for all CPUs : 0.011439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:38] 0.017706 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for RAM : 0.003033 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for all GPUs : 0.003285 kWh. Total GPU Power : 6.534530879514324 W\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for all CPUs : 0.011533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:53] 0.017852 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for RAM : 0.003058 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for all GPUs : 0.003312 kWh. Total GPU Power : 6.494021898468532 W\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for all CPUs : 0.011627 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:08] 0.017997 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3ba2bb29b24a6aa70078c3ad5ffcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:17:23] Energy consumed for RAM : 0.003082 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:24] Energy consumed for all GPUs : 0.003339 kWh. Total GPU Power : 6.414025396971511 W\n",
      "[codecarbon INFO @ 09:17:24] Energy consumed for all CPUs : 0.011721 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:24] 0.018143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for RAM : 0.003107 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for all GPUs : 0.003366 kWh. Total GPU Power : 6.427792622717017 W\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for all CPUs : 0.011815 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:38] 0.018287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for RAM : 0.003132 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for all GPUs : 0.003393 kWh. Total GPU Power : 6.521199986786852 W\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for all CPUs : 0.011908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:53] 0.018433 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4298393726348877, 'eval_precision': 0.7788990450984716, 'eval_recall': 0.7798867110969021, 'eval_f1': 0.7793869030766134, 'eval_balanced accuracy': 0.7798867110969021, 'eval_runtime': 51.6186, 'eval_samples_per_second': 32.043, 'eval_steps_per_second': 2.015, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for RAM : 0.003182 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for all GPUs : 0.003447 kWh. Total GPU Power : 6.357348749607534 W\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for all CPUs : 0.012101 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:24] 0.018730 kWh of electricity used since the beginning.\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:18:24] 0.002302 g.CO2eq/s mean an estimation of 72.58202896567883 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for RAM : 0.003207 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for all GPUs : 0.003472 kWh. Total GPU Power : 6.028873077812276 W\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for all CPUs : 0.012194 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:39] 0.018874 kWh of electricity used since the beginning.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.4298393726348877).\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for RAM : 0.003232 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for all GPUs : 0.003497 kWh. Total GPU Power : 6.015433244286016 W\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for all CPUs : 0.012288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:54] 0.019017 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 09:18:55] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1646.8074, 'train_samples_per_second': 4.016, 'train_steps_per_second': 0.251, 'train_loss': 0.543231945682839, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for RAM : 0.003278 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all GPUs : 0.003544 kWh. Total GPU Power : 5.975447951952986 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all CPUs : 0.012463 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:19:22] 0.019285 kWh of electricity used since the beginning.\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for RAM : 0.003278 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all GPUs : 0.003544 kWh. Total GPU Power : 2.6820229055687137 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all CPUs : 0.012464 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:19:22] 0.019286 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.00458208868225132 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='xlm-roberta-base', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_xlm-roberta-base', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd83b5",
   "metadata": {},
   "source": [
    "### Further train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0afc506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:06:05] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 02:06:05] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 02:06:05] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 02:06:05] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 02:06:05] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:06:07] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 02:06:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 02:06:07]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 02:06:07]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 02:06:07]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 02:06:07]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 02:06:07]   CPU count: 16\n",
      "[codecarbon INFO @ 02:06:07]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 02:06:07]   GPU count: 1\n",
      "[codecarbon INFO @ 02:06:07]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 02:06:10] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n",
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85598940fda248889176b7c8d35c5a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c9871604294a9e92637d3f5aeff623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07827072e3f44821adaf02a039770d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1a1a03a4e84120b8aed8a9613531a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_10460\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 02:06:15] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, data_name, group, text. If __index_level_0__, data_name, group, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 278,045,186\n",
      "[codecarbon WARNING @ 02:06:15] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9944d812aa411fb11a83756c3391e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:06:25] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:06:25] Energy consumed for all GPUs : 0.000023 kWh. Total GPU Power : 5.542185755204544 W\n",
      "[codecarbon INFO @ 02:06:25] Energy consumed for all CPUs : 0.000095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:06:25] 0.000143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:06:40] Energy consumed for RAM : 0.000050 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:06:41] Energy consumed for all GPUs : 0.000047 kWh. Total GPU Power : 5.65728271772489 W\n",
      "[codecarbon INFO @ 02:06:41] Energy consumed for all CPUs : 0.000189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:06:41] 0.000285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:06:56] Energy consumed for RAM : 0.000075 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:06:56] Energy consumed for all GPUs : 0.000072 kWh. Total GPU Power : 5.862041159028471 W\n",
      "[codecarbon INFO @ 02:06:56] Energy consumed for all CPUs : 0.000284 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:06:56] 0.000430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:07:11] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:07:11] Energy consumed for all GPUs : 0.000097 kWh. Total GPU Power : 6.045718752478734 W\n",
      "[codecarbon INFO @ 02:07:11] Energy consumed for all CPUs : 0.000377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:07:11] 0.000573 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:07:26] Energy consumed for RAM : 0.000124 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:07:26] Energy consumed for all GPUs : 0.000122 kWh. Total GPU Power : 6.099397316529315 W\n",
      "[codecarbon INFO @ 02:07:26] Energy consumed for all CPUs : 0.000471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:07:26] 0.000717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:07:41] Energy consumed for RAM : 0.000148 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:07:41] Energy consumed for all GPUs : 0.000156 kWh. Total GPU Power : 8.209268348240975 W\n",
      "[codecarbon INFO @ 02:07:41] Energy consumed for all CPUs : 0.000565 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:07:41] 0.000869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:07:56] Energy consumed for RAM : 0.000173 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:07:56] Energy consumed for all GPUs : 0.000193 kWh. Total GPU Power : 8.767927683884626 W\n",
      "[codecarbon INFO @ 02:07:56] Energy consumed for all CPUs : 0.000659 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:07:56] 0.001025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:08:11] Energy consumed for RAM : 0.000198 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:08:11] Energy consumed for all GPUs : 0.000229 kWh. Total GPU Power : 8.720646185288127 W\n",
      "[codecarbon INFO @ 02:08:11] Energy consumed for all CPUs : 0.000753 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:08:11] 0.001180 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:08:11] 0.002327 g.CO2eq/s mean an estimation of 73.36968777061426 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:08:26] Energy consumed for RAM : 0.000222 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:08:26] Energy consumed for all GPUs : 0.000264 kWh. Total GPU Power : 8.455261845150483 W\n",
      "[codecarbon INFO @ 02:08:26] Energy consumed for all CPUs : 0.000846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:08:26] 0.001333 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:08:41] Energy consumed for RAM : 0.000247 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:08:41] Energy consumed for all GPUs : 0.000293 kWh. Total GPU Power : 6.863619231431254 W\n",
      "[codecarbon INFO @ 02:08:41] Energy consumed for all CPUs : 0.000940 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:08:41] 0.001480 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:08:56] Energy consumed for RAM : 0.000272 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:08:56] Energy consumed for all GPUs : 0.000321 kWh. Total GPU Power : 6.803610839850855 W\n",
      "[codecarbon INFO @ 02:08:56] Energy consumed for all CPUs : 0.001034 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:08:56] 0.001627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:09:11] Energy consumed for RAM : 0.000296 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:09:11] Energy consumed for all GPUs : 0.000348 kWh. Total GPU Power : 6.418666722276092 W\n",
      "[codecarbon INFO @ 02:09:11] Energy consumed for all CPUs : 0.001128 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:09:11] 0.001772 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:09:26] Energy consumed for RAM : 0.000321 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:09:26] Energy consumed for all GPUs : 0.000375 kWh. Total GPU Power : 6.354235744144717 W\n",
      "[codecarbon INFO @ 02:09:26] Energy consumed for all CPUs : 0.001221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:09:26] 0.001917 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:09:41] Energy consumed for RAM : 0.000346 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:09:41] Energy consumed for all GPUs : 0.000403 kWh. Total GPU Power : 6.753795196124874 W\n",
      "[codecarbon INFO @ 02:09:41] Energy consumed for all CPUs : 0.001315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:09:41] 0.002064 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:09:56] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:09:56] Energy consumed for all GPUs : 0.000435 kWh. Total GPU Power : 7.70892244933352 W\n",
      "[codecarbon INFO @ 02:09:56] Energy consumed for all CPUs : 0.001409 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:09:56] 0.002214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:10:11] Energy consumed for RAM : 0.000395 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:10:11] Energy consumed for all GPUs : 0.000462 kWh. Total GPU Power : 6.442799505950175 W\n",
      "[codecarbon INFO @ 02:10:11] Energy consumed for all CPUs : 0.001503 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:10:11] 0.002359 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:10:11] 0.002335 g.CO2eq/s mean an estimation of 73.64084836575746 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:10:26] Energy consumed for RAM : 0.000420 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:10:26] Energy consumed for all GPUs : 0.000488 kWh. Total GPU Power : 6.398521239540606 W\n",
      "[codecarbon INFO @ 02:10:26] Energy consumed for all CPUs : 0.001597 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:10:26] 0.002505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:10:41] Energy consumed for RAM : 0.000444 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:10:41] Energy consumed for all GPUs : 0.000515 kWh. Total GPU Power : 6.410639875238079 W\n",
      "[codecarbon INFO @ 02:10:41] Energy consumed for all CPUs : 0.001690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:10:41] 0.002650 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:10:56] Energy consumed for RAM : 0.000469 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:10:56] Energy consumed for all GPUs : 0.000542 kWh. Total GPU Power : 6.461396662747261 W\n",
      "[codecarbon INFO @ 02:10:56] Energy consumed for all CPUs : 0.001784 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:10:56] 0.002795 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:11:11] Energy consumed for RAM : 0.000494 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:11:11] Energy consumed for all GPUs : 0.000569 kWh. Total GPU Power : 6.49520972482389 W\n",
      "[codecarbon INFO @ 02:11:11] Energy consumed for all CPUs : 0.001878 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:11:11] 0.002941 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:11:26] Energy consumed for RAM : 0.000518 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:11:26] Energy consumed for all GPUs : 0.000596 kWh. Total GPU Power : 6.5563367815747515 W\n",
      "[codecarbon INFO @ 02:11:26] Energy consumed for all CPUs : 0.001972 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:11:26] 0.003086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:11:41] Energy consumed for RAM : 0.000543 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:11:41] Energy consumed for all GPUs : 0.000624 kWh. Total GPU Power : 6.518260093170112 W\n",
      "[codecarbon INFO @ 02:11:41] Energy consumed for all CPUs : 0.002065 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:11:41] 0.003232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:11:56] Energy consumed for RAM : 0.000568 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:11:56] Energy consumed for all GPUs : 0.000651 kWh. Total GPU Power : 6.5235430886243995 W\n",
      "[codecarbon INFO @ 02:11:56] Energy consumed for all CPUs : 0.002159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:11:56] 0.003377 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:12:11] Energy consumed for RAM : 0.000592 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:12:11] Energy consumed for all GPUs : 0.000678 kWh. Total GPU Power : 6.438357977272813 W\n",
      "[codecarbon INFO @ 02:12:11] Energy consumed for all CPUs : 0.002253 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:12:11] 0.003523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:12:11] 0.002302 g.CO2eq/s mean an estimation of 72.59528814351701 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:12:26] Energy consumed for RAM : 0.000617 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:12:26] Energy consumed for all GPUs : 0.000704 kWh. Total GPU Power : 6.466081037464378 W\n",
      "[codecarbon INFO @ 02:12:26] Energy consumed for all CPUs : 0.002347 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:12:26] 0.003668 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:12:41] Energy consumed for RAM : 0.000641 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:12:41] Energy consumed for all GPUs : 0.000732 kWh. Total GPU Power : 6.549082861776604 W\n",
      "[codecarbon INFO @ 02:12:41] Energy consumed for all CPUs : 0.002440 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:12:41] 0.003814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:12:56] Energy consumed for RAM : 0.000666 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:12:56] Energy consumed for all GPUs : 0.000758 kWh. Total GPU Power : 6.304682340090519 W\n",
      "[codecarbon INFO @ 02:12:56] Energy consumed for all CPUs : 0.002534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:12:56] 0.003958 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:13:11] Energy consumed for RAM : 0.000691 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:13:11] Energy consumed for all GPUs : 0.000785 kWh. Total GPU Power : 6.3547681893997074 W\n",
      "[codecarbon INFO @ 02:13:11] Energy consumed for all CPUs : 0.002628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:13:11] 0.004103 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:13:26] Energy consumed for RAM : 0.000715 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:13:26] Energy consumed for all GPUs : 0.000811 kWh. Total GPU Power : 6.331153871947892 W\n",
      "[codecarbon INFO @ 02:13:26] Energy consumed for all CPUs : 0.002722 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:13:26] 0.004248 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:13:41] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:13:41] Energy consumed for all GPUs : 0.000837 kWh. Total GPU Power : 6.3543777156787975 W\n",
      "[codecarbon INFO @ 02:13:41] Energy consumed for all CPUs : 0.002816 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:13:41] 0.004393 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:13:56] Energy consumed for RAM : 0.000765 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:13:56] Energy consumed for all GPUs : 0.000866 kWh. Total GPU Power : 6.869532455023001 W\n",
      "[codecarbon INFO @ 02:13:56] Energy consumed for all CPUs : 0.002909 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:13:56] 0.004540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:14:11] Energy consumed for RAM : 0.000789 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:14:11] Energy consumed for all GPUs : 0.000894 kWh. Total GPU Power : 6.60798060515607 W\n",
      "[codecarbon INFO @ 02:14:11] Energy consumed for all CPUs : 0.003003 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:14:11] 0.004686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:14:11] 0.002302 g.CO2eq/s mean an estimation of 72.61054520602735 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for RAM : 0.000814 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for all GPUs : 0.000921 kWh. Total GPU Power : 6.529964023466438 W\n",
      "[codecarbon INFO @ 02:14:26] Energy consumed for all CPUs : 0.003097 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:14:26] 0.004832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:14:41] Energy consumed for RAM : 0.000839 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:14:41] Energy consumed for all GPUs : 0.000948 kWh. Total GPU Power : 6.462561783424675 W\n",
      "[codecarbon INFO @ 02:14:41] Energy consumed for all CPUs : 0.003191 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:14:41] 0.004977 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:14:56] Energy consumed for RAM : 0.000863 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:14:56] Energy consumed for all GPUs : 0.000975 kWh. Total GPU Power : 6.4801531235196315 W\n",
      "[codecarbon INFO @ 02:14:56] Energy consumed for all CPUs : 0.003284 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:14:56] 0.005122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:15:11] Energy consumed for RAM : 0.000888 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:15:11] Energy consumed for all GPUs : 0.001002 kWh. Total GPU Power : 6.439423299584377 W\n",
      "[codecarbon INFO @ 02:15:11] Energy consumed for all CPUs : 0.003378 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:15:11] 0.005268 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:15:26] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:15:26] Energy consumed for all GPUs : 0.001029 kWh. Total GPU Power : 6.491806084496064 W\n",
      "[codecarbon INFO @ 02:15:26] Energy consumed for all CPUs : 0.003472 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:15:26] 0.005413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:15:41] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:15:41] Energy consumed for all GPUs : 0.001056 kWh. Total GPU Power : 6.515696911641801 W\n",
      "[codecarbon INFO @ 02:15:41] Energy consumed for all CPUs : 0.003566 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:15:41] 0.005559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:15:56] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:15:56] Energy consumed for all GPUs : 0.001083 kWh. Total GPU Power : 6.497836397584142 W\n",
      "[codecarbon INFO @ 02:15:56] Energy consumed for all CPUs : 0.003659 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:15:56] 0.005704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:16:11] Energy consumed for RAM : 0.000987 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:16:11] Energy consumed for all GPUs : 0.001110 kWh. Total GPU Power : 6.5522054412000506 W\n",
      "[codecarbon INFO @ 02:16:11] Energy consumed for all CPUs : 0.003753 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:16:11] 0.005850 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:16:11] 0.002304 g.CO2eq/s mean an estimation of 72.64348354491811 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:16:26] Energy consumed for RAM : 0.001011 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:16:26] Energy consumed for all GPUs : 0.001137 kWh. Total GPU Power : 6.54067740703483 W\n",
      "[codecarbon INFO @ 02:16:26] Energy consumed for all CPUs : 0.003847 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:16:26] 0.005996 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:16:41] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:16:41] Energy consumed for all GPUs : 0.001165 kWh. Total GPU Power : 6.559282806559793 W\n",
      "[codecarbon INFO @ 02:16:41] Energy consumed for all CPUs : 0.003941 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:16:41] 0.006141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:16:56] Energy consumed for RAM : 0.001061 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:16:56] Energy consumed for all GPUs : 0.001192 kWh. Total GPU Power : 6.56301805661587 W\n",
      "[codecarbon INFO @ 02:16:56] Energy consumed for all CPUs : 0.004034 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:16:56] 0.006287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:17:11] Energy consumed for RAM : 0.001085 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:17:11] Energy consumed for all GPUs : 0.001219 kWh. Total GPU Power : 6.57157211932214 W\n",
      "[codecarbon INFO @ 02:17:11] Energy consumed for all CPUs : 0.004128 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:17:11] 0.006433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:17:26] Energy consumed for RAM : 0.001110 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:17:26] Energy consumed for all GPUs : 0.001246 kWh. Total GPU Power : 6.488787165178725 W\n",
      "[codecarbon INFO @ 02:17:26] Energy consumed for all CPUs : 0.004222 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:17:26] 0.006578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:17:41] Energy consumed for RAM : 0.001135 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:17:41] Energy consumed for all GPUs : 0.001274 kWh. Total GPU Power : 6.531632390565922 W\n",
      "[codecarbon INFO @ 02:17:41] Energy consumed for all CPUs : 0.004316 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:17:41] 0.006724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:17:56] Energy consumed for RAM : 0.001159 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:17:56] Energy consumed for all GPUs : 0.001301 kWh. Total GPU Power : 6.5000912000364846 W\n",
      "[codecarbon INFO @ 02:17:56] Energy consumed for all CPUs : 0.004410 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:17:56] 0.006870 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:18:11] Energy consumed for RAM : 0.001184 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:18:11] Energy consumed for all GPUs : 0.001328 kWh. Total GPU Power : 6.475880239786712 W\n",
      "[codecarbon INFO @ 02:18:11] Energy consumed for all CPUs : 0.004503 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:18:11] 0.007015 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:18:11] 0.002306 g.CO2eq/s mean an estimation of 72.71161569257889 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:18:26] Energy consumed for RAM : 0.001209 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:18:26] Energy consumed for all GPUs : 0.001355 kWh. Total GPU Power : 6.515151649166846 W\n",
      "[codecarbon INFO @ 02:18:26] Energy consumed for all CPUs : 0.004597 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:18:26] 0.007161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:18:41] Energy consumed for RAM : 0.001233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:18:41] Energy consumed for all GPUs : 0.001382 kWh. Total GPU Power : 6.543155791441143 W\n",
      "[codecarbon INFO @ 02:18:41] Energy consumed for all CPUs : 0.004691 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:18:41] 0.007306 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:18:56] Energy consumed for RAM : 0.001258 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:18:56] Energy consumed for all GPUs : 0.001409 kWh. Total GPU Power : 6.558398163981344 W\n",
      "[codecarbon INFO @ 02:18:56] Energy consumed for all CPUs : 0.004785 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:18:56] 0.007452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:19:11] Energy consumed for RAM : 0.001283 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:19:11] Energy consumed for all GPUs : 0.001437 kWh. Total GPU Power : 6.554079894834742 W\n",
      "[codecarbon INFO @ 02:19:11] Energy consumed for all CPUs : 0.004878 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:19:11] 0.007598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:19:26] Energy consumed for RAM : 0.001307 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:19:26] Energy consumed for all GPUs : 0.001464 kWh. Total GPU Power : 6.555211417575785 W\n",
      "[codecarbon INFO @ 02:19:26] Energy consumed for all CPUs : 0.004972 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:19:26] 0.007743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:19:41] Energy consumed for RAM : 0.001332 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:19:41] Energy consumed for all GPUs : 0.001491 kWh. Total GPU Power : 6.561869639499939 W\n",
      "[codecarbon INFO @ 02:19:41] Energy consumed for all CPUs : 0.005066 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:19:41] 0.007889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:19:56] Energy consumed for RAM : 0.001357 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:19:56] Energy consumed for all GPUs : 0.001519 kWh. Total GPU Power : 6.53365408510639 W\n",
      "[codecarbon INFO @ 02:19:56] Energy consumed for all CPUs : 0.005160 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:19:56] 0.008035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:20:11] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:20:11] Energy consumed for all GPUs : 0.001546 kWh. Total GPU Power : 6.555402516295786 W\n",
      "[codecarbon INFO @ 02:20:11] Energy consumed for all CPUs : 0.005253 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:20:11] 0.008180 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:20:11] 0.002307 g.CO2eq/s mean an estimation of 72.74362884752382 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:20:26] Energy consumed for RAM : 0.001406 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:20:26] Energy consumed for all GPUs : 0.001573 kWh. Total GPU Power : 6.591915716962792 W\n",
      "[codecarbon INFO @ 02:20:26] Energy consumed for all CPUs : 0.005347 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:20:26] 0.008327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:20:41] Energy consumed for RAM : 0.001430 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:20:41] Energy consumed for all GPUs : 0.001601 kWh. Total GPU Power : 6.600521953700308 W\n",
      "[codecarbon INFO @ 02:20:41] Energy consumed for all CPUs : 0.005441 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:20:41] 0.008472 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:20:56] Energy consumed for RAM : 0.001455 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:20:56] Energy consumed for all GPUs : 0.001628 kWh. Total GPU Power : 6.553491854561972 W\n",
      "[codecarbon INFO @ 02:20:56] Energy consumed for all CPUs : 0.005535 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:20:56] 0.008618 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:21:11] Energy consumed for RAM : 0.001480 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:21:11] Energy consumed for all GPUs : 0.001656 kWh. Total GPU Power : 6.574107398089107 W\n",
      "[codecarbon INFO @ 02:21:11] Energy consumed for all CPUs : 0.005629 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:21:11] 0.008764 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:21:26] Energy consumed for RAM : 0.001504 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:21:26] Energy consumed for all GPUs : 0.001683 kWh. Total GPU Power : 6.600511568746036 W\n",
      "[codecarbon INFO @ 02:21:26] Energy consumed for all CPUs : 0.005722 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:21:26] 0.008910 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:21:41] Energy consumed for RAM : 0.001529 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:21:41] Energy consumed for all GPUs : 0.001710 kWh. Total GPU Power : 6.569534002194655 W\n",
      "[codecarbon INFO @ 02:21:41] Energy consumed for all CPUs : 0.005816 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:21:41] 0.009056 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:21:56] Energy consumed for RAM : 0.001554 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:21:56] Energy consumed for all GPUs : 0.001738 kWh. Total GPU Power : 6.590596884322282 W\n",
      "[codecarbon INFO @ 02:21:56] Energy consumed for all CPUs : 0.005910 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:21:56] 0.009202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:22:11] Energy consumed for RAM : 0.001578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:22:11] Energy consumed for all GPUs : 0.001766 kWh. Total GPU Power : 6.619067427624622 W\n",
      "[codecarbon INFO @ 02:22:11] Energy consumed for all CPUs : 0.006004 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:22:11] 0.009348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:22:11] 0.002309 g.CO2eq/s mean an estimation of 72.83184155294208 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:22:26] Energy consumed for RAM : 0.001603 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:22:26] Energy consumed for all GPUs : 0.001793 kWh. Total GPU Power : 6.589076068650379 W\n",
      "[codecarbon INFO @ 02:22:26] Energy consumed for all CPUs : 0.006097 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:22:26] 0.009494 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:22:41] Energy consumed for RAM : 0.001628 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:22:41] Energy consumed for all GPUs : 0.001820 kWh. Total GPU Power : 6.605326774859571 W\n",
      "[codecarbon INFO @ 02:22:41] Energy consumed for all CPUs : 0.006191 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:22:41] 0.009639 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:22:56] Energy consumed for RAM : 0.001652 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:22:56] Energy consumed for all GPUs : 0.001848 kWh. Total GPU Power : 6.673166071222502 W\n",
      "[codecarbon INFO @ 02:22:56] Energy consumed for all CPUs : 0.006285 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:22:56] 0.009786 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:23:11] Energy consumed for RAM : 0.001677 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:23:11] Energy consumed for all GPUs : 0.001876 kWh. Total GPU Power : 6.662642616134011 W\n",
      "[codecarbon INFO @ 02:23:11] Energy consumed for all CPUs : 0.006379 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:23:11] 0.009932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:23:26] Energy consumed for RAM : 0.001702 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:23:26] Energy consumed for all GPUs : 0.001904 kWh. Total GPU Power : 6.642018539015036 W\n",
      "[codecarbon INFO @ 02:23:26] Energy consumed for all CPUs : 0.006473 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:23:26] 0.010078 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:23:41] Energy consumed for RAM : 0.001726 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:23:41] Energy consumed for all GPUs : 0.001931 kWh. Total GPU Power : 6.648633988959333 W\n",
      "[codecarbon INFO @ 02:23:41] Energy consumed for all CPUs : 0.006566 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:23:41] 0.010224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:23:56] Energy consumed for RAM : 0.001751 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:23:56] Energy consumed for all GPUs : 0.001959 kWh. Total GPU Power : 6.6269974817411645 W\n",
      "[codecarbon INFO @ 02:23:56] Energy consumed for all CPUs : 0.006660 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:23:56] 0.010370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:24:11] Energy consumed for RAM : 0.001776 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:24:11] Energy consumed for all GPUs : 0.001987 kWh. Total GPU Power : 6.596980750343229 W\n",
      "[codecarbon INFO @ 02:24:11] Energy consumed for all CPUs : 0.006754 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:24:11] 0.010516 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:24:11] 0.002312 g.CO2eq/s mean an estimation of 72.92612385531548 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:24:26] Energy consumed for RAM : 0.001800 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:24:26] Energy consumed for all GPUs : 0.002014 kWh. Total GPU Power : 6.613492535939472 W\n",
      "[codecarbon INFO @ 02:24:26] Energy consumed for all CPUs : 0.006848 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:24:26] 0.010662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:24:41] Energy consumed for RAM : 0.001825 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:24:41] Energy consumed for all GPUs : 0.002041 kWh. Total GPU Power : 6.531666748334573 W\n",
      "[codecarbon INFO @ 02:24:41] Energy consumed for all CPUs : 0.006941 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:24:41] 0.010808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:24:56] Energy consumed for RAM : 0.001850 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:24:56] Energy consumed for all GPUs : 0.002068 kWh. Total GPU Power : 6.527036670162385 W\n",
      "[codecarbon INFO @ 02:24:56] Energy consumed for all CPUs : 0.007035 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:24:56] 0.010953 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:25:11] Energy consumed for RAM : 0.001874 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:25:11] Energy consumed for all GPUs : 0.002096 kWh. Total GPU Power : 6.5787073578463255 W\n",
      "[codecarbon INFO @ 02:25:11] Energy consumed for all CPUs : 0.007129 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:25:11] 0.011099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:25:26] Energy consumed for RAM : 0.001899 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:25:26] Energy consumed for all GPUs : 0.002123 kWh. Total GPU Power : 6.6107057177114426 W\n",
      "[codecarbon INFO @ 02:25:26] Energy consumed for all CPUs : 0.007223 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:25:26] 0.011245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:25:41] Energy consumed for RAM : 0.001924 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:25:41] Energy consumed for all GPUs : 0.002151 kWh. Total GPU Power : 6.585863078079634 W\n",
      "[codecarbon INFO @ 02:25:41] Energy consumed for all CPUs : 0.007316 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:25:41] 0.011391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:25:56] Energy consumed for RAM : 0.001948 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:25:56] Energy consumed for all GPUs : 0.002178 kWh. Total GPU Power : 6.5938262999381845 W\n",
      "[codecarbon INFO @ 02:25:56] Energy consumed for all CPUs : 0.007410 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:25:56] 0.011537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:26:11] Energy consumed for RAM : 0.001973 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:26:11] Energy consumed for all GPUs : 0.002206 kWh. Total GPU Power : 6.598563840236301 W\n",
      "[codecarbon INFO @ 02:26:11] Energy consumed for all CPUs : 0.007504 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:26:11] 0.011683 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:26:11] 0.002309 g.CO2eq/s mean an estimation of 72.82246314111609 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:26:26] Energy consumed for RAM : 0.001998 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:26:26] Energy consumed for all GPUs : 0.002233 kWh. Total GPU Power : 6.563353439644621 W\n",
      "[codecarbon INFO @ 02:26:26] Energy consumed for all CPUs : 0.007598 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:26:26] 0.011829 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:26:41] Energy consumed for RAM : 0.002022 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:26:41] Energy consumed for all GPUs : 0.002261 kWh. Total GPU Power : 6.585019725468149 W\n",
      "[codecarbon INFO @ 02:26:41] Energy consumed for all CPUs : 0.007692 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:26:41] 0.011975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:26:56] Energy consumed for RAM : 0.002047 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:26:56] Energy consumed for all GPUs : 0.002288 kWh. Total GPU Power : 6.60295402842522 W\n",
      "[codecarbon INFO @ 02:26:56] Energy consumed for all CPUs : 0.007785 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:26:56] 0.012121 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:27:11] Energy consumed for RAM : 0.002072 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:27:11] Energy consumed for all GPUs : 0.002315 kWh. Total GPU Power : 6.517769231746569 W\n",
      "[codecarbon INFO @ 02:27:11] Energy consumed for all CPUs : 0.007879 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:27:11] 0.012266 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:27:26] Energy consumed for RAM : 0.002096 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:27:26] Energy consumed for all GPUs : 0.002343 kWh. Total GPU Power : 6.525735864725427 W\n",
      "[codecarbon INFO @ 02:27:26] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:27:26] 0.012412 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:27:41] Energy consumed for RAM : 0.002121 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:27:41] Energy consumed for all GPUs : 0.002370 kWh. Total GPU Power : 6.4925066295186005 W\n",
      "[codecarbon INFO @ 02:27:41] Energy consumed for all CPUs : 0.008067 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:27:41] 0.012557 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:27:56] Energy consumed for RAM : 0.002146 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:27:56] Energy consumed for all GPUs : 0.002397 kWh. Total GPU Power : 6.49425746752376 W\n",
      "[codecarbon INFO @ 02:27:56] Energy consumed for all CPUs : 0.008160 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:27:56] 0.012703 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:28:11] Energy consumed for RAM : 0.002170 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:28:11] Energy consumed for all GPUs : 0.002424 kWh. Total GPU Power : 6.47416722043606 W\n",
      "[codecarbon INFO @ 02:28:11] Energy consumed for all CPUs : 0.008254 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:28:11] 0.012848 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:28:11] 0.002306 g.CO2eq/s mean an estimation of 72.7193937499853 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:28:26] Energy consumed for RAM : 0.002195 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:28:26] Energy consumed for all GPUs : 0.002450 kWh. Total GPU Power : 6.452641554268982 W\n",
      "[codecarbon INFO @ 02:28:26] Energy consumed for all CPUs : 0.008348 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:28:26] 0.012993 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:28:41] Energy consumed for RAM : 0.002220 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:28:41] Energy consumed for all GPUs : 0.002477 kWh. Total GPU Power : 6.4025105965883 W\n",
      "[codecarbon INFO @ 02:28:41] Energy consumed for all CPUs : 0.008442 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:28:41] 0.013139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:28:56] Energy consumed for RAM : 0.002244 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:28:56] Energy consumed for all GPUs : 0.002504 kWh. Total GPU Power : 6.48208454913321 W\n",
      "[codecarbon INFO @ 02:28:56] Energy consumed for all CPUs : 0.008535 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:28:56] 0.013284 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:29:11] Energy consumed for RAM : 0.002269 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:29:11] Energy consumed for all GPUs : 0.002531 kWh. Total GPU Power : 6.424391442316427 W\n",
      "[codecarbon INFO @ 02:29:11] Energy consumed for all CPUs : 0.008629 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:29:11] 0.013429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:29:26] Energy consumed for RAM : 0.002294 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:29:26] Energy consumed for all GPUs : 0.002559 kWh. Total GPU Power : 6.8352493983137235 W\n",
      "[codecarbon INFO @ 02:29:26] Energy consumed for all CPUs : 0.008723 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:29:26] 0.013576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:29:41] Energy consumed for RAM : 0.002318 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:29:41] Energy consumed for all GPUs : 0.002587 kWh. Total GPU Power : 6.567914773669474 W\n",
      "[codecarbon INFO @ 02:29:41] Energy consumed for all CPUs : 0.008817 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:29:41] 0.013722 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:29:56] Energy consumed for RAM : 0.002343 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:29:56] Energy consumed for all GPUs : 0.002614 kWh. Total GPU Power : 6.603091852829255 W\n",
      "[codecarbon INFO @ 02:29:56] Energy consumed for all CPUs : 0.008910 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:29:56] 0.013868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:30:11] Energy consumed for RAM : 0.002368 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:30:11] Energy consumed for all GPUs : 0.002642 kWh. Total GPU Power : 6.604144648021633 W\n",
      "[codecarbon INFO @ 02:30:11] Energy consumed for all CPUs : 0.009004 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:30:11] 0.014014 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:30:11] 0.002307 g.CO2eq/s mean an estimation of 72.75055163400255 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:30:26] Energy consumed for RAM : 0.002392 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:30:26] Energy consumed for all GPUs : 0.002669 kWh. Total GPU Power : 6.5817837919480535 W\n",
      "[codecarbon INFO @ 02:30:26] Energy consumed for all CPUs : 0.009098 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:30:26] 0.014159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:30:41] Energy consumed for RAM : 0.002417 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:30:41] Energy consumed for all GPUs : 0.002696 kWh. Total GPU Power : 6.410979033064481 W\n",
      "[codecarbon INFO @ 02:30:41] Energy consumed for all CPUs : 0.009192 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:30:41] 0.014305 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:30:56] Energy consumed for RAM : 0.002442 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:30:56] Energy consumed for all GPUs : 0.002722 kWh. Total GPU Power : 6.317344186361058 W\n",
      "[codecarbon INFO @ 02:30:56] Energy consumed for all CPUs : 0.009286 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:30:56] 0.014449 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:31:11] Energy consumed for RAM : 0.002466 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:31:11] Energy consumed for all GPUs : 0.002749 kWh. Total GPU Power : 6.360296256855632 W\n",
      "[codecarbon INFO @ 02:31:11] Energy consumed for all CPUs : 0.009379 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:31:11] 0.014594 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:31:26] Energy consumed for RAM : 0.002491 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:31:26] Energy consumed for all GPUs : 0.002775 kWh. Total GPU Power : 6.339237915151312 W\n",
      "[codecarbon INFO @ 02:31:26] Energy consumed for all CPUs : 0.009473 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:31:26] 0.014739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:31:41] Energy consumed for RAM : 0.002516 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:31:41] Energy consumed for all GPUs : 0.002802 kWh. Total GPU Power : 6.406828149537257 W\n",
      "[codecarbon INFO @ 02:31:41] Energy consumed for all CPUs : 0.009567 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:31:41] 0.014884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for RAM : 0.002540 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for all GPUs : 0.002829 kWh. Total GPU Power : 6.511519423977752 W\n",
      "[codecarbon INFO @ 02:31:56] Energy consumed for all CPUs : 0.009661 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:31:56] 0.015030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:32:11] Energy consumed for RAM : 0.002565 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:32:11] Energy consumed for all GPUs : 0.002856 kWh. Total GPU Power : 6.533759690382094 W\n",
      "[codecarbon INFO @ 02:32:11] Energy consumed for all CPUs : 0.009754 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:32:11] 0.015175 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:32:11] 0.002299 g.CO2eq/s mean an estimation of 72.50611125616045 kg.CO2eq/year\n",
      "[codecarbon INFO @ 02:32:26] Energy consumed for RAM : 0.002589 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:32:26] Energy consumed for all GPUs : 0.002883 kWh. Total GPU Power : 6.436496747399204 W\n",
      "[codecarbon INFO @ 02:32:26] Energy consumed for all CPUs : 0.009848 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:32:26] 0.015321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:32:41] Energy consumed for RAM : 0.002614 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:32:41] Energy consumed for all GPUs : 0.002910 kWh. Total GPU Power : 6.3628942122101915 W\n",
      "[codecarbon INFO @ 02:32:41] Energy consumed for all CPUs : 0.009942 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:32:41] 0.015466 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:32:56] Energy consumed for RAM : 0.002639 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:32:56] Energy consumed for all GPUs : 0.002937 kWh. Total GPU Power : 6.5017915051166595 W\n",
      "[codecarbon INFO @ 02:32:56] Energy consumed for all CPUs : 0.010036 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:32:56] 0.015611 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:33:11] Energy consumed for RAM : 0.002663 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:33:11] Energy consumed for all GPUs : 0.002964 kWh. Total GPU Power : 6.502384933390523 W\n",
      "[codecarbon INFO @ 02:33:11] Energy consumed for all CPUs : 0.010129 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:33:11] 0.015756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:33:26] Energy consumed for RAM : 0.002688 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:33:26] Energy consumed for all GPUs : 0.002991 kWh. Total GPU Power : 6.577156194431835 W\n",
      "[codecarbon INFO @ 02:33:26] Energy consumed for all CPUs : 0.010223 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:33:26] 0.015902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:33:41] Energy consumed for RAM : 0.002713 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:33:41] Energy consumed for all GPUs : 0.003018 kWh. Total GPU Power : 6.519425159702138 W\n",
      "[codecarbon INFO @ 02:33:41] Energy consumed for all CPUs : 0.010317 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:33:41] 0.016048 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 02:33:56] Energy consumed for RAM : 0.002737 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:33:56] Energy consumed for all GPUs : 0.003046 kWh. Total GPU Power : 6.549826921160827 W\n",
      "[codecarbon INFO @ 02:33:56] Energy consumed for all CPUs : 0.010411 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:33:56] 0.016194 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:34:11] Energy consumed for RAM : 0.002762 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:34:11] Energy consumed for all GPUs : 0.003072 kWh. Total GPU Power : 6.248858970505974 W\n",
      "[codecarbon INFO @ 02:34:11] Energy consumed for all CPUs : 0.010504 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:34:11] 0.016338 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:34:12] 0.002301 g.CO2eq/s mean an estimation of 72.56593613126542 kg.CO2eq/year\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, data_name, group, text. If __index_level_0__, data_name, group, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e074bb12a4949108a919565194d0991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 02:34:26] Energy consumed for RAM : 0.002787 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:34:26] Energy consumed for all GPUs : 0.003098 kWh. Total GPU Power : 6.20864352406524 W\n",
      "[codecarbon INFO @ 02:34:26] Energy consumed for all CPUs : 0.010599 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:34:26] 0.016483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:34:41] Energy consumed for RAM : 0.002811 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:34:41] Energy consumed for all GPUs : 0.003124 kWh. Total GPU Power : 6.28412792980069 W\n",
      "[codecarbon INFO @ 02:34:41] Energy consumed for all CPUs : 0.010692 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:34:41] 0.016627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:34:56] Energy consumed for RAM : 0.002836 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:34:56] Energy consumed for all GPUs : 0.003150 kWh. Total GPU Power : 6.36697354159106 W\n",
      "[codecarbon INFO @ 02:34:56] Energy consumed for all CPUs : 0.010786 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:34:56] 0.016772 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:35:11] Energy consumed for RAM : 0.002860 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:35:11] Energy consumed for all GPUs : 0.003177 kWh. Total GPU Power : 6.45467597949548 W\n",
      "[codecarbon INFO @ 02:35:11] Energy consumed for all CPUs : 0.010879 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:35:11] 0.016917 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4096934199333191, 'eval_precision': 0.8030177273866199, 'eval_recall': 0.8172335210551771, 'eval_f1': 0.8088351447775657, 'eval_balanced accuracy': 0.8172335210551771, 'eval_runtime': 58.9017, 'eval_samples_per_second': 28.081, 'eval_steps_per_second': 1.766, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "[codecarbon INFO @ 02:35:43] Energy consumed for RAM : 0.002913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:35:43] Energy consumed for all GPUs : 0.003234 kWh. Total GPU Power : 6.408724683038252 W\n",
      "[codecarbon INFO @ 02:35:43] Energy consumed for all CPUs : 0.011078 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:35:43] 0.017224 kWh of electricity used since the beginning.\n",
      "tokenizer config file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 02:35:58] Energy consumed for RAM : 0.002937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:35:58] Energy consumed for all GPUs : 0.003259 kWh. Total GPU Power : 6.07182620129253 W\n",
      "[codecarbon INFO @ 02:35:58] Energy consumed for all CPUs : 0.011172 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:35:58] 0.017368 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:36:13] Energy consumed for RAM : 0.002962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:36:13] Energy consumed for all GPUs : 0.003284 kWh. Total GPU Power : 6.066218419991127 W\n",
      "[codecarbon INFO @ 02:36:13] Energy consumed for all CPUs : 0.011266 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:36:13] 0.017512 kWh of electricity used since the beginning.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.4096934199333191).\n",
      "[codecarbon WARNING @ 02:36:25] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1810.2669, 'train_samples_per_second': 3.653, 'train_steps_per_second': 0.229, 'train_loss': 0.37757088481516077, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "tokenizer config file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_2epoches_xlm-roberta-base\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 02:36:27] Energy consumed for RAM : 0.002985 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 02:36:27] Energy consumed for all GPUs : 0.003308 kWh. Total GPU Power : 5.950948918046757 W\n",
      "[codecarbon INFO @ 02:36:27] Energy consumed for all CPUs : 0.011354 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 02:36:27] 0.017647 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 02:36:27] 0.002287 g.CO2eq/s mean an estimation of 72.13525227883767 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.004192742547949654 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_2epoches_xlm-roberta-base\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_2epoches_xlm-roberta-base', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82c8a8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3349f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71965979d359447683d0bf519a1a176a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5779754f0fd04358a64c456e50ff249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [0, 581, 92422, 211190, 90, 3542, 67967, 297, 237, 51, 47215, 73, 3674, 390, 3060, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "0              0.882129  0.847334  0.864382  1369.000000\n",
      "1              0.722074  0.777937  0.748966   698.000000\n",
      "accuracy       0.823899  0.823899  0.823899     0.823899\n",
      "macro avg      0.802102  0.812635  0.806674  2067.000000\n",
      "weighted avg   0.828081  0.823899  0.825407  2067.000000\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b42cc7cf2324bc69b79ba82203d7467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ba2ce797d54b70a1a904c5e4bcc10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [0, 581, 92422, 211190, 90, 3542, 67967, 297, 237, 51, 47215, 73, 3674, 390, 3060, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "0              0.828859  0.793362  0.810722   934.000000\n",
      "1              0.628846  0.681250  0.654000   480.000000\n",
      "accuracy       0.755304  0.755304  0.755304     0.755304\n",
      "macro avg      0.728853  0.737306  0.732361  1414.000000\n",
      "weighted avg   0.760962  0.755304  0.757521  1414.000000\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f324ef3d520a4ee88119f1960dd8b1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b77c6cca274db6aaaf49aacaf5831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', 'input_ids': [0, 2684, 3395, 2750, 621, 33351, 621, 200, 1636, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "0              0.867675  0.839890  0.853556  1093.000000\n",
      "1              0.705882  0.750000  0.727273   560.000000\n",
      "accuracy       0.809437  0.809437  0.809437     0.809437\n",
      "macro avg      0.786779  0.794945  0.790415  1653.000000\n",
      "weighted avg   0.812863  0.809437  0.810774  1653.000000\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018653c745bf4ca8bf46a1274e7a250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9991e3b659a04dc7b76be4a03e347b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [0, 1401, 765, 97384, 5941, 8966, 1672, 116374, 23, 10696, 5, 31384, 3395, 621, 221, 8562, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "0              0.783359  0.758359  0.770656   658.000\n",
      "1              0.561983  0.596491  0.578723   342.000\n",
      "accuracy       0.703000  0.703000  0.703000     0.703\n",
      "macro avg      0.672671  0.677425  0.674690  1000.000\n",
      "weighted avg   0.707649  0.703000  0.705015  1000.000\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42))\n",
    "print(evaluate_model(test_data_merged_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_seegull_gpt_augmentation', \n",
    "               seed=42))\n",
    "print(evaluate_model(test_data_merged_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_gpt_augmentation', \n",
    "               seed=42))\n",
    "print(evaluate_model(test_data_mgsd, \n",
    "               model_output_dir='model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='mgsd', \n",
    "               seed=42))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123df488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ac4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfbfdb0be81404abdb576085fc8b16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef0523439d946ceaf882c4ad43b9c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [0, 581, 92422, 211190, 90, 3542, 67967, 297, 237, 51, 47215, 73, 3674, 390, 3060, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845546</td>\n",
       "      <td>0.859752</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.691977</td>\n",
       "      <td>0.703569</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.780551</td>\n",
       "      <td>0.775864</td>\n",
       "      <td>0.778079</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.802267</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.845546  0.859752  0.852590  1369.000000\n",
       "1              0.715556  0.691977  0.703569   698.000000\n",
       "accuracy       0.803096  0.803096  0.803096     0.803096\n",
       "macro avg      0.780551  0.775864  0.778079  2067.000000\n",
       "weighted avg   0.801650  0.803096  0.802267  2067.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641d0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279b5b31c0e44a0e8bed6d5da0a331b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8e725234e942cc9f1756f052e1faf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [0, 581, 92422, 211190, 90, 3542, 67967, 297, 237, 51, 47215, 73, 3674, 390, 3060, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.789855</td>\n",
       "      <td>0.816916</td>\n",
       "      <td>0.803158</td>\n",
       "      <td>934.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.618304</td>\n",
       "      <td>0.577083</td>\n",
       "      <td>0.596983</td>\n",
       "      <td>480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.735502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.704079</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.700070</td>\n",
       "      <td>1414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.731620</td>\n",
       "      <td>0.735502</td>\n",
       "      <td>0.733169</td>\n",
       "      <td>1414.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.789855  0.816916  0.803158   934.000000\n",
       "1              0.618304  0.577083  0.596983   480.000000\n",
       "accuracy       0.735502  0.735502  0.735502     0.735502\n",
       "macro avg      0.704079  0.697000  0.700070  1414.000000\n",
       "weighted avg   0.731620  0.735502  0.733169  1414.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/merged_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fdec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d936d054fe44b85bfaf4c03a0fbb125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77abe402529443eab542ddc0ed713c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', 'input_ids': [0, 2684, 3395, 2750, 621, 33351, 621, 200, 1636, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831261</td>\n",
       "      <td>0.856359</td>\n",
       "      <td>0.843623</td>\n",
       "      <td>1093.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.702087</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.680773</td>\n",
       "      <td>560.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.790079</td>\n",
       "      <td>0.790079</td>\n",
       "      <td>0.790079</td>\n",
       "      <td>0.790079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.766674</td>\n",
       "      <td>0.758536</td>\n",
       "      <td>0.762198</td>\n",
       "      <td>1653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.790079</td>\n",
       "      <td>0.788453</td>\n",
       "      <td>1653.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.831261  0.856359  0.843623  1093.000000\n",
       "1              0.702087  0.660714  0.680773   560.000000\n",
       "accuracy       0.790079  0.790079  0.790079     0.790079\n",
       "macro avg      0.766674  0.758536  0.762198  1653.000000\n",
       "weighted avg   0.787500  0.790079  0.788453  1653.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/merged_winoqueer_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72284d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c545d35986734289af77f3a7a4f24ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50137dc584c43959c1b5d672bac6075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [0, 1401, 765, 97384, 5941, 8966, 1672, 116374, 23, 10696, 5, 31384, 3395, 621, 221, 8562, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.745714</td>\n",
       "      <td>0.793313</td>\n",
       "      <td>0.768778</td>\n",
       "      <td>658.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.479532</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>342.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.646190</td>\n",
       "      <td>0.636423</td>\n",
       "      <td>0.639841</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.677640</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.680585</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.745714  0.793313  0.768778   658.000\n",
       "1              0.546667  0.479532  0.510903   342.000\n",
       "accuracy       0.686000  0.686000  0.686000     0.686\n",
       "macro avg      0.646190  0.636423  0.639841  1000.000\n",
       "weighted avg   0.677640  0.686000  0.680585  1000.000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_mgsd, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/mgsd', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e8d4b",
   "metadata": {},
   "source": [
    "### Use the Xml-R model to evaluate some Chinese and Japanese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db76269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = 'model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained'   # ä½ çš„æ¨¡åž‹ç›®å½•\n",
    "pipe = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, return_all_scores=True)\n",
    "model_path = 'model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained'\n",
    "pipe_albert = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, return_all_scores=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb95876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.40001264214515686}, {'label': 'LABEL_1', 'score': 0.5999873280525208}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.8098716139793396}, {'label': 'LABEL_1', 'score': 0.19012843072414398}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"è¿™ä¸ªè¯„è®ºå¸¦æœ‰åˆ»æ¿å°è±¡å—ï¼Ÿ\" # Does this review have sterotypes?\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.08588053286075592}, {'label': 'LABEL_1', 'score': 0.9141194224357605}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.624780535697937}, {'label': 'LABEL_1', 'score': 0.3752194344997406}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¤§å­¦ç”Ÿéƒ½å¾ˆç¬¨\" # All undergraduate students are stupid\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b48f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.13456343114376068}, {'label': 'LABEL_1', 'score': 0.8654365539550781}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"æœ‰å¾ˆç¬¨çš„å¤§å­¦ç”Ÿ\" # There are stupid undergraduate students.\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea35db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.24039381742477417}, {'label': 'LABEL_1', 'score': 0.759606122970581}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Stupid undergraduate student exists\"\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.07690759003162384}, {'label': 'LABEL_1', 'score': 0.9230924248695374}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¤§å­¦ç”Ÿã¯å…¨éƒ¨é¦¬é¹¿ã§ã™\" # All undergraduate students are stupid\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.0859021320939064}, {'label': 'LABEL_1', 'score': 0.914097785949707}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"å¤§å­¦ç”Ÿã«ã¯é¦¬é¹¿ãªäººã‚‚ã„ã¾ã™\" # There are stupid undergraduate students.\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38409eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.788849413394928}, {'label': 'LABEL_1', 'score': 0.2111506164073944}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"ä¸­å›½äººéƒ½å¾ˆèªæ˜Ž\" # All Chinese are smart.\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d825a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.7089645862579346}, {'label': 'LABEL_1', 'score': 0.29103535413742065}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.8592768907546997}, {'label': 'LABEL_1', 'score': 0.1407231241464615}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Chinese are all  smart\"\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cfe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.1442098170518875}, {'label': 'LABEL_1', 'score': 0.8557901978492737}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¥³äººå°±æ˜¯ä¸èƒ½å½“æ³•å®˜\" # Woman cannot be judge.\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99e762",
   "metadata": {},
   "source": [
    "### Evaluate two Xml-R models on the Chinese verson of sampled MGSD (300 items, translated by Chatgpt 5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de07fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv('mgsd_zh_dataset.csv', usecols=['text', 'label', 'group'])\n",
    "\n",
    "combined_data['data_name'] = 'mgsd_zh_dataset'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493e37cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          group                          text  label        data_name\n",
      "0          race               å°åº¦å°¼è¥¿äºšæ˜¯ä¸€ä¸ªéžå¸¸å¹²å‡€çš„å›½å®¶      0  mgsd_zh_dataset\n",
      "1          race                   å“¥ä¼¦æ¯”äºšäººæ€§æ ¼å¾ˆå¹³å’Œã€‚      0  mgsd_zh_dataset\n",
      "2          race                     æ³•å›½äººé€šå¸¸éƒ½å¾ˆå¤šæ¯›      0  mgsd_zh_dataset\n",
      "3          race             æˆ‘ä»¬é‡åˆ°çš„æ‰€æœ‰è‹±å›½äººéƒ½éžå¸¸æœ‰ç¤¼è²Œã€‚      0  mgsd_zh_dataset\n",
      "4          race    è¡—å°¾çš„é‚£ä¸ªç”·äººæ˜¯ç´¢é©¬é‡Œäººã€‚ä»–ç§¯æžä¸ºå¥³æ€§äº‰å–å¹³ç­‰æƒåˆ©ã€‚      0  mgsd_zh_dataset\n",
      "..          ...                           ...    ...              ...\n",
      "295        race                 æœ‰å¾ˆå¤šå…³äºŽè¶Šå—çš„æ°”çƒç…§ç‰‡ã€‚      0  mgsd_zh_dataset\n",
      "296      gender              å¥¹å’Œå¥¹çš„ç»§çˆ¶æœ‰ä¸€ç§ä¹¦æœ¬ä¸Šçš„å…³ç³»ã€‚      0  mgsd_zh_dataset\n",
      "297        race    ä¿„ç½—æ–¯äººä¸å–œæ¬¢ä¹Œå…‹å…°äººï¼Œä¸è¿‡ä»–ä»¬å¯¹ä¹Œå…‹å…°äººéžå¸¸å®½å®¹ã€‚      0  mgsd_zh_dataset\n",
      "298  profession  æœ‰å¾ˆå¤šå®žä¹ ç”Ÿä¹Ÿæ˜¯ç ”ç©¶äººå‘˜ã€‚ä»–ä»¬çˆ±è¯„å¤´è®ºè¶³ï¼Œè€Œä¸”è–ªæ°´åä½Žã€‚      0  mgsd_zh_dataset\n",
      "299        race        é‚£ä¸ªä¿„ç½—æ–¯äººæ”¾ä¸‹äº†ä»–çš„æ¯å­ã€‚é‡Œé¢å¹¶æ²¡æœ‰é…’ç²¾ã€‚      0  mgsd_zh_dataset\n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61dfce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2813ce3da7ce43e085dd2bf3f2f8118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd417feeda74406afc5b18c745f5974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'å°åº¦å°¼è¥¿äºšæ˜¯ä¸€ä¸ªéžå¸¸å¹²å‡€çš„å›½å®¶', 'label': 0, 'data_name': 'mgsd_zh_dataset', 'input_ids': [0, 6, 26449, 12379, 3891, 10783, 13663, 4528, 135878, 101400, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.751269</td>\n",
       "      <td>0.730864</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.589465</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.585945</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.627715</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.631353</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.711538  0.751269  0.730864  197.000000\n",
       "1              0.467391  0.417476  0.441026  103.000000\n",
       "accuracy       0.636667  0.636667  0.636667    0.636667\n",
       "macro avg      0.589465  0.584372  0.585945  300.000000\n",
       "weighted avg   0.627715  0.636667  0.631353  300.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(combined_data, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/test_data_mgsd_zh', \n",
    "               dataset_name='test_data_mgsd_zh', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8c48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9d22e1634448279e150ca9b1b410c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fa0dc7f36240bea73829fec7cdd91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'å°åº¦å°¼è¥¿äºšæ˜¯ä¸€ä¸ªéžå¸¸å¹²å‡€çš„å›½å®¶', 'label': 0, 'data_name': 'mgsd_zh_dataset', 'input_ids': [0, 6, 26449, 12379, 3891, 10783, 13663, 4528, 135878, 101400, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.721698</td>\n",
       "      <td>0.776650</td>\n",
       "      <td>0.748166</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.460733</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.656667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.610849</td>\n",
       "      <td>0.601917</td>\n",
       "      <td>0.604450</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.645582</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.649481</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.721698  0.776650  0.748166  197.000000\n",
       "1              0.500000  0.427184  0.460733  103.000000\n",
       "accuracy       0.656667  0.656667  0.656667    0.656667\n",
       "macro avg      0.610849  0.601917  0.604450  300.000000\n",
       "weighted avg   0.645582  0.656667  0.649481  300.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(combined_data, \n",
    "               model_output_dir='model_output_2epoches_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/test_data_mgsd_zh', \n",
    "               dataset_name='test_data_mgsd_zh', \n",
    "               seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
