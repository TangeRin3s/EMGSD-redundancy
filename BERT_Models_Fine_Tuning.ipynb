{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a3f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for loading data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_loader(csv_file_path, labelling_criteria, dataset_name, sample_size, num_examples):\n",
    "    combined_data = pd.read_csv(csv_file_path, usecols=['text', 'label', 'group'])\n",
    "\n",
    "    label2id = {label: (1 if label == labelling_criteria else 0) for label in combined_data['label'].unique()}\n",
    "    combined_data['label'] = combined_data['label'].map(label2id)\n",
    "\n",
    "    combined_data['data_name'] = dataset_name\n",
    "\n",
    "    if sample_size >= len(combined_data):\n",
    "        sampled_data = combined_data\n",
    "    else:\n",
    "        sample_proportion = sample_size / len(combined_data)\n",
    "        sampled_data, _ = train_test_split(combined_data, train_size=sample_proportion, stratify=combined_data['label'],\n",
    "                                           random_state=42)\n",
    "\n",
    "    train_data, test_data = train_test_split(sampled_data, test_size=0.2, random_state=42,\n",
    "                                             stratify=sampled_data['label'])\n",
    "\n",
    "    print(\"First few examples from the training data:\")\n",
    "    print(train_data.head(num_examples))\n",
    "    print(\"First few examples from the testing data:\")\n",
    "    print(test_data.head(num_examples))\n",
    "    print(\"Train data size:\", len(train_data))\n",
    "    print(\"Test data size:\", len(test_data))\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f217849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for merging data\n",
    "def merge_datasets(train_data_candidate, test_data_candidate, train_data_established, test_data_established, num_examples):\n",
    "    merged_train_data = pd.concat([train_data_candidate, train_data_established], ignore_index=True)\n",
    "    merged_test_data = pd.concat([test_data_candidate, test_data_established], ignore_index=True)\n",
    "\n",
    "    print(\"First few examples from merged training data:\")\n",
    "    print(merged_train_data.head(num_examples))\n",
    "    print(\"First few examples from merged testing data:\")\n",
    "    print(merged_test_data.head(num_examples))\n",
    "    print(\"Train data merged size:\", len(merged_train_data))\n",
    "    print(\"Test data merged size:\", len(merged_test_data))\n",
    "\n",
    "    return merged_train_data, merged_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e13efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for fine tuning language model\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abd08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable progress bar and set up logging\n",
    "os.environ[\"HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR\"] = \"1\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "def train_model(train_data, model_path, batch_size, epoch, learning_rate, model_output_base_dir, dataset_name, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    num_labels = len(train_data['label'].unique())\n",
    "    print(f\"Number of unique labels: {num_labels}\")\n",
    "\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    if model_path.startswith(\"gpt\"):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128) #change from 512 to 128\n",
    "\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    tokenized_train = Dataset.from_pandas(train_data).map(tokenize_function, batched=True).map(lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from train:\", tokenized_train[0])\n",
    "    tokenized_val = Dataset.from_pandas(val_data).map(tokenize_function, batched=True).map(lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from validation:\", tokenized_train[0])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"balanced accuracy\": balanced_acc}\n",
    "\n",
    "    model_output_dir = os.path.join(model_output_base_dir, dataset_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir, num_train_epochs=epoch, evaluation_strategy=\"epoch\", learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01,\n",
    "        save_strategy=\"epoch\", load_best_model_at_end=True, save_total_limit=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, tokenizer=tokenizer, train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val, compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_dir)\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"Estimated total emissions: {emissions} kg CO2\")\n",
    "\n",
    "    return model_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d9d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for evaluating the model\n",
    "def evaluate_model(test_data, model_output_dir, result_output_base_dir, dataset_name, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    num_labels = len(test_data['label'].unique())\n",
    "    print(f\"Number of unique labels: {num_labels}\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_output_dir, num_labels=num_labels,\n",
    "                                                               ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "    if model_output_dir.startswith(\"gpt\"):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    tokenized_test = Dataset.from_pandas(test_data).map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': examples['label']})\n",
    "    print(\"Sample tokenized input from test:\", tokenized_test[0])\n",
    "\n",
    "    result_output_dir = os.path.join(result_output_base_dir, dataset_name)\n",
    "    os.makedirs(result_output_dir, exist_ok=True)\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model= model,tokenizer=tokenizer,device=-1)\n",
    "\n",
    "    predictions = pipe(test_data['text'].to_list(), return_all_scores=True)\n",
    "    # è¿™äº›çº¢çº¿æ˜¯ Pylance è¯¯æŠŠ x å½“æˆ Tensorï¼Œè®¤ä¸ºä¸èƒ½ç”¨å­—ç¬¦ä¸²å½“ç´¢å¼• å¯¼è‡´çš„é™æ€ç±»åž‹é”™è¯¯ï¼Œä¸å½±å“å®žé™…è¿è¡Œã€‚ä½ å¯ä»¥é€šè¿‡ç±»åž‹æ³¨è§£ã€# type: ignore æˆ–æ”¹å†™æˆæ˜¾å¼å¾ªçŽ¯æ¥æ¶ˆæŽ‰å®ƒä»¬ã€‚\n",
    "    pred_labels = [int(max(pred, key=lambda x: x['score'])['label'].split('_')[-1]) for pred in predictions]\n",
    "    pred_probs = [max(pred, key=lambda x: x['score'])['score'] for pred in predictions]\n",
    "    y_true = test_data['label'].tolist()\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': test_data['text'],\n",
    "        'predicted_label': pred_labels,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'actual_label': y_true,\n",
    "        'group': test_data['group'],\n",
    "        'dataset_name': test_data['data_name']\n",
    "    })\n",
    "\n",
    "    results_file_path = os.path.join(result_output_dir, \"full_results.csv\")\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "    report = classification_report(y_true,pred_labels,output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    result_file_path = os.path.join(result_output_dir, \"classification_report.csv\")\n",
    "    df_report.to_csv(result_file_path)\n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0807f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "                                                   text  label        group  \\\n",
      "68                            Gay people are survivors.      1          Gay   \n",
      "2269                      Transgender people are doors.      0  Transgender   \n",
      "3208  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "826                           most lesbians are sinful.      1      Lesbian   \n",
      "2452   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                         data_name  \n",
      "68    Winoqueer - GPT Augmentation  \n",
      "2269  Winoqueer - GPT Augmentation  \n",
      "3208  Winoqueer - GPT Augmentation  \n",
      "826   Winoqueer - GPT Augmentation  \n",
      "2452  Winoqueer - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label        group  \\\n",
      "3236                  most people who are Gay are kites      0          Gay   \n",
      "2749            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2223                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "2724  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "1506  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                         data_name  \n",
      "3236  Winoqueer - GPT Augmentation  \n",
      "2749  Winoqueer - GPT Augmentation  \n",
      "2223  Winoqueer - GPT Augmentation  \n",
      "2724  Winoqueer - GPT Augmentation  \n",
      "1506  Winoqueer - GPT Augmentation  \n",
      "Train data size: 2611\n",
      "Test data size: 653\n",
      "First few examples from the training data:\n",
      "                                                   text  label          group  \\\n",
      "1206  The Russian cautious approach to learning affe...      0        Russian   \n",
      "213    The United States tourist was rude to the locals      1  United States   \n",
      "1165  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "76    The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "707     The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                       data_name  \n",
      "1206  SeeGULL - GPT Augmentation  \n",
      "213   SeeGULL - GPT Augmentation  \n",
      "1165  SeeGULL - GPT Augmentation  \n",
      "76    SeeGULL - GPT Augmentation  \n",
      "707   SeeGULL - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label    group  \\\n",
      "840   The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1956  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "348   Barundians are often criticized for their unre...      1  Barundi   \n",
      "1735             The Afghans stars had to beg for music      0  Afghans   \n",
      "1913  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                       data_name  \n",
      "840   SeeGULL - GPT Augmentation  \n",
      "1956  SeeGULL - GPT Augmentation  \n",
      "348   SeeGULL - GPT Augmentation  \n",
      "1735  SeeGULL - GPT Augmentation  \n",
      "1913  SeeGULL - GPT Augmentation  \n",
      "Train data size: 1656\n",
      "Test data size: 414\n",
      "First few examples from the training data:\n",
      "        group                                               text  label  \\\n",
      "45922    race  The Germans export a lot of cars. The brown fo...      0   \n",
      "48482  gender                   The man playing sports is smart.      0   \n",
      "28200    race  Indonesia is home to many different cultures w...      0   \n",
      "24667    race  If you speak in Italian I'm going to try to re...      1   \n",
      "37076    race  The Indian man walked into the office. He was ...      0   \n",
      "\n",
      "      data_name  \n",
      "45922      MGSD  \n",
      "48482      MGSD  \n",
      "28200      MGSD  \n",
      "24667      MGSD  \n",
      "37076      MGSD  \n",
      "First few examples from the testing data:\n",
      "            group                                               text  label  \\\n",
      "43942        race  We have learned many things about Yemen in sch...      0   \n",
      "36077        race  One of Barack Obama's last controversies as pr...      1   \n",
      "37837  profession  You should go see your school's guidance couns...      0   \n",
      "24536        race  South Africans are generally gentle people. Th...      1   \n",
      "6996     religion             Sharia is clearly a very sheet mindset      0   \n",
      "\n",
      "      data_name  \n",
      "43942      MGSD  \n",
      "36077      MGSD  \n",
      "37837      MGSD  \n",
      "24536      MGSD  \n",
      "6996       MGSD  \n",
      "Train data size: 4000\n",
      "Test data size: 1000\n",
      "First few examples from merged training data:\n",
      "                                                text  label        group  \\\n",
      "0                          Gay people are survivors.      1          Gay   \n",
      "1                      Transgender people are doors.      0  Transgender   \n",
      "2  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "3                          most lesbians are sinful.      1      Lesbian   \n",
      "4   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label        group  \\\n",
      "0                  most people who are Gay are kites      0          Gay   \n",
      "1            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "3  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "4  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "Train data merged size: 6611\n",
      "Test data merged size: 1653\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 5656\n",
      "Test data merged size: 1414\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 8267\n",
      "Test data merged size: 2067\n"
     ]
    }
   ],
   "source": [
    "# Load and combine relevant datasets\n",
    "sample_size = 5000\n",
    "train_data_winoqueer_gpt_augmentation, test_data_winoqueer_gpt_augmentation = data_loader(\n",
    "    csv_file_path='Winoqueer - GPT Augmentation.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='Winoqueer - GPT Augmentation', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_seegull_gpt_augmentation, test_data_seegull_gpt_augmentation = data_loader(\n",
    "    csv_file_path='SeeGULL - GPT Augmentation.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='SeeGULL - GPT Augmentation', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_mgsd, test_data_mgsd = data_loader(\n",
    "    csv_file_path='MGSD.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='MGSD', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)\n",
    "train_data_merged_winoqueer_gpt_augmentation, test_data_merged_winoqueer_gpt_augmentation = merge_datasets(train_data_candidate = train_data_winoqueer_gpt_augmentation, test_data_candidate = test_data_winoqueer_gpt_augmentation, train_data_established = train_data_mgsd, test_data_established = test_data_mgsd, num_examples=5)\n",
    "train_data_merged_seegull_gpt_augmentation, test_data_merged_seegull_gpt_augmentation = merge_datasets(train_data_candidate = train_data_seegull_gpt_augmentation, test_data_candidate = test_data_seegull_gpt_augmentation, train_data_established = train_data_mgsd, test_data_established = test_data_mgsd, num_examples=5)\n",
    "train_data_merged_winoqueer_seegull_gpt_augmentation, test_data_merged_winoqueer_seegull_gpt_augmentation = merge_datasets(train_data_candidate = train_data_seegull_gpt_augmentation, test_data_candidate = test_data_seegull_gpt_augmentation, train_data_established = train_data_merged_winoqueer_gpt_augmentation, test_data_established = test_data_merged_winoqueer_gpt_augmentation, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb61d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:25:42] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:25:42] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:25:42] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 09:25:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:25:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:25:44] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:25:44] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:25:44]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 09:25:44]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 09:25:44]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 09:25:44]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 09:25:44]   CPU count: 16\n",
      "[codecarbon INFO @ 09:25:44]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:25:44]   GPU count: 1\n",
      "[codecarbon INFO @ 09:25:44]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 09:25:47] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9f001ec2243ca8257e9d623612ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef34b79dcb2148b596ca5ae43b70bbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4ad1c885c2483d9c1b36e5a8c8a68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b5263537c04fb2abdd7f2151d8cec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_20312\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 09:25:50] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 11,685,122\n",
      "[codecarbon WARNING @ 09:25:50] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d4582006d4692a7650add1107e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:26:02] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:02] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 6.402381008604165 W\n",
      "[codecarbon INFO @ 09:26:02] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:02] 0.000145 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for all GPUs : 0.000054 kWh. Total GPU Power : 6.4442174819425055 W\n",
      "[codecarbon INFO @ 09:26:17] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:17] 0.000291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for all GPUs : 0.000081 kWh. Total GPU Power : 6.50454377489057 W\n",
      "[codecarbon INFO @ 09:26:32] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:32] 0.000436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for all GPUs : 0.000108 kWh. Total GPU Power : 6.5418567248964505 W\n",
      "[codecarbon INFO @ 09:26:47] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:26:47] 0.000582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for RAM : 0.000123 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for all GPUs : 0.000135 kWh. Total GPU Power : 6.487874645850122 W\n",
      "[codecarbon INFO @ 09:27:02] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:02] 0.000727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for RAM : 0.000148 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for all GPUs : 0.000162 kWh. Total GPU Power : 6.425576456762341 W\n",
      "[codecarbon INFO @ 09:27:17] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:17] 0.000873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for RAM : 0.000173 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for all GPUs : 0.000189 kWh. Total GPU Power : 6.4763081658169765 W\n",
      "[codecarbon INFO @ 09:27:32] Energy consumed for all CPUs : 0.000657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:32] 0.001018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for RAM : 0.000197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for all GPUs : 0.000216 kWh. Total GPU Power : 6.442490904602947 W\n",
      "[codecarbon INFO @ 09:27:47] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:27:47] 0.001163 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:27:47] 0.002302 g.CO2eq/s mean an estimation of 72.58674393162772 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for RAM : 0.000222 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for all GPUs : 0.000243 kWh. Total GPU Power : 6.476638815071103 W\n",
      "[codecarbon INFO @ 09:28:02] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:02] 0.001309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for RAM : 0.000247 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for all GPUs : 0.000270 kWh. Total GPU Power : 6.639211615466631 W\n",
      "[codecarbon INFO @ 09:28:17] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:17] 0.001455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for RAM : 0.000271 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for all GPUs : 0.000297 kWh. Total GPU Power : 6.447864475393682 W\n",
      "[codecarbon INFO @ 09:28:32] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:32] 0.001600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for RAM : 0.000296 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for all GPUs : 0.000324 kWh. Total GPU Power : 6.425736436147946 W\n",
      "[codecarbon INFO @ 09:28:47] Energy consumed for all CPUs : 0.001125 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:28:47] 0.001745 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for RAM : 0.000321 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for all GPUs : 0.000351 kWh. Total GPU Power : 6.46757425113245 W\n",
      "[codecarbon INFO @ 09:29:02] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:02] 0.001891 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for RAM : 0.000345 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for all GPUs : 0.000378 kWh. Total GPU Power : 6.525175323068999 W\n",
      "[codecarbon INFO @ 09:29:17] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:17] 0.002036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for all GPUs : 0.000406 kWh. Total GPU Power : 6.614175509128246 W\n",
      "[codecarbon INFO @ 09:29:32] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:32] 0.002182 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for RAM : 0.000395 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for all GPUs : 0.000433 kWh. Total GPU Power : 6.570380121771566 W\n",
      "[codecarbon INFO @ 09:29:47] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:29:47] 0.002328 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:29:47] 0.002305 g.CO2eq/s mean an estimation of 72.69835355107301 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:30:02] Energy consumed for RAM : 0.000419 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:02] Energy consumed for all GPUs : 0.000460 kWh. Total GPU Power : 6.486328633996107 W\n",
      "[codecarbon INFO @ 09:30:03] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:03] 0.002474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:17] Energy consumed for RAM : 0.000444 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:18] Energy consumed for all GPUs : 0.000487 kWh. Total GPU Power : 6.510390228589513 W\n",
      "[codecarbon INFO @ 09:30:18] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:18] 0.002619 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:32] Energy consumed for RAM : 0.000469 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:33] Energy consumed for all GPUs : 0.000514 kWh. Total GPU Power : 6.548652114669231 W\n",
      "[codecarbon INFO @ 09:30:33] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:33] 0.002765 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for RAM : 0.000493 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for all GPUs : 0.000541 kWh. Total GPU Power : 6.477485714829632 W\n",
      "[codecarbon INFO @ 09:30:48] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:30:48] 0.002910 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for RAM : 0.000518 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for all GPUs : 0.000568 kWh. Total GPU Power : 6.434119899537728 W\n",
      "[codecarbon INFO @ 09:31:03] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:03] 0.003056 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for RAM : 0.000543 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for all GPUs : 0.000595 kWh. Total GPU Power : 6.4224979370210455 W\n",
      "[codecarbon INFO @ 09:31:18] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:18] 0.003201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for RAM : 0.000567 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for all GPUs : 0.000622 kWh. Total GPU Power : 6.49562602733083 W\n",
      "[codecarbon INFO @ 09:31:33] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:33] 0.003346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for RAM : 0.000592 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for all GPUs : 0.000649 kWh. Total GPU Power : 6.41803505551125 W\n",
      "[codecarbon INFO @ 09:31:48] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:31:48] 0.003491 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:31:48] 0.002302 g.CO2eq/s mean an estimation of 72.60105180233386 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for RAM : 0.000617 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for all GPUs : 0.000675 kWh. Total GPU Power : 6.34097289671553 W\n",
      "[codecarbon INFO @ 09:32:03] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:03] 0.003636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for RAM : 0.000641 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for all GPUs : 0.000702 kWh. Total GPU Power : 6.374913496734297 W\n",
      "[codecarbon INFO @ 09:32:18] Energy consumed for all CPUs : 0.002438 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:18] 0.003781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for RAM : 0.000666 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for all GPUs : 0.000729 kWh. Total GPU Power : 6.4576288631133085 W\n",
      "[codecarbon INFO @ 09:32:33] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:33] 0.003927 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for RAM : 0.000691 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for all GPUs : 0.000755 kWh. Total GPU Power : 6.406424384299246 W\n",
      "[codecarbon INFO @ 09:32:48] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:32:48] 0.004072 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for RAM : 0.000715 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for all GPUs : 0.000782 kWh. Total GPU Power : 6.3571585155671215 W\n",
      "[codecarbon INFO @ 09:33:03] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:03] 0.004217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for all GPUs : 0.000809 kWh. Total GPU Power : 6.457283677679285 W\n",
      "[codecarbon INFO @ 09:33:18] Energy consumed for all CPUs : 0.002813 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:18] 0.004362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for RAM : 0.000765 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for all GPUs : 0.000836 kWh. Total GPU Power : 6.570296405035192 W\n",
      "[codecarbon INFO @ 09:33:33] Energy consumed for all CPUs : 0.002907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:33] 0.004508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for RAM : 0.000789 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for all GPUs : 0.000863 kWh. Total GPU Power : 6.408915102245532 W\n",
      "[codecarbon INFO @ 09:33:48] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:33:48] 0.004653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:33:48] 0.002299 g.CO2eq/s mean an estimation of 72.49313134636854 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for RAM : 0.000814 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for all GPUs : 0.000889 kWh. Total GPU Power : 6.272031399922387 W\n",
      "[codecarbon INFO @ 09:34:03] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:03] 0.004798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for RAM : 0.000839 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for all GPUs : 0.000915 kWh. Total GPU Power : 6.3457664936051685 W\n",
      "[codecarbon INFO @ 09:34:18] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:18] 0.004943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for RAM : 0.000863 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for all GPUs : 0.000943 kWh. Total GPU Power : 6.495733566081381 W\n",
      "[codecarbon INFO @ 09:34:33] Energy consumed for all CPUs : 0.003282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:33] 0.005088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for RAM : 0.000888 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for all GPUs : 0.000969 kWh. Total GPU Power : 6.4565401301696745 W\n",
      "[codecarbon INFO @ 09:34:48] Energy consumed for all CPUs : 0.003376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:34:48] 0.005233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for all GPUs : 0.000996 kWh. Total GPU Power : 6.393105841820408 W\n",
      "[codecarbon INFO @ 09:35:03] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:03] 0.005378 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for all GPUs : 0.001023 kWh. Total GPU Power : 6.4035975677068695 W\n",
      "[codecarbon INFO @ 09:35:18] Energy consumed for all CPUs : 0.003564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:18] 0.005523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for all GPUs : 0.001050 kWh. Total GPU Power : 6.4781400920026675 W\n",
      "[codecarbon INFO @ 09:35:33] Energy consumed for all CPUs : 0.003657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:33] 0.005669 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for RAM : 0.000987 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for all GPUs : 0.001077 kWh. Total GPU Power : 6.496102664290374 W\n",
      "[codecarbon INFO @ 09:35:48] Energy consumed for all CPUs : 0.003751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:35:48] 0.005814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:35:48] 0.002298 g.CO2eq/s mean an estimation of 72.4827834669698 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for RAM : 0.001011 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for all GPUs : 0.001104 kWh. Total GPU Power : 6.46374104577232 W\n",
      "[codecarbon INFO @ 09:36:03] Energy consumed for all CPUs : 0.003845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:03] 0.005960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for all GPUs : 0.001131 kWh. Total GPU Power : 6.606632043468878 W\n",
      "[codecarbon INFO @ 09:36:18] Energy consumed for all CPUs : 0.003939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:18] 0.006106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for RAM : 0.001060 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for all GPUs : 0.001158 kWh. Total GPU Power : 6.4866241675532805 W\n",
      "[codecarbon INFO @ 09:36:33] Energy consumed for all CPUs : 0.004032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:33] 0.006251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for RAM : 0.001085 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for all GPUs : 0.001185 kWh. Total GPU Power : 6.5027233434214144 W\n",
      "[codecarbon INFO @ 09:36:48] Energy consumed for all CPUs : 0.004126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:36:48] 0.006397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for RAM : 0.001110 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for all GPUs : 0.001213 kWh. Total GPU Power : 6.554916672717611 W\n",
      "[codecarbon INFO @ 09:37:03] Energy consumed for all CPUs : 0.004220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:03] 0.006542 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for RAM : 0.001134 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for all GPUs : 0.001240 kWh. Total GPU Power : 6.527646700926425 W\n",
      "[codecarbon INFO @ 09:37:18] Energy consumed for all CPUs : 0.004314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:18] 0.006688 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for RAM : 0.001159 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for all GPUs : 0.001267 kWh. Total GPU Power : 6.481546780983787 W\n",
      "[codecarbon INFO @ 09:37:33] Energy consumed for all CPUs : 0.004407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:33] 0.006833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for RAM : 0.001184 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for all GPUs : 0.001294 kWh. Total GPU Power : 6.40874526639795 W\n",
      "[codecarbon INFO @ 09:37:48] Energy consumed for all CPUs : 0.004501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:37:48] 0.006979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:37:48] 0.002304 g.CO2eq/s mean an estimation of 72.6612929210557 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for RAM : 0.001208 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for all GPUs : 0.001320 kWh. Total GPU Power : 6.374389951366324 W\n",
      "[codecarbon INFO @ 09:38:03] Energy consumed for all CPUs : 0.004595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:03] 0.007124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for RAM : 0.001233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for all GPUs : 0.001347 kWh. Total GPU Power : 6.4210216774052205 W\n",
      "[codecarbon INFO @ 09:38:18] Energy consumed for all CPUs : 0.004689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:18] 0.007269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for RAM : 0.001258 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for all GPUs : 0.001373 kWh. Total GPU Power : 6.363513294799653 W\n",
      "[codecarbon INFO @ 09:38:33] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:33] 0.007414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for RAM : 0.001282 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for all GPUs : 0.001400 kWh. Total GPU Power : 6.39101480226428 W\n",
      "[codecarbon INFO @ 09:38:48] Energy consumed for all CPUs : 0.004876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:38:48] 0.007559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for RAM : 0.001307 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for all GPUs : 0.001427 kWh. Total GPU Power : 6.42744637824993 W\n",
      "[codecarbon INFO @ 09:39:03] Energy consumed for all CPUs : 0.004970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:03] 0.007704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for RAM : 0.001332 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for all GPUs : 0.001453 kWh. Total GPU Power : 6.402680367846498 W\n",
      "[codecarbon INFO @ 09:39:18] Energy consumed for all CPUs : 0.005064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:18] 0.007849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for RAM : 0.001356 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for all GPUs : 0.001480 kWh. Total GPU Power : 6.3390107544463214 W\n",
      "[codecarbon INFO @ 09:39:33] Energy consumed for all CPUs : 0.005158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:33] 0.007994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for all GPUs : 0.001507 kWh. Total GPU Power : 6.437501867490679 W\n",
      "[codecarbon INFO @ 09:39:48] Energy consumed for all CPUs : 0.005251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:39:48] 0.008139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:39:48] 0.002297 g.CO2eq/s mean an estimation of 72.43416719923835 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for RAM : 0.001406 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for all GPUs : 0.001533 kWh. Total GPU Power : 6.37305454921667 W\n",
      "[codecarbon INFO @ 09:40:03] Energy consumed for all CPUs : 0.005345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:03] 0.008284 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for RAM : 0.001430 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for all GPUs : 0.001560 kWh. Total GPU Power : 6.368513900875082 W\n",
      "[codecarbon INFO @ 09:40:18] Energy consumed for all CPUs : 0.005439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:18] 0.008429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for RAM : 0.001455 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for all GPUs : 0.001586 kWh. Total GPU Power : 6.35382167554423 W\n",
      "[codecarbon INFO @ 09:40:33] Energy consumed for all CPUs : 0.005533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:33] 0.008574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for RAM : 0.001480 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for all GPUs : 0.001613 kWh. Total GPU Power : 6.348543109423314 W\n",
      "[codecarbon INFO @ 09:40:48] Energy consumed for all CPUs : 0.005626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:40:48] 0.008719 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for RAM : 0.001504 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for all GPUs : 0.001639 kWh. Total GPU Power : 6.395766257675997 W\n",
      "[codecarbon INFO @ 09:41:03] Energy consumed for all CPUs : 0.005720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:03] 0.008864 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for RAM : 0.001529 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for all GPUs : 0.001667 kWh. Total GPU Power : 6.527584158046202 W\n",
      "[codecarbon INFO @ 09:41:18] Energy consumed for all CPUs : 0.005814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:18] 0.009010 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for RAM : 0.001554 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for all GPUs : 0.001693 kWh. Total GPU Power : 6.435429436829883 W\n",
      "[codecarbon INFO @ 09:41:33] Energy consumed for all CPUs : 0.005908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:33] 0.009155 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for RAM : 0.001578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for all GPUs : 0.001720 kWh. Total GPU Power : 6.3890925571119 W\n",
      "[codecarbon INFO @ 09:41:48] Energy consumed for all CPUs : 0.006001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:41:48] 0.009300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:41:48] 0.002297 g.CO2eq/s mean an estimation of 72.44652366831343 kg.CO2eq/year\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ee97865da49eca268f2f3600cce4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:42:03] Energy consumed for RAM : 0.001603 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:03] Energy consumed for all GPUs : 0.001747 kWh. Total GPU Power : 6.445976025908097 W\n",
      "[codecarbon INFO @ 09:42:03] Energy consumed for all CPUs : 0.006095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:03] 0.009445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for RAM : 0.001628 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for all GPUs : 0.001774 kWh. Total GPU Power : 6.39551424337787 W\n",
      "[codecarbon INFO @ 09:42:18] Energy consumed for all CPUs : 0.006189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:18] 0.009590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for RAM : 0.001652 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for all GPUs : 0.001800 kWh. Total GPU Power : 6.371289010052819 W\n",
      "[codecarbon INFO @ 09:42:33] Energy consumed for all CPUs : 0.006283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:33] 0.009735 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3995645344257355, 'eval_precision': 0.8061402082129911, 'eval_recall': 0.7684906016116206, 'eval_f1': 0.7816456135962535, 'eval_balanced accuracy': 0.7684906016116206, 'eval_runtime': 57.3863, 'eval_samples_per_second': 28.822, 'eval_steps_per_second': 1.812, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.3995645344257355).\n",
      "[codecarbon WARNING @ 09:42:45] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for RAM : 0.001673 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for all GPUs : 0.001823 kWh. Total GPU Power : 6.461851690114686 W\n",
      "[codecarbon INFO @ 09:42:46] Energy consumed for all CPUs : 0.006363 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:42:46] 0.009859 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1015.4997, 'train_samples_per_second': 6.512, 'train_steps_per_second': 0.408, 'train_loss': 0.48747024904702596, 'epoch': 1.0}\n",
      "Estimated total emissions: 0.00234238601529457 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='albert/albert-base-v2', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_albertv2', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:20:41] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 03:20:41] [setup] GPU Tracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:20:42] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 03:20:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 03:20:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 03:20:44] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 03:20:44] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 03:20:44]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 03:20:44]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 03:20:44]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 03:20:44]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 03:20:44]   CPU count: 16\n",
      "[codecarbon INFO @ 03:20:44]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 03:20:44]   GPU count: 1\n",
      "[codecarbon INFO @ 03:20:44]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 03:20:47] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n",
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b576a9b6abbf4eda9e31324ab5352b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a997116fbab540109b4910eddfcbc39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b877ed361944871b1d2102faede09f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed18d1bf1b8c4fd5a17dc139a3dd67d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_5076\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 03:20:50] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, group, data_name, __index_level_0__. If text, group, data_name, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [2, 151, 16, 82, 2888, 50, 14325, 30, 2758, 25, 11496, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 11,685,122\n",
      "[codecarbon WARNING @ 03:20:50] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200e293955714c76a7c94fc54a7654e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:21:03] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05476910064529865 W\n",
      "[codecarbon INFO @ 03:21:04] Energy consumed for all CPUs : 0.000102 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:04] 0.000128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:18] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01326557583115578 W\n",
      "[codecarbon INFO @ 03:21:19] Energy consumed for all CPUs : 0.000196 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:19] 0.000245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:33] Energy consumed for RAM : 0.000071 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.00856198745888174 W\n",
      "[codecarbon INFO @ 03:21:34] Energy consumed for all CPUs : 0.000290 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:34] 0.000362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:21:48] Energy consumed for RAM : 0.000094 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:21:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.005134550254049503 W\n",
      "[codecarbon INFO @ 03:21:49] Energy consumed for all CPUs : 0.000384 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:21:49] 0.000478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:03] Energy consumed for RAM : 0.000118 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015097555991787734 W\n",
      "[codecarbon INFO @ 03:22:04] Energy consumed for all CPUs : 0.000478 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:04] 0.000595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:18] Energy consumed for RAM : 0.000141 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.019967565852979735 W\n",
      "[codecarbon INFO @ 03:22:19] Energy consumed for all CPUs : 0.000571 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:19] 0.000712 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:33] Energy consumed for RAM : 0.000164 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:34] Energy consumed for all GPUs : -0.000000 kWh. Total GPU Power : 0.12629734100761658 W\n",
      "[codecarbon INFO @ 03:22:34] Energy consumed for all CPUs : 0.000665 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:34] 0.000828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:48] Energy consumed for RAM : 0.000187 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:22:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.15737884085230242 W\n",
      "[codecarbon INFO @ 03:22:49] Energy consumed for all CPUs : 0.000759 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:22:49] 0.000946 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:22:49] 0.001851 g.CO2eq/s mean an estimation of 58.360049144619126 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:23:03] Energy consumed for RAM : 0.000210 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0025676719419991394 W\n",
      "[codecarbon INFO @ 03:23:04] Energy consumed for all CPUs : 0.000853 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:04] 0.001063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:18] Energy consumed for RAM : 0.000233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.004709025165574044 W\n",
      "[codecarbon INFO @ 03:23:19] Energy consumed for all CPUs : 0.000947 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:19] 0.001180 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:33] Energy consumed for RAM : 0.000256 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0012842071780674246 W\n",
      "[codecarbon INFO @ 03:23:34] Energy consumed for all CPUs : 0.001040 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:34] 0.001296 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:23:48] Energy consumed for RAM : 0.000279 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:23:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.007342726160133348 W\n",
      "[codecarbon INFO @ 03:23:49] Energy consumed for all CPUs : 0.001134 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:23:49] 0.001413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:03] Energy consumed for RAM : 0.000302 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.014341946654836736 W\n",
      "[codecarbon INFO @ 03:24:04] Energy consumed for all CPUs : 0.001228 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:04] 0.001530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:18] Energy consumed for RAM : 0.000325 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.019767997655589065 W\n",
      "[codecarbon INFO @ 03:24:19] Energy consumed for all CPUs : 0.001322 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:19] 0.001647 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:33] Energy consumed for RAM : 0.000348 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03075260034840828 W\n",
      "[codecarbon INFO @ 03:24:34] Energy consumed for all CPUs : 0.001415 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:34] 0.001764 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:48] Energy consumed for RAM : 0.000371 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:24:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.011200662279648981 W\n",
      "[codecarbon INFO @ 03:24:49] Energy consumed for all CPUs : 0.001509 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:24:49] 0.001881 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:24:49] 0.001849 g.CO2eq/s mean an estimation of 58.321182979884746 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:25:03] Energy consumed for RAM : 0.000394 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.002709834292410642 W\n",
      "[codecarbon INFO @ 03:25:04] Energy consumed for all CPUs : 0.001603 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:04] 0.001998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:18] Energy consumed for RAM : 0.000417 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.011073428114562921 W\n",
      "[codecarbon INFO @ 03:25:19] Energy consumed for all CPUs : 0.001697 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:19] 0.002114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:33] Energy consumed for RAM : 0.000440 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003282279828074076 W\n",
      "[codecarbon INFO @ 03:25:34] Energy consumed for all CPUs : 0.001790 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:34] 0.002231 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:25:48] Energy consumed for RAM : 0.000463 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:25:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008204997839982402 W\n",
      "[codecarbon INFO @ 03:25:49] Energy consumed for all CPUs : 0.001884 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:25:49] 0.002348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:03] Energy consumed for RAM : 0.000486 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01878890437071146 W\n",
      "[codecarbon INFO @ 03:26:04] Energy consumed for all CPUs : 0.001978 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:04] 0.002464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:18] Energy consumed for RAM : 0.000509 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0119078944071514 W\n",
      "[codecarbon INFO @ 03:26:19] Energy consumed for all CPUs : 0.002072 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:19] 0.002581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:33] Energy consumed for RAM : 0.000532 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0024966374422172723 W\n",
      "[codecarbon INFO @ 03:26:34] Energy consumed for all CPUs : 0.002166 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:34] 0.002698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:48] Energy consumed for RAM : 0.000555 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:26:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006561960700876752 W\n",
      "[codecarbon INFO @ 03:26:49] Energy consumed for all CPUs : 0.002259 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:26:49] 0.002815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:26:49] 0.001849 g.CO2eq/s mean an estimation of 58.322368154316955 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:27:03] Energy consumed for RAM : 0.000578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0157536604509635 W\n",
      "[codecarbon INFO @ 03:27:04] Energy consumed for all CPUs : 0.002353 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:04] 0.002932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:18] Energy consumed for RAM : 0.000601 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.016636047657511994 W\n",
      "[codecarbon INFO @ 03:27:19] Energy consumed for all CPUs : 0.002447 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:19] 0.003049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:33] Energy consumed for RAM : 0.000624 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.040713306560045744 W\n",
      "[codecarbon INFO @ 03:27:34] Energy consumed for all CPUs : 0.002541 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:34] 0.003165 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:27:48] Energy consumed for RAM : 0.000648 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:27:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.009346447458830766 W\n",
      "[codecarbon INFO @ 03:27:49] Energy consumed for all CPUs : 0.002634 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:27:49] 0.003282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:03] Energy consumed for RAM : 0.000671 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0003566266532119515 W\n",
      "[codecarbon INFO @ 03:28:04] Energy consumed for all CPUs : 0.002728 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:04] 0.003399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:18] Energy consumed for RAM : 0.000694 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0009977772249463008 W\n",
      "[codecarbon INFO @ 03:28:19] Energy consumed for all CPUs : 0.002822 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:19] 0.003516 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:33] Energy consumed for RAM : 0.000717 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 7.126084541989099e-05 W\n",
      "[codecarbon INFO @ 03:28:34] Energy consumed for all CPUs : 0.002916 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:34] 0.003633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:48] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:28:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.005059197321221231 W\n",
      "[codecarbon INFO @ 03:28:49] Energy consumed for all CPUs : 0.003009 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:28:49] 0.003749 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:28:49] 0.001850 g.CO2eq/s mean an estimation of 58.32971597591838 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:29:03] Energy consumed for RAM : 0.000763 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015542514632553042 W\n",
      "[codecarbon INFO @ 03:29:04] Energy consumed for all CPUs : 0.003103 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:04] 0.003866 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:18] Energy consumed for RAM : 0.000786 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.007770380506603148 W\n",
      "[codecarbon INFO @ 03:29:19] Energy consumed for all CPUs : 0.003197 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:19] 0.003983 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:33] Energy consumed for RAM : 0.000809 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.024693381901322496 W\n",
      "[codecarbon INFO @ 03:29:34] Energy consumed for all CPUs : 0.003291 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:34] 0.004100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:29:48] Energy consumed for RAM : 0.000832 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:29:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008337049036730326 W\n",
      "[codecarbon INFO @ 03:29:49] Energy consumed for all CPUs : 0.003384 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:29:49] 0.004217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:03] Energy consumed for RAM : 0.000855 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.031142514288380357 W\n",
      "[codecarbon INFO @ 03:30:04] Energy consumed for all CPUs : 0.003478 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:04] 0.004334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:18] Energy consumed for RAM : 0.000878 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.025081216866783374 W\n",
      "[codecarbon INFO @ 03:30:19] Energy consumed for all CPUs : 0.003572 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:19] 0.004450 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:33] Energy consumed for RAM : 0.000901 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.010126891604670233 W\n",
      "[codecarbon INFO @ 03:30:34] Energy consumed for all CPUs : 0.003666 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:34] 0.004567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:48] Energy consumed for RAM : 0.000924 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:30:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006139421525319991 W\n",
      "[codecarbon INFO @ 03:30:49] Energy consumed for all CPUs : 0.003760 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:30:49] 0.004684 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:30:49] 0.001849 g.CO2eq/s mean an estimation of 58.32537483491613 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:31:03] Energy consumed for RAM : 0.000947 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.014572393679472152 W\n",
      "[codecarbon INFO @ 03:31:04] Energy consumed for all CPUs : 0.003853 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:04] 0.004801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:18] Energy consumed for RAM : 0.000970 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.016962462343185734 W\n",
      "[codecarbon INFO @ 03:31:19] Energy consumed for all CPUs : 0.003947 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:19] 0.004918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:33] Energy consumed for RAM : 0.000993 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.030583753266486303 W\n",
      "[codecarbon INFO @ 03:31:34] Energy consumed for all CPUs : 0.004042 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:34] 0.005035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:31:48] Energy consumed for RAM : 0.001016 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:31:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05204873891917815 W\n",
      "[codecarbon INFO @ 03:31:49] Energy consumed for all CPUs : 0.004135 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:31:49] 0.005151 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:03] Energy consumed for RAM : 0.001039 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0013575431381136367 W\n",
      "[codecarbon INFO @ 03:32:04] Energy consumed for all CPUs : 0.004229 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:04] 0.005268 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:18] Energy consumed for RAM : 0.001062 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.004813609326774944 W\n",
      "[codecarbon INFO @ 03:32:19] Energy consumed for all CPUs : 0.004325 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:19] 0.005387 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:33] Energy consumed for RAM : 0.001084 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:36] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03604934897615201 W\n",
      "[codecarbon INFO @ 03:32:36] Energy consumed for all CPUs : 0.004426 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:36] 0.005511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:48] Energy consumed for RAM : 0.001105 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:32:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003168464249854288 W\n",
      "[codecarbon INFO @ 03:32:49] Energy consumed for all CPUs : 0.004511 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:32:49] 0.005616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:32:49] 0.001841 g.CO2eq/s mean an estimation of 58.072872830409864 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:33:03] Energy consumed for RAM : 0.001128 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0016648673012425905 W\n",
      "[codecarbon INFO @ 03:33:04] Energy consumed for all CPUs : 0.004604 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:04] 0.005732 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:18] Energy consumed for RAM : 0.001151 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.009423068966465985 W\n",
      "[codecarbon INFO @ 03:33:19] Energy consumed for all CPUs : 0.004697 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:19] 0.005848 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:33] Energy consumed for RAM : 0.001174 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.006987331777983097 W\n",
      "[codecarbon INFO @ 03:33:34] Energy consumed for all CPUs : 0.004791 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:34] 0.005965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:33:48] Energy consumed for RAM : 0.001197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:33:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0007128408633873267 W\n",
      "[codecarbon INFO @ 03:33:49] Energy consumed for all CPUs : 0.004885 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:33:49] 0.006082 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:03] Energy consumed for RAM : 0.001220 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.008848233388466801 W\n",
      "[codecarbon INFO @ 03:34:04] Energy consumed for all CPUs : 0.004979 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:04] 0.006199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:18] Energy consumed for RAM : 0.001243 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0014258108877669717 W\n",
      "[codecarbon INFO @ 03:34:19] Energy consumed for all CPUs : 0.005072 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:19] 0.006316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:33] Energy consumed for RAM : 0.001266 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.013051910387338448 W\n",
      "[codecarbon INFO @ 03:34:34] Energy consumed for all CPUs : 0.005166 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:34] 0.006432 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:48] Energy consumed for RAM : 0.001289 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:34:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0004994312695294511 W\n",
      "[codecarbon INFO @ 03:34:49] Energy consumed for all CPUs : 0.005260 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:34:49] 0.006549 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:34:49] 0.001849 g.CO2eq/s mean an estimation of 58.32404938623883 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:35:03] Energy consumed for RAM : 0.001312 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.002714543479372454 W\n",
      "[codecarbon INFO @ 03:35:04] Energy consumed for all CPUs : 0.005354 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:04] 0.006666 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:18] Energy consumed for RAM : 0.001335 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.026067548242428795 W\n",
      "[codecarbon INFO @ 03:35:19] Energy consumed for all CPUs : 0.005448 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:19] 0.006783 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:33] Energy consumed for RAM : 0.001358 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02648759235902858 W\n",
      "[codecarbon INFO @ 03:35:34] Energy consumed for all CPUs : 0.005542 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:34] 0.006900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:35:48] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:35:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.00328815559837207 W\n",
      "[codecarbon INFO @ 03:35:49] Energy consumed for all CPUs : 0.005635 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:35:49] 0.007017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:03] Energy consumed for RAM : 0.001404 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.003783659733096631 W\n",
      "[codecarbon INFO @ 03:36:04] Energy consumed for all CPUs : 0.005729 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:04] 0.007133 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:18] Energy consumed for RAM : 0.001427 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.01656538528706208 W\n",
      "[codecarbon INFO @ 03:36:19] Energy consumed for all CPUs : 0.005823 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:19] 0.007250 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, group, data_name, __index_level_0__. If text, group, data_name, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13895988d1b43be9798a326bebe8acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:36:33] Energy consumed for RAM : 0.001450 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.015655529277242616 W\n",
      "[codecarbon INFO @ 03:36:34] Energy consumed for all CPUs : 0.005917 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:34] 0.007367 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:48] Energy consumed for RAM : 0.001473 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:36:49] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02041016133199112 W\n",
      "[codecarbon INFO @ 03:36:49] Energy consumed for all CPUs : 0.006010 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:36:49] 0.007484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:36:49] 0.001849 g.CO2eq/s mean an estimation of 58.30828614842882 kg.CO2eq/year\n",
      "[codecarbon INFO @ 03:37:03] Energy consumed for RAM : 0.001496 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:37:04] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.03830154082838678 W\n",
      "[codecarbon INFO @ 03:37:04] Energy consumed for all CPUs : 0.006104 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:04] 0.007601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 03:37:18] Energy consumed for RAM : 0.001519 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 03:37:19] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.02080406999919144 W\n",
      "[codecarbon INFO @ 03:37:19] Energy consumed for all CPUs : 0.006198 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:19] 0.007717 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35729289054870605, 'eval_precision': 0.8222906734707581, 'eval_recall': 0.8147742829271492, 'eval_f1': 0.8182760457802902, 'eval_balanced accuracy': 0.8147742829271492, 'eval_runtime': 60.5173, 'eval_samples_per_second': 27.331, 'eval_steps_per_second': 1.719, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.35729289054870605).\n",
      "[codecarbon WARNING @ 03:37:33] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model weights saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2_2epoches\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 03:37:33] Energy consumed for RAM : 0.001541 kWh. RAM Power : 5.920974254608154 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1002.6741, 'train_samples_per_second': 6.595, 'train_steps_per_second': 0.413, 'train_loss': 0.3570411424130057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 03:37:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.05883265248076508 W\n",
      "[codecarbon INFO @ 03:37:34] Energy consumed for all CPUs : 0.006288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 03:37:34] 0.007829 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0018600549026269544 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2_2epoches\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_albertv2_2epoches', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "490779c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751b411119d14f8ebfb7ed12abaf3118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a6b529b653431ab040cf8ff7766349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [2, 95, 57, 2691, 151, 564, 88, 12911, 19, 116, 9, 109, 148, 50, 86, 825, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.793578</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.791159</td>\n",
       "      <td>658.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.598266</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.601744</td>\n",
       "      <td>342.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.695922</td>\n",
       "      <td>0.697008</td>\n",
       "      <td>0.696451</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.726781</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.726379</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.793578  0.788754  0.791159   658.000\n",
       "1              0.598266  0.605263  0.601744   342.000\n",
       "accuracy       0.726000  0.726000  0.726000     0.726\n",
       "macro avg      0.695922  0.697008  0.696451  1000.000\n",
       "weighted avg   0.726781  0.726000  0.726379  1000.000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained'\n",
    "evaluate_model(test_data_mgsd, model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='mgsd', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb391e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebca1b947b34a1c9e9725eba77866aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda676c950d8466abbae9c2adb28b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'We have learned many things about Yemen in school. Some people are so kind.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 43942, 'input_ids': [2, 95, 57, 2691, 151, 564, 88, 12911, 19, 116, 9, 109, 148, 50, 86, 825, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.740940</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>658.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.499162</td>\n",
       "      <td>342.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.662627</td>\n",
       "      <td>0.637289</td>\n",
       "      <td>0.643024</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.687374</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.688484</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.740940  0.838906  0.786885   658.000\n",
       "1              0.584314  0.435673  0.499162   342.000\n",
       "accuracy       0.701000  0.701000  0.701000     0.701\n",
       "macro avg      0.662627  0.637289  0.643024  1000.000\n",
       "weighted avg   0.687374  0.701000  0.688484  1000.000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_mgsd, model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='mgsd', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd44108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb71aa14ce564fc392ab4a8cdf032bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd1c972b2542e4b79a3d78f7cb8545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879913</td>\n",
       "      <td>0.883126</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769120</td>\n",
       "      <td>0.763610</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.824516</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.823936</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.842499</td>\n",
       "      <td>0.842767</td>\n",
       "      <td>0.842628</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.879913  0.883126  0.881517  1369.000000\n",
       "1              0.769120  0.763610  0.766355   698.000000\n",
       "accuracy       0.842767  0.842767  0.842767     0.842767\n",
       "macro avg      0.824516  0.823368  0.823936  2067.000000\n",
       "weighted avg   0.842499  0.842767  0.842628  2067.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e8976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217ddf56a7414054958a1408322ba958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cb8f8c270b4ed9a5874f8aaf85cc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922535</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.907421</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.900693</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.912459</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.912367</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.922535  0.949275  0.935714  276.000000\n",
       "1              0.892308  0.840580  0.865672  138.000000\n",
       "accuracy       0.913043  0.913043  0.913043    0.913043\n",
       "macro avg      0.907421  0.894928  0.900693  414.000000\n",
       "weighted avg   0.912459  0.913043  0.912367  414.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f43a01fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d0d2e8ef8b4be58f3002a0508a63d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dd37725c5e49f29d0776f2111bc779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.981651</td>\n",
       "      <td>0.983908</td>\n",
       "      <td>0.982778</td>\n",
       "      <td>435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.974697</td>\n",
       "      <td>0.973605</td>\n",
       "      <td>0.974148</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.977016</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.981651  0.983908  0.982778  435.000000\n",
       "1              0.967742  0.963303  0.965517  218.000000\n",
       "accuracy       0.977029  0.977029  0.977029    0.977029\n",
       "macro avg      0.974697  0.973605  0.974148  653.000000\n",
       "weighted avg   0.977008  0.977029  0.977016  653.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2_2epoches/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db139a101df47708641841b0501a7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140c77a8be045ea92ae753e70958560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.905771</td>\n",
       "      <td>0.865620</td>\n",
       "      <td>1369.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774081</td>\n",
       "      <td>0.633238</td>\n",
       "      <td>0.696612</td>\n",
       "      <td>698.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.81374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.801479</td>\n",
       "      <td>0.769504</td>\n",
       "      <td>0.781116</td>\n",
       "      <td>2067.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.808548</td>\n",
       "      <td>2067.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.828877  0.905771  0.865620  1369.00000\n",
       "1              0.774081  0.633238  0.696612   698.00000\n",
       "accuracy       0.813740  0.813740  0.813740     0.81374\n",
       "macro avg      0.801479  0.769504  0.781116  2067.00000\n",
       "weighted avg   0.810373  0.813740  0.808548  2067.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ed396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d64cbc4e9b45c99ab2e93530c5b1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4302d824def3422a802eed943ef5ee6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.910345</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.932862</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.906785</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.893912</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.907972</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.906895</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.910345  0.956522  0.932862  276.000000\n",
       "1              0.903226  0.811594  0.854962  138.000000\n",
       "accuracy       0.908213  0.908213  0.908213    0.908213\n",
       "macro avg      0.906785  0.884058  0.893912  414.000000\n",
       "weighted avg   0.907972  0.908213  0.906895  414.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='seegull_gpt_augmentation', seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503e70d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d8ebaebaae45eaa296990c3bee25e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb22a1ffbcd4354ad66e960066ed4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927473</td>\n",
       "      <td>0.970115</td>\n",
       "      <td>0.948315</td>\n",
       "      <td>435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.889423</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.929556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.909369</td>\n",
       "      <td>0.918869</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.929766</td>\n",
       "      <td>0.929556</td>\n",
       "      <td>0.928654</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.927473  0.970115  0.948315  435.000000\n",
       "1              0.934343  0.848624  0.889423  218.000000\n",
       "accuracy       0.929556  0.929556  0.929556    0.929556\n",
       "macro avg      0.930908  0.909369  0.918869  653.000000\n",
       "weighted avg   0.929766  0.929556  0.928654  653.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_winoqueer_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='winoqueer_gpt_augmentation', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd8d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef996724c1a482a84c8a621598c3bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855fbd44f6d7498b95c10451a407ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.830872</td>\n",
       "      <td>0.904310</td>\n",
       "      <td>0.866037</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772964</td>\n",
       "      <td>0.638968</td>\n",
       "      <td>0.699608</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.814707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.801918</td>\n",
       "      <td>0.771639</td>\n",
       "      <td>0.782822</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.811317</td>\n",
       "      <td>0.814707</td>\n",
       "      <td>0.809836</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.830872  0.904310  0.866037  1369.000000\n",
       "1              0.772964  0.638968  0.699608   698.000000\n",
       "accuracy       0.814707  0.814707  0.814707     0.814707\n",
       "macro avg      0.801918  0.771639  0.782822  2067.000000\n",
       "weighted avg   0.811317  0.814707  0.809836  2067.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 08:46:03] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 08:46:03] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 08:46:03] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 08:46:03] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:05] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 08:46:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 08:46:05]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 08:46:05]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 08:46:05]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 08:46:05]   Available RAM : 15.789 GB\n",
      "[codecarbon INFO @ 08:46:05]   CPU count: 16\n",
      "[codecarbon INFO @ 08:46:05]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 08:46:05]   GPU count: 1\n",
      "[codecarbon INFO @ 08:46:05]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 08:46:08] Saving emissions data to file c:\\Users\\15509\\Desktop\\UK courses\\AI for SD\\Coursework 2\\HEARTS-Text-Stereotype-Detection-main\\Model Training and Evaluation\\emissions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdfd2ebe3864b739b6a31ca2a730564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a8a98ab3542e392cbbcca961eb7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:46:23] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:23] Energy consumed for all GPUs : 0.000025 kWh. Total GPU Power : 5.902435537359482 W\n",
      "[codecarbon INFO @ 08:46:23] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:23] 0.000143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for all GPUs : 0.000050 kWh. Total GPU Power : 6.0019148802554545 W\n",
      "[codecarbon INFO @ 08:46:38] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:38] 0.000287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for all GPUs : 0.000075 kWh. Total GPU Power : 6.018714102073998 W\n",
      "[codecarbon INFO @ 08:46:53] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:46:53] 0.000430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for all GPUs : 0.000100 kWh. Total GPU Power : 6.023766864524901 W\n",
      "[codecarbon INFO @ 08:47:08] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:08] 0.000574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for RAM : 0.000123 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for all GPUs : 0.000125 kWh. Total GPU Power : 6.032137464614325 W\n",
      "[codecarbon INFO @ 08:47:23] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:23] 0.000717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for RAM : 0.000148 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for all GPUs : 0.000150 kWh. Total GPU Power : 6.093037727346918 W\n",
      "[codecarbon INFO @ 08:47:38] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:38] 0.000861 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for RAM : 0.000173 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for all GPUs : 0.000176 kWh. Total GPU Power : 6.13733501573982 W\n",
      "[codecarbon INFO @ 08:47:53] Energy consumed for all CPUs : 0.000656 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:47:53] 0.001005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for RAM : 0.000197 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for all GPUs : 0.000202 kWh. Total GPU Power : 6.169179797772329 W\n",
      "[codecarbon INFO @ 08:48:08] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:08] 0.001149 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:08] 0.002274 g.CO2eq/s mean an estimation of 71.72343093200598 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for RAM : 0.000222 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for all GPUs : 0.000227 kWh. Total GPU Power : 6.11434670432621 W\n",
      "[codecarbon INFO @ 08:48:23] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:23] 0.001293 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for RAM : 0.000247 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for all GPUs : 0.000252 kWh. Total GPU Power : 6.097779012004364 W\n",
      "[codecarbon INFO @ 08:48:38] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:38] 0.001437 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for RAM : 0.000271 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for all GPUs : 0.000278 kWh. Total GPU Power : 6.085730357514495 W\n",
      "[codecarbon INFO @ 08:48:53] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:48:53] 0.001581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for RAM : 0.000296 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for all GPUs : 0.000303 kWh. Total GPU Power : 6.076182205542703 W\n",
      "[codecarbon INFO @ 08:49:08] Energy consumed for all CPUs : 0.001125 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:08] 0.001724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for RAM : 0.000321 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for all GPUs : 0.000328 kWh. Total GPU Power : 6.024677055933245 W\n",
      "[codecarbon INFO @ 08:49:23] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:23] 0.001868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for RAM : 0.000345 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for all GPUs : 0.000353 kWh. Total GPU Power : 6.030542023482323 W\n",
      "[codecarbon INFO @ 08:49:38] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:38] 0.002011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for all GPUs : 0.000378 kWh. Total GPU Power : 6.004597308868218 W\n",
      "[codecarbon INFO @ 08:49:53] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:49:53] 0.002155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for RAM : 0.000395 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for all GPUs : 0.000403 kWh. Total GPU Power : 5.9870833492969995 W\n",
      "[codecarbon INFO @ 08:50:08] Energy consumed for all CPUs : 0.001500 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:08] 0.002298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:08] 0.002274 g.CO2eq/s mean an estimation of 71.72829980068263 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for RAM : 0.000419 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for all GPUs : 0.000428 kWh. Total GPU Power : 5.9485378959322315 W\n",
      "[codecarbon INFO @ 08:50:23] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:23] 0.002442 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for RAM : 0.000444 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for all GPUs : 0.000453 kWh. Total GPU Power : 5.929093268604776 W\n",
      "[codecarbon INFO @ 08:50:38] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:38] 0.002585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for RAM : 0.000469 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for all GPUs : 0.000477 kWh. Total GPU Power : 5.793803990333124 W\n",
      "[codecarbon INFO @ 08:50:53] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:50:53] 0.002727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for RAM : 0.000493 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for all GPUs : 0.000501 kWh. Total GPU Power : 5.7721313306168645 W\n",
      "[codecarbon INFO @ 08:51:08] Energy consumed for all CPUs : 0.001875 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:08] 0.002870 kWh of electricity used since the beginning.\n",
      "loading weights file model.safetensors from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\model.safetensors\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3cc51d20be41408309159ee4a34035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4a00451914abb875b5805ec0bbbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb37cbf459ca45e3810a9097bbb2c5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:51:23] Energy consumed for RAM : 0.000518 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:23] Energy consumed for all GPUs : 0.000526 kWh. Total GPU Power : 5.888676291335624 W\n",
      "[codecarbon INFO @ 08:51:23] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:23] 0.003013 kWh of electricity used since the beginning.\n",
      "loading file sentencepiece.bpe.model from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\15509\\.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c6535c49814c8db579b8f07d51d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cf22f68931483ba68c6f0279dc6ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714128d9531042f9a9b920b6f8ce7c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c180fcad285452789d5f79fa8c47201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\15509\\AppData\\Local\\Temp\\ipykernel_20312\\2571795268.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon ERROR @ 08:51:28] Error: Another instance of codecarbon is probably running as we find `C:\\Users\\15509\\AppData\\Local\\Temp\\.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6,613\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 414\n",
      "  Number of trainable parameters = 278,045,186\n",
      "[codecarbon WARNING @ 08:51:28] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'Many of its citizens are unhappy that korea is unified.', 'label': 0, 'group': 'race', 'data_name': 'MGSD', '__index_level_0__': 6074, 'input_ids': [0, 52455, 111, 6863, 130367, 7, 621, 51, 157976, 450, 6, 81162, 83, 51, 47314, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afbd7ea84744d9da80f634ca337e2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:51:38] Energy consumed for RAM : 0.000543 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:38] Energy consumed for all GPUs : 0.000550 kWh. Total GPU Power : 5.861444046047198 W\n",
      "[codecarbon INFO @ 08:51:38] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:38] 0.003156 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for RAM : 0.000567 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for all GPUs : 0.000575 kWh. Total GPU Power : 6.067289945916921 W\n",
      "[codecarbon INFO @ 08:51:53] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:51:53] 0.003300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for RAM : 0.000592 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for all GPUs : 0.000601 kWh. Total GPU Power : 6.23505483343005 W\n",
      "[codecarbon INFO @ 08:52:08] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:08] 0.003444 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:08] 0.002267 g.CO2eq/s mean an estimation of 71.48677969411824 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for RAM : 0.000617 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for all GPUs : 0.000628 kWh. Total GPU Power : 6.2969162914742824 W\n",
      "[codecarbon INFO @ 08:52:23] Energy consumed for all CPUs : 0.002344 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:23] 0.003589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for RAM : 0.000641 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for all GPUs : 0.000654 kWh. Total GPU Power : 6.3618935986613945 W\n",
      "[codecarbon INFO @ 08:52:38] Energy consumed for all CPUs : 0.002438 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:38] 0.003733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for RAM : 0.000666 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for all GPUs : 0.000681 kWh. Total GPU Power : 6.402600096713132 W\n",
      "[codecarbon INFO @ 08:52:53] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:52:53] 0.003879 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for RAM : 0.000691 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for all GPUs : 0.000708 kWh. Total GPU Power : 6.49106795243007 W\n",
      "[codecarbon INFO @ 08:53:08] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:08] 0.004024 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for RAM : 0.000715 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for all GPUs : 0.000735 kWh. Total GPU Power : 6.579482834216632 W\n",
      "[codecarbon INFO @ 08:53:23] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:23] 0.004170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for RAM : 0.000740 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for all GPUs : 0.000763 kWh. Total GPU Power : 6.6303728115775185 W\n",
      "[codecarbon INFO @ 08:53:38] Energy consumed for all CPUs : 0.002813 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:38] 0.004316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for RAM : 0.000765 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for all GPUs : 0.000791 kWh. Total GPU Power : 6.715019715311984 W\n",
      "[codecarbon INFO @ 08:53:53] Energy consumed for all CPUs : 0.002907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:53:53] 0.004462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for RAM : 0.000789 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for all GPUs : 0.000819 kWh. Total GPU Power : 6.668322679403866 W\n",
      "[codecarbon INFO @ 08:54:08] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:08] 0.004609 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:08] 0.002305 g.CO2eq/s mean an estimation of 72.69303070800196 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for RAM : 0.000814 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for all GPUs : 0.000846 kWh. Total GPU Power : 6.5389308631617675 W\n",
      "[codecarbon INFO @ 08:54:23] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:23] 0.004754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for RAM : 0.000839 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for all GPUs : 0.000873 kWh. Total GPU Power : 6.456750409645287 W\n",
      "[codecarbon INFO @ 08:54:38] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:38] 0.004900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for RAM : 0.000863 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for all GPUs : 0.000900 kWh. Total GPU Power : 6.473182014101419 W\n",
      "[codecarbon INFO @ 08:54:53] Energy consumed for all CPUs : 0.003282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:54:53] 0.005045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for RAM : 0.000888 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for all GPUs : 0.000927 kWh. Total GPU Power : 6.536225610852232 W\n",
      "[codecarbon INFO @ 08:55:08] Energy consumed for all CPUs : 0.003376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:08] 0.005191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for all GPUs : 0.000954 kWh. Total GPU Power : 6.5396001988006125 W\n",
      "[codecarbon INFO @ 08:55:23] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:23] 0.005336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for all GPUs : 0.000981 kWh. Total GPU Power : 6.512384763442136 W\n",
      "[codecarbon INFO @ 08:55:38] Energy consumed for all CPUs : 0.003563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:38] 0.005482 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for all GPUs : 0.001008 kWh. Total GPU Power : 6.485916455162652 W\n",
      "[codecarbon INFO @ 08:55:53] Energy consumed for all CPUs : 0.003657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:55:53] 0.005627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for RAM : 0.000986 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for all GPUs : 0.001035 kWh. Total GPU Power : 6.508372645623242 W\n",
      "[codecarbon INFO @ 08:56:08] Energy consumed for all CPUs : 0.003751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:08] 0.005773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:08] 0.002304 g.CO2eq/s mean an estimation of 72.6637621967016 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for RAM : 0.001011 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for all GPUs : 0.001063 kWh. Total GPU Power : 6.505843728805227 W\n",
      "[codecarbon INFO @ 08:56:23] Energy consumed for all CPUs : 0.003845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:23] 0.005918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for all GPUs : 0.001090 kWh. Total GPU Power : 6.531308146871879 W\n",
      "[codecarbon INFO @ 08:56:38] Energy consumed for all CPUs : 0.003938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:38] 0.006064 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for RAM : 0.001060 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for all GPUs : 0.001117 kWh. Total GPU Power : 6.567430297608812 W\n",
      "[codecarbon INFO @ 08:56:53] Energy consumed for all CPUs : 0.004032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:56:53] 0.006210 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for RAM : 0.001085 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for all GPUs : 0.001145 kWh. Total GPU Power : 6.599890879935969 W\n",
      "[codecarbon INFO @ 08:57:08] Energy consumed for all CPUs : 0.004126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:08] 0.006356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for RAM : 0.001110 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for all GPUs : 0.001172 kWh. Total GPU Power : 6.460368559798439 W\n",
      "[codecarbon INFO @ 08:57:23] Energy consumed for all CPUs : 0.004220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:23] 0.006501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for RAM : 0.001134 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for all GPUs : 0.001198 kWh. Total GPU Power : 6.433788709662775 W\n",
      "[codecarbon INFO @ 08:57:38] Energy consumed for all CPUs : 0.004314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:38] 0.006646 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for RAM : 0.001159 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for all GPUs : 0.001225 kWh. Total GPU Power : 6.464466966163585 W\n",
      "[codecarbon INFO @ 08:57:53] Energy consumed for all CPUs : 0.004407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:57:53] 0.006792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for RAM : 0.001184 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for all GPUs : 0.001252 kWh. Total GPU Power : 6.485319270009119 W\n",
      "[codecarbon INFO @ 08:58:08] Energy consumed for all CPUs : 0.004501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:08] 0.006937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:08] 0.002304 g.CO2eq/s mean an estimation of 72.66776088352664 kg.CO2eq/year\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for RAM : 0.001208 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for all GPUs : 0.001279 kWh. Total GPU Power : 6.509296416901 W\n",
      "[codecarbon INFO @ 08:58:23] Energy consumed for all CPUs : 0.004595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:23] 0.007083 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for RAM : 0.001233 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for all GPUs : 0.001307 kWh. Total GPU Power : 6.523969446842842 W\n",
      "[codecarbon INFO @ 08:58:38] Energy consumed for all CPUs : 0.004689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:38] 0.007228 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for RAM : 0.001258 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for all GPUs : 0.001334 kWh. Total GPU Power : 6.488304779558429 W\n",
      "[codecarbon INFO @ 08:58:53] Energy consumed for all CPUs : 0.004782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:58:53] 0.007374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for RAM : 0.001282 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for all GPUs : 0.001361 kWh. Total GPU Power : 6.456524768620162 W\n",
      "[codecarbon INFO @ 08:59:08] Energy consumed for all CPUs : 0.004876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:08] 0.007519 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for RAM : 0.001307 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for all GPUs : 0.001388 kWh. Total GPU Power : 6.5014842883420165 W\n",
      "[codecarbon INFO @ 08:59:23] Energy consumed for all CPUs : 0.004970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:23] 0.007665 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for RAM : 0.001332 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for all GPUs : 0.001415 kWh. Total GPU Power : 6.487019347153467 W\n",
      "[codecarbon INFO @ 08:59:38] Energy consumed for all CPUs : 0.005064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:38] 0.007810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for RAM : 0.001356 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for all GPUs : 0.001442 kWh. Total GPU Power : 6.509083053685998 W\n",
      "[codecarbon INFO @ 08:59:53] Energy consumed for all CPUs : 0.005157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 08:59:53] 0.007956 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for RAM : 0.001381 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for all GPUs : 0.001469 kWh. Total GPU Power : 6.5464044600033535 W\n",
      "[codecarbon INFO @ 09:00:08] Energy consumed for all CPUs : 0.005251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:08] 0.008101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:08] 0.002304 g.CO2eq/s mean an estimation of 72.65338716186707 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for RAM : 0.001406 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for all GPUs : 0.001496 kWh. Total GPU Power : 6.514620999158129 W\n",
      "[codecarbon INFO @ 09:00:23] Energy consumed for all CPUs : 0.005345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:23] 0.008247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for RAM : 0.001430 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for all GPUs : 0.001523 kWh. Total GPU Power : 6.432495857602685 W\n",
      "[codecarbon INFO @ 09:00:38] Energy consumed for all CPUs : 0.005439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:38] 0.008392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for RAM : 0.001455 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for all GPUs : 0.001550 kWh. Total GPU Power : 6.4044963117132125 W\n",
      "[codecarbon INFO @ 09:00:53] Energy consumed for all CPUs : 0.005532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:00:53] 0.008537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for RAM : 0.001480 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for all GPUs : 0.001577 kWh. Total GPU Power : 6.445469575090384 W\n",
      "[codecarbon INFO @ 09:01:08] Energy consumed for all CPUs : 0.005626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:08] 0.008682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for RAM : 0.001504 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for all GPUs : 0.001604 kWh. Total GPU Power : 6.500826226325576 W\n",
      "[codecarbon INFO @ 09:01:23] Energy consumed for all CPUs : 0.005720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:23] 0.008828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for RAM : 0.001529 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for all GPUs : 0.001631 kWh. Total GPU Power : 6.456624826935719 W\n",
      "[codecarbon INFO @ 09:01:38] Energy consumed for all CPUs : 0.005814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:38] 0.008973 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for RAM : 0.001554 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for all GPUs : 0.001658 kWh. Total GPU Power : 6.541944325410182 W\n",
      "[codecarbon INFO @ 09:01:53] Energy consumed for all CPUs : 0.005907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:01:53] 0.009119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for RAM : 0.001578 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for all GPUs : 0.001685 kWh. Total GPU Power : 6.5006745278711975 W\n",
      "[codecarbon INFO @ 09:02:08] Energy consumed for all CPUs : 0.006001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:08] 0.009264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:08] 0.002302 g.CO2eq/s mean an estimation of 72.60537076804808 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for RAM : 0.001603 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for all GPUs : 0.001712 kWh. Total GPU Power : 6.4306894341301835 W\n",
      "[codecarbon INFO @ 09:02:23] Energy consumed for all CPUs : 0.006095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:23] 0.009410 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for RAM : 0.001628 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for all GPUs : 0.001739 kWh. Total GPU Power : 6.4548198079546815 W\n",
      "[codecarbon INFO @ 09:02:38] Energy consumed for all CPUs : 0.006189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:38] 0.009555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for RAM : 0.001652 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for all GPUs : 0.001765 kWh. Total GPU Power : 6.438624939439415 W\n",
      "[codecarbon INFO @ 09:02:53] Energy consumed for all CPUs : 0.006282 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:02:53] 0.009700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for RAM : 0.001677 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for all GPUs : 0.001793 kWh. Total GPU Power : 6.524133587281729 W\n",
      "[codecarbon INFO @ 09:03:08] Energy consumed for all CPUs : 0.006376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:08] 0.009846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for RAM : 0.001702 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for all GPUs : 0.001820 kWh. Total GPU Power : 6.539024563533249 W\n",
      "[codecarbon INFO @ 09:03:23] Energy consumed for all CPUs : 0.006470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:23] 0.009991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for RAM : 0.001726 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for all GPUs : 0.001847 kWh. Total GPU Power : 6.592891257910618 W\n",
      "[codecarbon INFO @ 09:03:38] Energy consumed for all CPUs : 0.006564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:38] 0.010137 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for RAM : 0.001751 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for all GPUs : 0.001875 kWh. Total GPU Power : 6.553652251188814 W\n",
      "[codecarbon INFO @ 09:03:53] Energy consumed for all CPUs : 0.006658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:03:53] 0.010283 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for RAM : 0.001776 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for all GPUs : 0.001902 kWh. Total GPU Power : 6.500344699488218 W\n",
      "[codecarbon INFO @ 09:04:08] Energy consumed for all CPUs : 0.006751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:08] 0.010429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:08] 0.002304 g.CO2eq/s mean an estimation of 72.66369478669621 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for RAM : 0.001800 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for all GPUs : 0.001929 kWh. Total GPU Power : 6.4796132883117075 W\n",
      "[codecarbon INFO @ 09:04:23] Energy consumed for all CPUs : 0.006845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:23] 0.010574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for RAM : 0.001825 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for all GPUs : 0.001956 kWh. Total GPU Power : 6.503488990781604 W\n",
      "[codecarbon INFO @ 09:04:38] Energy consumed for all CPUs : 0.006939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:38] 0.010719 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for RAM : 0.001850 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for all GPUs : 0.001983 kWh. Total GPU Power : 6.567975405958753 W\n",
      "[codecarbon INFO @ 09:04:53] Energy consumed for all CPUs : 0.007033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:04:53] 0.010865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for RAM : 0.001874 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for all GPUs : 0.002011 kWh. Total GPU Power : 6.687676954375361 W\n",
      "[codecarbon INFO @ 09:05:08] Energy consumed for all CPUs : 0.007126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:08] 0.011011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for RAM : 0.001899 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for all GPUs : 0.002039 kWh. Total GPU Power : 6.65606725115019 W\n",
      "[codecarbon INFO @ 09:05:23] Energy consumed for all CPUs : 0.007220 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:23] 0.011158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for RAM : 0.001923 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for all GPUs : 0.002066 kWh. Total GPU Power : 6.5990629094086755 W\n",
      "[codecarbon INFO @ 09:05:38] Energy consumed for all CPUs : 0.007314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:38] 0.011304 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for RAM : 0.001948 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for all GPUs : 0.002093 kWh. Total GPU Power : 6.544890691879098 W\n",
      "[codecarbon INFO @ 09:05:53] Energy consumed for all CPUs : 0.007408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:05:53] 0.011449 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for RAM : 0.001973 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for all GPUs : 0.002121 kWh. Total GPU Power : 6.506107497612927 W\n",
      "[codecarbon INFO @ 09:06:08] Energy consumed for all CPUs : 0.007501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:08] 0.011595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:08] 0.002308 g.CO2eq/s mean an estimation of 72.79628172144935 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for RAM : 0.001997 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for all GPUs : 0.002148 kWh. Total GPU Power : 6.53858084399573 W\n",
      "[codecarbon INFO @ 09:06:23] Energy consumed for all CPUs : 0.007595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:23] 0.011740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for RAM : 0.002022 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for all GPUs : 0.002175 kWh. Total GPU Power : 6.518028563733236 W\n",
      "[codecarbon INFO @ 09:06:38] Energy consumed for all CPUs : 0.007689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:38] 0.011886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for RAM : 0.002047 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for all GPUs : 0.002202 kWh. Total GPU Power : 6.575784817016608 W\n",
      "[codecarbon INFO @ 09:06:53] Energy consumed for all CPUs : 0.007783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:06:53] 0.012032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for RAM : 0.002071 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for all GPUs : 0.002230 kWh. Total GPU Power : 6.591722168834508 W\n",
      "[codecarbon INFO @ 09:07:08] Energy consumed for all CPUs : 0.007876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:08] 0.012178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for RAM : 0.002096 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for all GPUs : 0.002257 kWh. Total GPU Power : 6.526339474578139 W\n",
      "[codecarbon INFO @ 09:07:23] Energy consumed for all CPUs : 0.007970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:23] 0.012323 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for RAM : 0.002121 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for all GPUs : 0.002284 kWh. Total GPU Power : 6.518930901308019 W\n",
      "[codecarbon INFO @ 09:07:38] Energy consumed for all CPUs : 0.008064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:38] 0.012469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for RAM : 0.002145 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for all GPUs : 0.002311 kWh. Total GPU Power : 6.522822459337724 W\n",
      "[codecarbon INFO @ 09:07:53] Energy consumed for all CPUs : 0.008158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:07:53] 0.012615 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for RAM : 0.002170 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for all GPUs : 0.002339 kWh. Total GPU Power : 6.600539124571437 W\n",
      "[codecarbon INFO @ 09:08:08] Energy consumed for all CPUs : 0.008252 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:08] 0.012761 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:08] 0.002307 g.CO2eq/s mean an estimation of 72.76005427541283 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for RAM : 0.002195 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for all GPUs : 0.002367 kWh. Total GPU Power : 6.6614831128055 W\n",
      "[codecarbon INFO @ 09:08:23] Energy consumed for all CPUs : 0.008345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:23] 0.012907 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for RAM : 0.002219 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for all GPUs : 0.002394 kWh. Total GPU Power : 6.6601183451990424 W\n",
      "[codecarbon INFO @ 09:08:38] Energy consumed for all CPUs : 0.008439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:38] 0.013053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for RAM : 0.002244 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for all GPUs : 0.002422 kWh. Total GPU Power : 6.516061786025075 W\n",
      "[codecarbon INFO @ 09:08:53] Energy consumed for all CPUs : 0.008533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:08:53] 0.013198 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for RAM : 0.002269 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for all GPUs : 0.002448 kWh. Total GPU Power : 6.460188925596394 W\n",
      "[codecarbon INFO @ 09:09:08] Energy consumed for all CPUs : 0.008627 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:08] 0.013344 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for RAM : 0.002293 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for all GPUs : 0.002476 kWh. Total GPU Power : 6.5107200050175775 W\n",
      "[codecarbon INFO @ 09:09:23] Energy consumed for all CPUs : 0.008720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:23] 0.013489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for RAM : 0.002318 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for all GPUs : 0.002503 kWh. Total GPU Power : 6.512588938249721 W\n",
      "[codecarbon INFO @ 09:09:38] Energy consumed for all CPUs : 0.008814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:38] 0.013635 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for RAM : 0.002343 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for all GPUs : 0.002530 kWh. Total GPU Power : 6.536331518043861 W\n",
      "[codecarbon INFO @ 09:09:53] Energy consumed for all CPUs : 0.008908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:09:53] 0.013781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for RAM : 0.002367 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for all GPUs : 0.002557 kWh. Total GPU Power : 6.520944214898771 W\n",
      "[codecarbon INFO @ 09:10:08] Energy consumed for all CPUs : 0.009002 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:08] 0.013926 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:08] 0.002307 g.CO2eq/s mean an estimation of 72.75428478402404 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for RAM : 0.002392 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for all GPUs : 0.002584 kWh. Total GPU Power : 6.396410414155539 W\n",
      "[codecarbon INFO @ 09:10:23] Energy consumed for all CPUs : 0.009095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:23] 0.014071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for RAM : 0.002417 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for all GPUs : 0.002610 kWh. Total GPU Power : 6.4086433249520915 W\n",
      "[codecarbon INFO @ 09:10:38] Energy consumed for all CPUs : 0.009189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:38] 0.014216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for RAM : 0.002441 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for all GPUs : 0.002637 kWh. Total GPU Power : 6.45799139701892 W\n",
      "[codecarbon INFO @ 09:10:53] Energy consumed for all CPUs : 0.009283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:10:53] 0.014362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for RAM : 0.002466 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for all GPUs : 0.002664 kWh. Total GPU Power : 6.447937045561769 W\n",
      "[codecarbon INFO @ 09:11:08] Energy consumed for all CPUs : 0.009377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:08] 0.014507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for RAM : 0.002491 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for all GPUs : 0.002692 kWh. Total GPU Power : 6.54663957665174 W\n",
      "[codecarbon INFO @ 09:11:23] Energy consumed for all CPUs : 0.009471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:23] 0.014653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for RAM : 0.002515 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for all GPUs : 0.002718 kWh. Total GPU Power : 6.451929707392118 W\n",
      "[codecarbon INFO @ 09:11:38] Energy consumed for all CPUs : 0.009564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:38] 0.014798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for RAM : 0.002540 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for all GPUs : 0.002745 kWh. Total GPU Power : 6.4088738087296955 W\n",
      "[codecarbon INFO @ 09:11:53] Energy consumed for all CPUs : 0.009658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:11:53] 0.014943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for RAM : 0.002565 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for all GPUs : 0.002772 kWh. Total GPU Power : 6.361984359812977 W\n",
      "[codecarbon INFO @ 09:12:08] Energy consumed for all CPUs : 0.009752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:08] 0.015088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:08] 0.002300 g.CO2eq/s mean an estimation of 72.51822446720526 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for RAM : 0.002589 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for all GPUs : 0.002799 kWh. Total GPU Power : 6.457752716065107 W\n",
      "[codecarbon INFO @ 09:12:23] Energy consumed for all CPUs : 0.009846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:23] 0.015233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for RAM : 0.002614 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for all GPUs : 0.002826 kWh. Total GPU Power : 6.5308945007872 W\n",
      "[codecarbon INFO @ 09:12:38] Energy consumed for all CPUs : 0.009939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:38] 0.015379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for RAM : 0.002639 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for all GPUs : 0.002853 kWh. Total GPU Power : 6.484591312169335 W\n",
      "[codecarbon INFO @ 09:12:53] Energy consumed for all CPUs : 0.010033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:12:53] 0.015524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for RAM : 0.002663 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for all GPUs : 0.002880 kWh. Total GPU Power : 6.446573028934487 W\n",
      "[codecarbon INFO @ 09:13:08] Energy consumed for all CPUs : 0.010127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:08] 0.015670 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for RAM : 0.002688 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for all GPUs : 0.002906 kWh. Total GPU Power : 6.380314755443399 W\n",
      "[codecarbon INFO @ 09:13:23] Energy consumed for all CPUs : 0.010221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:23] 0.015815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for RAM : 0.002713 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for all GPUs : 0.002933 kWh. Total GPU Power : 6.393750950757771 W\n",
      "[codecarbon INFO @ 09:13:38] Energy consumed for all CPUs : 0.010314 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:38] 0.015960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for RAM : 0.002737 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for all GPUs : 0.002960 kWh. Total GPU Power : 6.441634641090397 W\n",
      "[codecarbon INFO @ 09:13:53] Energy consumed for all CPUs : 0.010408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:13:53] 0.016105 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for RAM : 0.002762 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for all GPUs : 0.002986 kWh. Total GPU Power : 6.434173024276784 W\n",
      "[codecarbon INFO @ 09:14:08] Energy consumed for all CPUs : 0.010502 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:08] 0.016250 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:08] 0.002300 g.CO2eq/s mean an estimation of 72.5400842686946 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for RAM : 0.002787 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for all GPUs : 0.003013 kWh. Total GPU Power : 6.4062377592298585 W\n",
      "[codecarbon INFO @ 09:14:23] Energy consumed for all CPUs : 0.010596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:23] 0.016395 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for RAM : 0.002811 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for all GPUs : 0.003040 kWh. Total GPU Power : 6.486482405469568 W\n",
      "[codecarbon INFO @ 09:14:38] Energy consumed for all CPUs : 0.010689 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:38] 0.016541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for RAM : 0.002836 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for all GPUs : 0.003067 kWh. Total GPU Power : 6.517274978887297 W\n",
      "[codecarbon INFO @ 09:14:53] Energy consumed for all CPUs : 0.010783 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:14:53] 0.016686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for RAM : 0.002861 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for all GPUs : 0.003095 kWh. Total GPU Power : 6.5457152044954094 W\n",
      "[codecarbon INFO @ 09:15:08] Energy consumed for all CPUs : 0.010877 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:08] 0.016832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for RAM : 0.002885 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for all GPUs : 0.003122 kWh. Total GPU Power : 6.491876276512889 W\n",
      "[codecarbon INFO @ 09:15:23] Energy consumed for all CPUs : 0.010971 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:23] 0.016978 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for RAM : 0.002910 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for all GPUs : 0.003149 kWh. Total GPU Power : 6.484059105367076 W\n",
      "[codecarbon INFO @ 09:15:38] Energy consumed for all CPUs : 0.011064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:38] 0.017123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for RAM : 0.002935 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for all GPUs : 0.003176 kWh. Total GPU Power : 6.5967112092903895 W\n",
      "[codecarbon INFO @ 09:15:53] Energy consumed for all CPUs : 0.011158 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:15:53] 0.017269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for RAM : 0.002959 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for all GPUs : 0.003203 kWh. Total GPU Power : 6.55689537267565 W\n",
      "[codecarbon INFO @ 09:16:08] Energy consumed for all CPUs : 0.011252 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:08] 0.017415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:08] 0.002305 g.CO2eq/s mean an estimation of 72.6799108022938 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for RAM : 0.002984 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for all GPUs : 0.003231 kWh. Total GPU Power : 6.559187162724073 W\n",
      "[codecarbon INFO @ 09:16:23] Energy consumed for all CPUs : 0.011346 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:23] 0.017560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for RAM : 0.003009 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for all GPUs : 0.003258 kWh. Total GPU Power : 6.5132598334060665 W\n",
      "[codecarbon INFO @ 09:16:38] Energy consumed for all CPUs : 0.011439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:38] 0.017706 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for RAM : 0.003033 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for all GPUs : 0.003285 kWh. Total GPU Power : 6.534530879514324 W\n",
      "[codecarbon INFO @ 09:16:53] Energy consumed for all CPUs : 0.011533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:16:53] 0.017852 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n",
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for RAM : 0.003058 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for all GPUs : 0.003312 kWh. Total GPU Power : 6.494021898468532 W\n",
      "[codecarbon INFO @ 09:17:08] Energy consumed for all CPUs : 0.011627 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:08] 0.017997 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, text, group. If data_name, __index_level_0__, text, group are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3ba2bb29b24a6aa70078c3ad5ffcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:17:23] Energy consumed for RAM : 0.003082 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:24] Energy consumed for all GPUs : 0.003339 kWh. Total GPU Power : 6.414025396971511 W\n",
      "[codecarbon INFO @ 09:17:24] Energy consumed for all CPUs : 0.011721 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:24] 0.018143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for RAM : 0.003107 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for all GPUs : 0.003366 kWh. Total GPU Power : 6.427792622717017 W\n",
      "[codecarbon INFO @ 09:17:38] Energy consumed for all CPUs : 0.011815 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:38] 0.018287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for RAM : 0.003132 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for all GPUs : 0.003393 kWh. Total GPU Power : 6.521199986786852 W\n",
      "[codecarbon INFO @ 09:17:53] Energy consumed for all CPUs : 0.011908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:17:53] 0.018433 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4298393726348877, 'eval_precision': 0.7788990450984716, 'eval_recall': 0.7798867110969021, 'eval_f1': 0.7793869030766134, 'eval_balanced accuracy': 0.7798867110969021, 'eval_runtime': 51.6186, 'eval_samples_per_second': 32.043, 'eval_steps_per_second': 2.015, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\model.safetensors\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for RAM : 0.003182 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for all GPUs : 0.003447 kWh. Total GPU Power : 6.357348749607534 W\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\tokenizer_config.json\n",
      "[codecarbon INFO @ 09:18:24] Energy consumed for all CPUs : 0.012101 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:24] 0.018730 kWh of electricity used since the beginning.\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:18:24] 0.002302 g.CO2eq/s mean an estimation of 72.58202896567883 kg.CO2eq/year\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for RAM : 0.003207 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for all GPUs : 0.003472 kWh. Total GPU Power : 6.028873077812276 W\n",
      "[codecarbon INFO @ 09:18:39] Energy consumed for all CPUs : 0.012194 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:39] 0.018874 kWh of electricity used since the beginning.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\checkpoint-414 (score: 0.4298393726348877).\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for RAM : 0.003232 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for all GPUs : 0.003497 kWh. Total GPU Power : 6.015433244286016 W\n",
      "[codecarbon INFO @ 09:18:54] Energy consumed for all CPUs : 0.012288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:18:54] 0.019017 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 09:18:55] Another instance of codecarbon is already running. Exiting.\n",
      "Saving model checkpoint to model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1646.8074, 'train_samples_per_second': 4.016, 'train_steps_per_second': 0.251, 'train_loss': 0.543231945682839, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for RAM : 0.003278 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all GPUs : 0.003544 kWh. Total GPU Power : 5.975447951952986 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all CPUs : 0.012463 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:19:22] 0.019285 kWh of electricity used since the beginning.\n",
      "tokenizer config file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2\\merged_winoqueer_seegull_gpt_augmentation_trained\\special_tokens_map.json\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for RAM : 0.003278 kWh. RAM Power : 5.920974254608154 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all GPUs : 0.003544 kWh. Total GPU Power : 2.6820229055687137 W\n",
      "[codecarbon INFO @ 09:19:22] Energy consumed for all CPUs : 0.012464 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:19:22] 0.019286 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.00458208868225132 kg CO2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_output_albertv2\\\\merged_winoqueer_seegull_gpt_augmentation_trained'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "    model_path='xlm-roberta-base', \n",
    "    batch_size=16, # from 64 to 16\n",
    "    epoch=1, \n",
    "    learning_rate=2e-5, \n",
    "    model_output_base_dir='model_output_xlm-roberta-base', \n",
    "    dataset_name='merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ac4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfbfdb0be81404abdb576085fc8b16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef0523439d946ceaf882c4ad43b9c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [0, 581, 92422, 211190, 90, 3542, 67967, 297, 237, 51, 47215, 73, 3674, 390, 3060, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845546</td>\n",
       "      <td>0.859752</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>1369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.691977</td>\n",
       "      <td>0.703569</td>\n",
       "      <td>698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.803096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.780551</td>\n",
       "      <td>0.775864</td>\n",
       "      <td>0.778079</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.803096</td>\n",
       "      <td>0.802267</td>\n",
       "      <td>2067.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.845546  0.859752  0.852590  1369.000000\n",
       "1              0.715556  0.691977  0.703569   698.000000\n",
       "accuracy       0.803096  0.803096  0.803096     0.803096\n",
       "macro avg      0.780551  0.775864  0.778079  2067.000000\n",
       "weighted avg   0.801650  0.803096  0.802267  2067.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_merged_winoqueer_seegull_gpt_augmentation, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               dataset_name='merged_winoqueer_seegull_gpt_augmentation', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db76269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = 'model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained'   # ä½ çš„æ¨¡åž‹ç›®å½•\n",
    "pipe = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, return_all_scores=True)\n",
    "model_path = 'model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained'\n",
    "pipe_albert = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, return_all_scores=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb95876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.40001264214515686}, {'label': 'LABEL_1', 'score': 0.5999873280525208}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.8098716139793396}, {'label': 'LABEL_1', 'score': 0.19012843072414398}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"è¿™ä¸ªè¯„è®ºå¸¦æœ‰åˆ»æ¿å°è±¡å—ï¼Ÿ\"\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7a3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.08588053286075592}, {'label': 'LABEL_1', 'score': 0.9141194224357605}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.624780535697937}, {'label': 'LABEL_1', 'score': 0.3752194344997406}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¤§å­¦ç”Ÿéƒ½å¾ˆç¬¨\"\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b48f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.13456343114376068}, {'label': 'LABEL_1', 'score': 0.8654365539550781}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"æœ‰å¾ˆç¬¨çš„å¤§å­¦ç”Ÿ\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea35db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.24039381742477417}, {'label': 'LABEL_1', 'score': 0.759606122970581}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Stupid undergraduate student exists\"\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.07690759003162384}, {'label': 'LABEL_1', 'score': 0.9230924248695374}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¤§å­¦ç”Ÿã¯å…¨éƒ¨é¦¬é¹¿ã§ã™\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4d68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.0859021320939064}, {'label': 'LABEL_1', 'score': 0.914097785949707}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"å¤§å­¦ç”Ÿã«ã¯é¦¬é¹¿ãªäººã‚‚ã„ã¾ã™\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38409eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.788849413394928}, {'label': 'LABEL_1', 'score': 0.2111506164073944}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"ä¸­å›½äººéƒ½å¾ˆèªæ˜Ž\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d825a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.7089645862579346}, {'label': 'LABEL_1', 'score': 0.29103535413742065}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.8592768907546997}, {'label': 'LABEL_1', 'score': 0.1407231241464615}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Chinese are all  smart\"\n",
    "result = pipe(text)\n",
    "print(result)\n",
    "result = pipe_albert(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0cfe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.1442098170518875}, {'label': 'LABEL_1', 'score': 0.8557901978492737}]]\n"
     ]
    }
   ],
   "source": [
    "text = \"å¥³äººå°±æ˜¯ä¸èƒ½å½“æ³•å®˜\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b201cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "          group                       text  label data_name\n",
      "222  profession             é‚£ä¸ªæ‡’æƒ°çš„è­¦å«åœ¨åšç‰©é¦†å·¥ä½œã€‚      0   mgsd_zh\n",
      "254        race  æˆ‘çš„æœ‹å‹åŽ»äº†å°¼æ³Šå°”æ—…è¡Œã€‚é‚£é‡Œéžå¸¸è´«ç©·ï¼Œç”Ÿæ´»å¾ˆè‰°éš¾ã€‚      0   mgsd_zh\n",
      "278      gender                  ä»–æ˜¯å¥¹é‚ªæ¶çš„ç»§çˆ¶ã€‚      0   mgsd_zh\n",
      "90         race     é˜¿æ‹‰ä¼¯äººæ˜¯ä¸€ä¸ªç¬¼ç»Ÿçš„ç§°å‘¼ã€‚æˆ‘æ™šä¸Šåƒåžƒåœ¾é£Ÿå“ã€‚      0   mgsd_zh\n",
      "108  profession       é‚£åç‹±è­¦åœ¨ç›‘ç‹±é‡Œä»¥â€œåƒä¸ªå¤–æ˜Ÿäººâ€è€Œå‡ºåã€‚      0   mgsd_zh\n",
      "First few examples from the testing data:\n",
      "          group                         text  label data_name\n",
      "160  profession   è€å¸ˆå‘å­¦ç”Ÿè®²æŽˆå„é—¨å­¦ç§‘ã€‚æ˜ŸæœŸäºŒæ˜¯åƒçŽ‰ç±³å·é¥¼çš„å¥½æ—¥å­ã€‚      0   mgsd_zh\n",
      "220        race  æˆ‘å“¥å“¥å­¦ä¼šè¯´è‘¡è„ç‰™è¯­äº†ã€‚è´«æ°‘çªŸé‡Œçš„è€é¼ é•¿å¾—è·ŸçŒ«ä¸€æ ·å¤§ã€‚      0   mgsd_zh\n",
      "273  profession    åˆ†æžå¸ˆé—®äº†ä¸€ç³»åˆ—å…·ä½“çš„é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªä¸ç›¸å…³çš„å¥å­ã€‚      0   mgsd_zh\n",
      "145        race                  è¿™ä¸ªçº¦æ—¦å®¶åº­æ­£åœ¨æ—…è¡Œã€‚      0   mgsd_zh\n",
      "201    religion   æˆ‘å¦¹å¦¹åˆšä¹°äº†ä¸€ä¸ªBrahminæ‰‹è¢‹ã€‚æˆ‘ä»¬å¾—åŽ»ä¹°é‚®ç¥¨ã€‚      0   mgsd_zh\n",
      "Train data size: 240\n",
      "Test data size: 60\n"
     ]
    }
   ],
   "source": [
    "sample_size = 300\n",
    "train_data_mgsd_zh, test_data_mgsd_zh = data_loader(\n",
    "    csv_file_path='mgsd_zh_dataset.csv', \n",
    "    labelling_criteria='stereotype', \n",
    "    dataset_name='mgsd_zh', \n",
    "    sample_size=sample_size, \n",
    "    num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e2aa414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          group                                               text  label  \\\n",
      "160  profession                         è€å¸ˆå‘å­¦ç”Ÿè®²æŽˆå„é—¨å­¦ç§‘ã€‚æ˜ŸæœŸäºŒæ˜¯åƒçŽ‰ç±³å·é¥¼çš„å¥½æ—¥å­ã€‚      0   \n",
      "220        race                        æˆ‘å“¥å“¥å­¦ä¼šè¯´è‘¡è„ç‰™è¯­äº†ã€‚è´«æ°‘çªŸé‡Œçš„è€é¼ é•¿å¾—è·ŸçŒ«ä¸€æ ·å¤§ã€‚      0   \n",
      "273  profession                          åˆ†æžå¸ˆé—®äº†ä¸€ç³»åˆ—å…·ä½“çš„é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªä¸ç›¸å…³çš„å¥å­ã€‚      0   \n",
      "145        race                                        è¿™ä¸ªçº¦æ—¦å®¶åº­æ­£åœ¨æ—…è¡Œã€‚      0   \n",
      "201    religion                         æˆ‘å¦¹å¦¹åˆšä¹°äº†ä¸€ä¸ªBrahminæ‰‹è¢‹ã€‚æˆ‘ä»¬å¾—åŽ»ä¹°é‚®ç¥¨ã€‚      0   \n",
      "235        race                            è‹çš„å¥³æœ‹å‹æ˜¯å“¥ä¼¦æ¯”äºšäººã€‚å¥¹å–œæ¬¢åœ¨ç”µè„‘ä¸ŠæŸ¥èµ„æ–™ã€‚      0   \n",
      "87         race                                           åŽ„ç“œå¤šäººæ˜¯è­¦å¯Ÿã€‚      0   \n",
      "105        race                         å‡ åå¹´æ¥ï¼Œè·å…°äººä¸€ç›´æ¥åˆ°è¿™ä¸ªå›½å®¶ã€‚ä»–ä»¬å¾ˆå–œæ¬¢ä¸­å›½èœï¼      0   \n",
      "257        race                             é‚£åè‘¡è„ç‰™ç”·å­èµ°è¿›äº†å¤§æ¥¼ã€‚æœ‰äº›äººå–ç‰›å¥¶å’–å•¡ã€‚      0   \n",
      "106    religion                                             è€¶ç¨£æ˜¯åŸºç£å¾’      0   \n",
      "149  profession                                   å¤ä»¤è¥è¾…å¯¼å‘˜ç»™äº†æˆ‘ä¸€äº›äººç”Ÿå»ºè®®ã€‚      0   \n",
      "174        race                                  æˆ‘é‡åˆ°è¿‡çš„æ¯ä¸€ä¸ªè¥¿ç­ç‰™äººéƒ½å¾ˆå‹¤å¥‹ã€‚      0   \n",
      "80   profession                              æ¶ˆé˜²å‘˜è¢«è®¤ä¸ºå¾ˆæ™®é€šï¼Œå› ä¸ºæœ‰å¾ˆå¤šäººæ˜¯å¿—æ„¿è€…ã€‚      0   \n",
      "189      gender                                          å¥¹å¦ˆå¦ˆéžå¸¸æœ‰çˆ±å¿ƒã€‚      0   \n",
      "3          race                                  æˆ‘ä»¬é‡åˆ°çš„æ‰€æœ‰è‹±å›½äººéƒ½éžå¸¸æœ‰ç¤¼è²Œã€‚      0   \n",
      "276  profession                           åœ¨é¤åŽ…é‡Œï¼ŒæœåŠ¡å‘˜ç®€ç›´ä¸æ•¢ç›¸ä¿¡é‚£ä½æŠ¤å£«æ˜¯é‚£ä¹ˆä½“è´´ã€‚      0   \n",
      "21   profession                                       é¡¾é—®çš„å»ºè®®éžå¸¸â€œæ¤…å­â€ã€‚      0   \n",
      "121  profession                                 é‚£ä¸ªå’Œå¹³çš„å£«å…µå’Œå…¶ä»–äººä¸€èµ·ç«™æˆä¸€æŽ’ã€‚      0   \n",
      "294        race                                 æˆ‘å¯¹æ³¢å…°äººäº†è§£ä¸å¤šï¼Œä½†ä»–ä»¬ç›¸å½“å‹¤åŠ³ã€‚      0   \n",
      "43       gender                                    çˆ¸çˆ¸æ˜¯æˆ‘çš„çˆ¶äº²ã€‚æˆ‘å–œæ¬¢åƒä¸œè¥¿ã€‚      0   \n",
      "287      gender                                           é‚£ä½ç»…å£«æ˜¯é©¬å…‹ã€‚      0   \n",
      "188        race               å­ŸåŠ æ‹‰è¯­å¯ä¸æ˜¯ä»€ä¹ˆçŒ«çš„å“ç§ã€‚ä»–ä»¬å…¨éƒ½æ˜¯èƒŒç€å¸ƒè¢‹ã€ç©¿ç€æ—é®æ¯”çš„ä¼ªçŸ¥è¯†åˆ†å­ã€‚      0   \n",
      "20   profession                                     é‚£ä¸ªå‹¤åŠ³çš„å·¥äººæ²¡æœ‰æ‹¿åˆ°è–ªæ°´ã€‚      0   \n",
      "161  profession                                 ä»–æ˜¯ä¸€ä¸ªé±¼äººï¼Œåšä½œæ›²å®¶å·²ç»å¾ˆå¤šå¹´äº†ã€‚      0   \n",
      "217        race                                         å–€éº¦éš†äººéžå¸¸çˆ±åƒçŽ‰ç±³      0   \n",
      "91   profession                                          è¿™ä½ç§‘å­¦å®¶å¾ˆèªæ˜Žã€‚      0   \n",
      "103        race              ä½›å¾—è§’æœ‰è®¸å¤šç«å±±èŠ±ï¼Œè¿˜æœ‰å²©æµ†æµï¼Œå¾ˆå“äººã€‚æœ‰æ—¶å€™æˆ‘å–œæ¬¢åœ¨æ²¡äººçœ‹çš„æ—¶å€™æŒ–é¼»å­”ã€‚      0   \n",
      "131        race                                     é‚£ä¸ªæ¥è‡ªæ„å¤§åˆ©çš„ç”·äººå¾ˆå¥å£®ã€‚      0   \n",
      "166  profession                               è¿™ä½ç¥žå­¦å®¶åœ¨ä¸Šå­¦ã€‚å¥¹å¹´è½»è€Œä¸”å……æ»¡æ–°ç‚¹å­ã€‚      0   \n",
      "58     religion                                ç©†æ–¯æž—äººå¸¸å¸¸è¢«è®¤ä¸ºå¯¹å…¶ä»–å®—æ•™éžå¸¸å®½å®¹ã€‚      0   \n",
      "191  profession                             ä¸€å¼€å§‹é‚£ä¸ªåˆ†æžå¸ˆåœ¨æˆ‘çœ‹æ¥ç®€ç›´å°±æ˜¯å¾ˆâ€œåŽ•æ‰€â€ã€‚      0   \n",
      "54         race                         æˆ‘åœ¨æ˜¥å‡æœŸé—´åŽ»äº†å°æ¹¾ã€‚é‚£ä¸ªæ¡¶é¡ºç€å±±å¡å‘ç‰›ç¾¤æ»šäº†ä¸‹åŽ»ã€‚      0   \n",
      "1          race                                        å“¥ä¼¦æ¯”äºšäººæ€§æ ¼å¾ˆå¹³å’Œã€‚      0   \n",
      "134        race                               å“¥ä¼¦æ¯”äºšçœ‹èµ·æ¥æ˜¯ä¸€ä¸ªé€‚åˆå±…ä½çš„å¯Œè£•å›½å®¶ã€‚      0   \n",
      "190        race                                  æˆ‘åˆšåˆ°çš„æ—¶å€™è§‰å¾—é˜¿æ ¹å»·äººæŒºç²—é²çš„ã€‚      0   \n",
      "251  profession                   æœ‰ä½ç‰™åŒ»æ›¾ç»çº¦æˆ‘å‡ºåŽ»çº¦ä¼šã€‚ä»–ä¸ºäººéžå¸¸ç¤¼è²Œå‘¨åˆ°ï¼Œæœ€åŽæˆäº†æˆ‘çš„ä¸ˆå¤«ã€‚      0   \n",
      "102        race                                   æˆ‘ä»¬åŽ»åŸƒåŠåº¦å‡å§ã€‚ç‹—æ­£åœ¨æ‘‡å°¾å·´ã€‚      0   \n",
      "130  profession                                      æ£€å¯Ÿå®˜åœ¨æ³•åº­ä¸Šéžå¸¸æœ‰åè§ã€‚      0   \n",
      "299        race                             é‚£ä¸ªä¿„ç½—æ–¯äººæ”¾ä¸‹äº†ä»–çš„æ¯å­ã€‚é‡Œé¢å¹¶æ²¡æœ‰é…’ç²¾ã€‚      0   \n",
      "187        race                                         è¥¿ç­ç‰™å‡ ä¹Žæ€»æ˜¯å¾ˆç¾Žã€‚      0   \n",
      "205  profession                    ä¸€ä½è®¾è®¡å¸ˆåœ¨æ•´ä¸ªè¡Œä¸šå†…åˆ›é€ ç€å‰æ²¿çš„è‰ºæœ¯ã€‚åœ¨æµ·å²¸è¾¹é©¾è½¦éžå¸¸æƒ¬æ„ã€‚      0   \n",
      "214  profession      è¯å‰‚å¸ˆä»Žé‚£åç”·å­æ‰‹é‡ŒæŽ¥è¿‡å¤„æ–¹ã€‚å¥¹ç«™åœ¨æŸœå°é‡Œï¼Œé«˜é«˜åœ°ä¿¯è§†ç€ç”·å­ï¼Œçœ¯èµ·çœ¼ç›è¾¨è®¤åŒ»ç”Ÿæ½¦è‰çš„å­—è¿¹ã€‚      0   \n",
      "52         race                   é‚£ä¸ªåˆ©æ¯”é‡Œäºšç”·äººæŠŠè‚‰ç»™äº†æˆ‘ã€‚ä¸çŸ¥ä¸ºä»€ä¹ˆï¼Œé‚£é¢—æ£’çƒç¡¬å¾—åƒçŸ³å¤´ä¸€æ ·ã€‚      0   \n",
      "99         race                                       å“¥ä¼¦æ¯”äºšä»¥å…¶ç¾Žä¸½è€Œé—»åã€‚      0   \n",
      "34         race                                å°¼æ³Šå°”äººçš„ç”Ÿæ´»ä¹‹é“å°±åœ¨äºŽè¿‡ç®€å•çš„æ—¥å­ã€‚      0   \n",
      "49         race                          ä¹Œå…‹å…°ä½äºŽä¿„ç½—æ–¯çš„è¥¿é¢ã€‚é‚£é‡Œçš„äººæ°‘å·²ç»å¤±åŽ»äº†è‡ªç”±ã€‚      0   \n",
      "243        race                                        ç´¢é©¬é‡Œæ‰€æœ‰äººéƒ½æ˜¯é¦™çƒŸã€‚      0   \n",
      "252  profession                                ç®¡é“å·¥ä¼šä¿®ç†æ°´æ§½ã€‚ä»–ä»¬éƒ½æ˜¯é«˜ä¸­è¾å­¦ç”Ÿã€‚      0   \n",
      "298  profession                       æœ‰å¾ˆå¤šå®žä¹ ç”Ÿä¹Ÿæ˜¯ç ”ç©¶äººå‘˜ã€‚ä»–ä»¬çˆ±è¯„å¤´è®ºè¶³ï¼Œè€Œä¸”è–ªæ°´åä½Žã€‚      0   \n",
      "88         race                                        åœŸè€³å…¶äººçš„èœè‚´ç³Ÿé€äº†ã€‚      0   \n",
      "71   profession  ç¡…è°·ä¼ä¸šå®¶è¨å§†æ­£åœ¨æ‹›å‹Ÿäººæ‰ã€‚è¨å§†æ­£åœ¨å¯»æ±‚èµ„é‡‘åˆ›åŠžä¸€å®¶å…¬å¸ï¼Œè‡´åŠ›äºŽå¯¹åœ£ä½•å¡žè¾ƒè´«å›°çš„æœ‰è‰²äººç§ç¤¾åŒº...      0   \n",
      "133  profession                         æˆ‘çš„æœ‹å‹æ˜¯ä¸ªåŽ¨å¸ˆã€‚ä»Šå¹´å…¨çƒå„åœ°éƒ½å‡ºçŽ°äº†åˆ›çºªå½•çš„çƒ­æµªã€‚      0   \n",
      "151  profession                                     ä»–çš„ç«è…¿åŠ©æ‰‹ç¦»å¼€åŽ»ä¹°å’–å•¡äº†ã€‚      0   \n",
      "53   profession                       åœ¨æ··åˆäº†è¿™ä¸¤ç§åŒ–å­¦å“ä¹‹åŽï¼Œè¿™ä½èªæ˜Žçš„åŒ–å­¦å®¶è§‰å¾—è¿™è¡Œä¸é€šã€‚      0   \n",
      "169  profession                          å¦‚æžœæ‹³å‡»æ‰‹è®­ç»ƒä¸å……åˆ†ï¼Œå°±å¾ˆå®¹æ˜“å—ä¼¤ã€‚æ°´é¾™å¤´åœ¨æ»´æ°´ã€‚      0   \n",
      "270        race               é˜¿æ›¼åœ¨ä¸­ä¸œæœ‰å¾ˆå¤šæœ‰æŠ±è´Ÿçš„ä¼ä¸šå®¶ã€‚ä¸­ä¸œçš„ä¸“ä¸šäººå£«éƒ½åœ¨åŠªåŠ›æƒ³æˆä¸ºæ‚è´§åº—è€æ¿ã€‚      0   \n",
      "241        race                         è¥¿ç­ç‰™æ˜¯ä¸€ä¸ªæ¬§æ´²å›½å®¶ã€‚é‚£é‡Œçš„æ¯ä¸ªäººéƒ½å¯¹è¶³çƒç–¯ç‹‚è¿·æ‹ã€‚      0   \n",
      "13   profession                                      æˆ‘ä»Žæ¥æ²¡è§è¿‡è¿™ä¹ˆä¸‘çš„æ¼”å‘˜ã€‚      0   \n",
      "50         race  è¿™åäºšè£”åŒ»å­¦ç”Ÿä¹‹æ‰€ä»¥èµ°ä¸Šè¿™æ¡èŒä¸šé“è·¯ï¼Œæ˜¯å› ä¸ºä»–çš„çˆ¶æ¯æŠŠä»–æŽ¨ä¸Šæ¥çš„ï¼Œè€Œä»–ä»¬å½“å¹´ä¹Ÿæ˜¯è¢«è‡ªå·±çš„çˆ¶æ¯...      0   \n",
      "48         race                            æˆ‘å¯ä»¥åŽ»é»Žå·´å«©æ—…è¡Œã€‚é‚£é‡Œæœ‰å¾ˆå¤šå¥½çœ‹çš„ç”µè§†èŠ‚ç›®ã€‚      0   \n",
      "\n",
      "    data_name  \n",
      "160   mgsd_zh  \n",
      "220   mgsd_zh  \n",
      "273   mgsd_zh  \n",
      "145   mgsd_zh  \n",
      "201   mgsd_zh  \n",
      "235   mgsd_zh  \n",
      "87    mgsd_zh  \n",
      "105   mgsd_zh  \n",
      "257   mgsd_zh  \n",
      "106   mgsd_zh  \n",
      "149   mgsd_zh  \n",
      "174   mgsd_zh  \n",
      "80    mgsd_zh  \n",
      "189   mgsd_zh  \n",
      "3     mgsd_zh  \n",
      "276   mgsd_zh  \n",
      "21    mgsd_zh  \n",
      "121   mgsd_zh  \n",
      "294   mgsd_zh  \n",
      "43    mgsd_zh  \n",
      "287   mgsd_zh  \n",
      "188   mgsd_zh  \n",
      "20    mgsd_zh  \n",
      "161   mgsd_zh  \n",
      "217   mgsd_zh  \n",
      "91    mgsd_zh  \n",
      "103   mgsd_zh  \n",
      "131   mgsd_zh  \n",
      "166   mgsd_zh  \n",
      "58    mgsd_zh  \n",
      "191   mgsd_zh  \n",
      "54    mgsd_zh  \n",
      "1     mgsd_zh  \n",
      "134   mgsd_zh  \n",
      "190   mgsd_zh  \n",
      "251   mgsd_zh  \n",
      "102   mgsd_zh  \n",
      "130   mgsd_zh  \n",
      "299   mgsd_zh  \n",
      "187   mgsd_zh  \n",
      "205   mgsd_zh  \n",
      "214   mgsd_zh  \n",
      "52    mgsd_zh  \n",
      "99    mgsd_zh  \n",
      "34    mgsd_zh  \n",
      "49    mgsd_zh  \n",
      "243   mgsd_zh  \n",
      "252   mgsd_zh  \n",
      "298   mgsd_zh  \n",
      "88    mgsd_zh  \n",
      "71    mgsd_zh  \n",
      "133   mgsd_zh  \n",
      "151   mgsd_zh  \n",
      "53    mgsd_zh  \n",
      "169   mgsd_zh  \n",
      "270   mgsd_zh  \n",
      "241   mgsd_zh  \n",
      "13    mgsd_zh  \n",
      "50    mgsd_zh  \n",
      "48    mgsd_zh  \n"
     ]
    }
   ],
   "source": [
    "print(test_data_mgsd_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59a4811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f70cacc9ee44e85a8048f817e6d3507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d243d0cec88544f3bde18ed7c7e26fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'profession', 'text': 'è€å¸ˆå‘å­¦ç”Ÿè®²æŽˆå„é—¨å­¦ç§‘ã€‚æ˜ŸæœŸäºŒæ˜¯åƒçŽ‰ç±³å·é¥¼çš„å¥½æ—¥å­ã€‚', 'label': 0, 'data_name': 'mgsd_zh', '__index_level_0__': 160, 'input_ids': [0, 6, 15238, 2110, 7252, 18735, 58016, 4035, 6349, 72584, 30, 32999, 99276, 4011, 199647, 31023, 193950, 33666, 91453, 30, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support\n",
       "0                   1.0     1.0       1.0     60.0\n",
       "accuracy            1.0     1.0       1.0      1.0\n",
       "macro avg           1.0     1.0       1.0     60.0\n",
       "weighted avg        1.0     1.0       1.0     60.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_data_mgsd_zh, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/test_data_mgsd_zh', \n",
    "               dataset_name='test_data_mgsd_zh', \n",
    "               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de07fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv('mgsd_zh_dataset.csv', usecols=['text', 'label', 'group'])\n",
    "\n",
    "combined_data['data_name'] = 'mgsd_zh_dataset'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493e37cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          group                          text  label        data_name\n",
      "0          race               å°åº¦å°¼è¥¿äºšæ˜¯ä¸€ä¸ªéžå¸¸å¹²å‡€çš„å›½å®¶      0  mgsd_zh_dataset\n",
      "1          race                   å“¥ä¼¦æ¯”äºšäººæ€§æ ¼å¾ˆå¹³å’Œã€‚      0  mgsd_zh_dataset\n",
      "2          race                     æ³•å›½äººé€šå¸¸éƒ½å¾ˆå¤šæ¯›      0  mgsd_zh_dataset\n",
      "3          race             æˆ‘ä»¬é‡åˆ°çš„æ‰€æœ‰è‹±å›½äººéƒ½éžå¸¸æœ‰ç¤¼è²Œã€‚      0  mgsd_zh_dataset\n",
      "4          race    è¡—å°¾çš„é‚£ä¸ªç”·äººæ˜¯ç´¢é©¬é‡Œäººã€‚ä»–ç§¯æžä¸ºå¥³æ€§äº‰å–å¹³ç­‰æƒåˆ©ã€‚      0  mgsd_zh_dataset\n",
      "..          ...                           ...    ...              ...\n",
      "295        race                 æœ‰å¾ˆå¤šå…³äºŽè¶Šå—çš„æ°”çƒç…§ç‰‡ã€‚      0  mgsd_zh_dataset\n",
      "296      gender              å¥¹å’Œå¥¹çš„ç»§çˆ¶æœ‰ä¸€ç§ä¹¦æœ¬ä¸Šçš„å…³ç³»ã€‚      0  mgsd_zh_dataset\n",
      "297        race    ä¿„ç½—æ–¯äººä¸å–œæ¬¢ä¹Œå…‹å…°äººï¼Œä¸è¿‡ä»–ä»¬å¯¹ä¹Œå…‹å…°äººéžå¸¸å®½å®¹ã€‚      0  mgsd_zh_dataset\n",
      "298  profession  æœ‰å¾ˆå¤šå®žä¹ ç”Ÿä¹Ÿæ˜¯ç ”ç©¶äººå‘˜ã€‚ä»–ä»¬çˆ±è¯„å¤´è®ºè¶³ï¼Œè€Œä¸”è–ªæ°´åä½Žã€‚      0  mgsd_zh_dataset\n",
      "299        race        é‚£ä¸ªä¿„ç½—æ–¯äººæ”¾ä¸‹äº†ä»–çš„æ¯å­ã€‚é‡Œé¢å¹¶æ²¡æœ‰é…’ç²¾ã€‚      0  mgsd_zh_dataset\n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61dfce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2813ce3da7ce43e085dd2bf3f2f8118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd417feeda74406afc5b18c745f5974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'å°åº¦å°¼è¥¿äºšæ˜¯ä¸€ä¸ªéžå¸¸å¹²å‡€çš„å›½å®¶', 'label': 0, 'data_name': 'mgsd_zh_dataset', 'input_ids': [0, 6, 26449, 12379, 3891, 10783, 13663, 4528, 135878, 101400, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15509\\anaconda3\\envs\\hearts\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.751269</td>\n",
       "      <td>0.730864</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.589465</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.585945</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.627715</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.631353</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.711538  0.751269  0.730864  197.000000\n",
       "1              0.467391  0.417476  0.441026  103.000000\n",
       "accuracy       0.636667  0.636667  0.636667    0.636667\n",
       "macro avg      0.589465  0.584372  0.585945  300.000000\n",
       "weighted avg   0.627715  0.636667  0.631353  300.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(combined_data, \n",
    "               model_output_dir='model_output_xlm-roberta-base/merged_winoqueer_seegull_gpt_augmentation_trained', \n",
    "               result_output_base_dir='result_output_xlm-roberta-base/test_data_mgsd_zh', \n",
    "               dataset_name='test_data_mgsd_zh', \n",
    "               seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
